{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spec 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import KMNIST\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to NAS with RL. There, the actions were decided hyperparameters; here, actions are deciding which weights to go to. Rather than update weights (such as with backpropagation), we move to certain weights (i.e., dettached layers).\n",
    "\n",
    "\n",
    "<u> States</u>  Continuous state space with each state representing the the output of the CNN.\n",
    "\n",
    "<u> Actions </u> Continuous action space with each action being a probability distribution of which next layer to go to.\n",
    "\n",
    "<u> Reward </u> Loss from CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information does the gradient give us? How can we use it?\n",
    "- The partial derivatives tell us the rate of change of the loss due to that weight. It is a vector of slopes. So then we have some interesting pieces of information:\n",
    "    - the weight \n",
    "    - the weight's rate of change\n",
    "    - the weight's importance (in terms of the partial derivative of the loss w.r.t. this weight)\n",
    "\n",
    "Using the gradient at each layer\n",
    "- Keep track of information about the weights by index\n",
    "- This is one benefit of having the same dimension for the layer/weights in each detached layer\n",
    "- The trend for a weight can indicate what change we want to make\n",
    "- But maybe this doesn't work because each change to the weight makes sense in terms of the context (i.e., the changes to all the other weights; e.g., a certain change in weight be unecessary if we make some other change to a different weight)\n",
    "\n",
    "- A connection is that if I knew how a weight was changing, then I could know what to look for (here, what next layer we want because it has that weight at a desired value).\n",
    "\n",
    "When does the network stop?\n",
    "- One possibility is in the case of a CNN, it stops when it reaches the dense layer (i.e., when the RL agent chooses that for its action). \n",
    "\n",
    "What is the reward for the controller?\n",
    "- The loss at each layer\n",
    "\n",
    "What is my state space? Is it continuous or discrete?\n",
    "- If my states are the detached layers, then one state will have the same value regardless of where in the sequence of layers it and by which layers was it preceded - but of course that information (sequence and composition) are critically important to how the built network performs\n",
    "- Discrete: Each state is a network (i.e., the network after that action, which was the attachment of some layer)\n",
    "- Continuous: If each state is a network, how can approximate networks and use a continuous model?\n",
    "    - We can represent the network as a vector with something like [number_of_detached_layers, weights]\n",
    "    - The downside of this approach is there is a set number of layers (set architecture)\n",
    "    - But this doesn't work either because it doesn't seem to do anything with the data\n",
    "\n",
    "Do the weights of individual detached layers get updated? When do they get updated?\n",
    "- TODO\n",
    "\n",
    "How is the value of the state (so that detached layer in this network, so far) getting updated?\n",
    "- TODO\n",
    "\n",
    "Is there an opportunity to use square matrices and the inverse property to engineer backwards a solution? Can we work back from some output to the matrices (weights) we need?\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "# define the train and val splits\n",
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading the CIFAR10 dataset...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "[INFO] generating the train/validation split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the KMNIST dataset\n",
    "print(\"[INFO] loading the CIFAR10 dataset...\")\n",
    "trainData = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,\n",
    "\ttransform=transforms.ToTensor())\n",
    "testData = torchvision.datasets.MNIST(root=\"data\", train=False, download=True,\n",
    "\ttransform=transforms.ToTensor())\n",
    "\n",
    "# calculate the train/validation split\n",
    "print(\"[INFO] generating the train/validation split...\")\n",
    "numTrainSamples = int(len(trainData) * TRAIN_SPLIT)\n",
    "numValSamples = int(len(trainData) * VAL_SPLIT)\n",
    "(trainData, valData) = torch.utils.data.random_split(trainData,\n",
    "\t[numTrainSamples, numValSamples],\n",
    "\tgenerator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the train, validation, and test data loaders\n",
    "trainDataLoader = torch.utils.data.DataLoader(trainData, shuffle=True,batch_size=BATCH_SIZE)\n",
    "valDataLoader = torch.utils.data.DataLoader(valData, batch_size=BATCH_SIZE)\n",
    "testDataLoader = torch.utils.data.DataLoader(testData, batch_size=BATCH_SIZE)\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "valSteps = len(valDataLoader.dataset) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetachedLayer(nn.Module):\n",
    "    '''\n",
    "    Layer that will be chained with other layers by the\n",
    "    RL agent.\n",
    "    '''\n",
    "    def __init__(self, in_channels, classes, kernel_size=(5,5)):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels=20, kernel_size=kernel_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=kernel_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=800, out_features=500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=classes)  # final output has 10 classes\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # the '1' in flatten keeps the first dimension,\n",
    "        # which is the batch size 4\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing the model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] initializing the model...\")\n",
    "model = DetachedLayer(\n",
    "\tin_channels=1,\n",
    "\tclasses=len(trainData.dataset.classes)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=INIT_LR)\n",
    "lossFn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    }
   ],
   "source": [
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in range(0, EPOCHS):\n",
    "\tmodel.train()\n",
    "\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\tfor (x, y) in trainDataLoader:\n",
    "\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = lossFn(pred, y)\n",
    "\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))\n",
    "print(\"[INFO] evaluating network...\")\n",
    "with torch.no_grad():\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\tpreds = []\n",
    "\tfor (x, y) in testDataLoader:\n",
    "\t\tx = x.to(device)\n",
    "\t\t\n",
    "\t\tpred = model(x)\n",
    "\t\tpreds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "\n",
    "print(classification_report(testData.targets.cpu().numpy(),\n",
    "\tnp.array(preds), target_names=testData.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 9/10\n",
      "Train loss: 0.00514, Train accuracy: 0.0000\n",
      "Val loss: 0.00000, Val accuracy: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\n",
    "\t# calculate the training and validation accuracy\n",
    "\ttrainCorrect = trainCorrect / len(trainDataLoader.dataset)\n",
    "\tvalCorrect = valCorrect / len(valDataLoader.dataset)\n",
    "\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"train_acc\"].append(trainCorrect)\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\tH[\"val_acc\"].append(valCorrect)\n",
    "\n",
    "\tprint(\"Average train loss: {:.5f}, Average train accuracy: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, trainCorrect))\n",
    "\tprint(\"Average validation loss: {:.5f}, Average validation accuracy: {:.4f}\\n\".format(\n",
    "\t\tavgValLoss, valCorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('masters-thesis-3CL5olUq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4ad1d2f18a2ff8426cfdd9be91e5890d73ede4af27f55ec1abd481aac780cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
