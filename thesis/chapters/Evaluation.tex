\chapter{Evaluation}
\label{Evaluation}
The following research questions guided evaluation of REML:
\begin{itemize}
    \item Can REML enable models to transfer knowledge from one domain 
    to another (can it be sample efficient)? 
    \item Can REML generate models that learn from a few examples 
    (can it do few shot learning)? 
    \item Can REML generate models for different modes of learning such 
    as regression and classification (does it support multimodal learning)?
\end{itemize}
The first two questions (i.e., whether REML is sample efficient and can 
support few shot learning) are evaluated in context of the third question 
for both regression and classification tasks. 
\\\\
Each learining mode evaluates the performance of the outer network (i.e., the 
meta-policy) and the inner networks generated and trained by the meta-policy. 
The outer network's policy is evaluated according to its:
\begin{itemize}
    \item return per epoch (i.e., a pass through all tasks)
    \item ability to converge in fewer training steps than a neural network trained 
    without a meta-learner with the same hyperparameters for the same tasks (i.e.,
    show \textit{transfer learning})
    \item ability to generalize to unseen tasks with few data points (e.g., k=5,
    k=10) (i.e., show \textit{few-shot learning})
\end{itemize}
The inner networks are evaluated according to the requirements of the task
for the regression and for classification. For regression, the networks'
loss is tracked per timestep to see if it is learning as well as the curve fit
for the sine curve in the task. For classification, the networks' accuracy across
classes is evaluated.
\section{Regression}
\subsection{Sine curves}
REML was first evaluated on relatively simple sine curves, following the same
protocol 2018, Finn et all performed on MAML \cite{FinAbbLev:17}. 
The task is defined as a sine curve within a distribution of curves that vary in 
their amplitude and phase shift. The amplitude is chosen from [0.1, 5.0] and 
phase shift is chosen from [0, pi]. 
\\\\
The outer-network's role for the task is to construct a neural network
that can provide a best fit curve for the sinusoidal function. The inner-network's
role (as it is constructed by the outer network) is to predict a value as close
as possible to the target output for the input.
\\\\
The inner network generated by the outer network had set hyperparameters. Each
network could have up to 5 layers, with the hidden layers represented as fully
connected layers with 40 nodes. ReLU activation functions were used in between
each hidden layer, and the identity function for the final output layer as 
this is regression. Leaky ReLU was tested with different alpha values but
it generally slowed down convergence and there was no need to used it as 
this bounded design (i.e., a pre-set number of layers output by the outer
network) prevented a vanishing gradient.
\section{Classification}
\subsection{Miniimagenet}
\subsection{Omniglot}

\minitoc 