\chapter{Background}
\label{Background}

\minitoc 

\section{Artificial Neural Networks}
Artificial neural networks (ANNs) are non-linear computational models that approximate 
a target function   $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, where $n$ and $m$ are 
integers \cite{Bis:06}. Given a set $X = \{ (\mathbf{x}_1, \mathbf{y}_1), \ldots, (\mathbf{x}_N,
 \mathbf{y}_N) \} \subset \R^n \times \R^m$ of input-output pairs of size $N$ the model is 
 trained to approximate $f$ such that $f(\mathbf{x}_i) = \mathbf{y}_i \forall i \in \{1, 2, 
 \ldots, N\}$. By the Universal Approximation theorem, a neural network's approximation of a 
 continuous function is theoretically guaranteed to be as precise as it needs to be given the 
 network has at least one hidden layer with some finite number of nodes \cite{HorStiWhi:89}. 

\subsection{Starting from linear regression}
Consider the problem of predicting the value of one or more continuous target variables 
$\mathbf{t} \subset \R^m$ provided a $D$-dimensional vector $\mathbf{x}_n$ of input 
variables, or what is called regression. Given a set consisting of $N$ observation and 
value pairs $\set{(\mathbf{x}_{n}, \mathbf{t}_{n})}_{n=1}^N$, the objective is to predict 
the value for any input vector $\mathbf{x}_n$ such that it is as as close as possible 
to the provided target value $\mathbf{t}_n$. 
\\\\
One approach is linear regression, or a linear combination over the components of an input
pattern $\mathbf{x}_n$
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = w_0 + w_1x_1 + \ldots + w_Dx_D
\end{equation}
where $\mathbf{x}_n$ has $D$ dimensions, $\mathbf{x}_n = (x_1, \ldots, x_D)^T$ and $w \in \R^{D+1}$ 
represents the parameters of the function, $w = (w_0, \ldots, w_D)$ and $D$ is extended to $D+1$ 
for the bias weight $w_0$. 
\\\\
As is, this regression function is limited to being a linear function over the input vector 
$\mathbf{x}_n$. Non-linear basis functions $\phi$ on the input variables make the function 
$y\left(\mathbf{x}_n, \mathbf{w}\right)$ non-linear for an input $\mathbf{x}_n$:
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = w_0 + \sum_{i=1}^{D} w_i \phi_i\left(x_i\right)
\end{equation}
This equation can be simplified further if we define a useful basis function for the bias
$\phi_0 (\mathbf{x}) =1$ such that
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = \sum_{i=0}^{D} w_i \phi_i\left(x_i\right)
\end{equation}
Despite producing non linear outputs over the input $\mathbf{x}$ this is \textit{linear
regression} because it is linear with respect to $\mathbf{w}$.

\subsection{Constructing neural networks}
Basic ANNs can be seen as an extension to linear regression where the basis functions become 
parameterized. The basis functions continue to be non-linear functions over the linear 
combination of the input, but now the output of the basis function is dependent on the 
learned coefficients $\set{w_j}$. In this construction, basis functions are known as 
\textit{activation} functions $h$ in the context of neural networks. 
\\\\
We start by rewriting equation 1.2 as a linear combinations over the input variables to 
produce $a$ or the \textit{activation}. 
\begin{equation}
    a = \sum_{i=1}^{D} w_{i}x_i + w_{0} \phi\left(x_i\right)
\end{equation}
The value $a$  is transformed using a non-linear activation function $h$. This 
transformation produces $z$ and is referred to as a \textit{hidden unit}. 
\begin{equation}
    z = h\left(a\right)
\end{equation}
The coefficients $\set{w_j}$ parameterizing this non-linear transformation
are referred to as a \textit{layer}.
\\\\
An ANN has a minimum of two layers - an input layer and output layer. 
However, ANNs are not limited to 2 layers.
ANNs can have $l$ many layers where $l \in [2, +\infty)$. 
Networks with $>2$ layers are referred to as \textit{deep neural networks}.
For the purposes of the background, we will continue with the simple 2-layer case to
establish preliminaries.
\\\\
The input layer operates on an input $(x_1, \ldots, x_D)$ to produce \textit{activations}
$a_j = (a_1, \ldots, a_M)$, where $M$ denotes the number of parameters $\set{w_j}$ in the 
input layer. 
The parameters for the input layer are represented with a superscript (1) and the parameters 
for the output layer will be represented with a superscript (2).
\begin{equation}
    a_j = \sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}
\end{equation}
The activations are passed through a non-linear activation $h$
\begin{equation}
    z_j = h\left(a_{j}\right)
\end{equation}
The output layer then transforms the hidden units $z_j$ to produce output unit activations 
$a_k$ where $k \in (1, \ldots, K)$ and $K$ is the number of outputs expected for this problem
(i.e., appropriate to the target variable $\mathbf{t}_i$ for $\mathbf{x}_i$).
\begin{equation}
     a_k = \sum_{j=1}^{M} w_{kj}^{(2)}z_j + w_{k0}^{(2)} 
\end{equation}
The activation $a_k$ is transformed by a different non-linear activation function that is 
appropriate for $K$. Here this activation function is represented as $\sigma$. A common choice 
of activation function $h$ for non-output layers is the rectified linear unit 
$h(a) = \min(0, a)$. A common choice of activation function for $\sigma$ is the sigmoid 
function $\sigma(a) = \frac{1}{1 + e^{-a}}$ for classification problems and the identity $y_k = a_k$ 
for simple regression problems.
We now present the equation for a \textit{feed-forward} pass through a 2-layer ANN.
\begin{equation}
    y_{k}(\mathbf{x}_n, \mathbf{w}) =  \sigma \left( \sum_{j=1}^{M}  w_{kj}^{(2)}h \left( \sum_{i=1}^{D} w_{ji}^{(1)} + w_{j0}^{(1)} \phi\left(x_i\right)\right) + w_{k0}^{(2)} \right)
\end{equation}
A neural network then is a non-linear function over an input $\mathbf{x_n}$ to an output $y_k$ 
that seeks to approximate $\mathbf{t_n}$ and is controlled by a set of adaptable parameters
$\mathbf{w}$.
\subsection{Training a neural network}
The goal of learning for a neural network is to optimize the parameters of the network such 
that the loss function $E(X, \mathbf{w})$ takes the lowest value. Continuing with the 
previous example for regression, we look at the sum-of-squares error function 
\begin{equation}
    E(X, \mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N} \norm{y\left(\mathbf{x}_n, \mathbf{w}  \right) - \mathbf{t}_n}^2
\end{equation}
There is typically not an analytical solution and iterative procedures are used to 
minimize the loss function $E$. The steps taken are
\begin{equation}
    \mathbf{w}^{(\tau+1)} = \mathbf{w^{(\tau)}} + \Delta \mathbf{w}^{(\tau)}
\end{equation} where $\tau$ is the iteration step. An approach for the weight update step 
with $\Delta \mathbf{w}^{(\tau)}$ is to use the gradient of $E(X, \mathbf{w})$ with 
respect to the parameters $\mathbf{w}$. The weights are updated in the direction of 
steepest error function decrease or in the $- \nabla E(X, \mathbf{w})$ direction.
\begin{equation}
     \mathbf{w}^{(\tau+1)} = \mathbf{w^{(\tau)}} + \alpha \nabla E\left( X, \mathbf{w}^{(\tau)} \right)
\end{equation} where $\alpha > 0$ is the learning rate controlling the size of update step 
taken. This iterative procedure is called \textit{gradient descent optimization} 
\cite{RumHinWil:86}. 
\subsection{Error function derivatives}
The gradient $\nabla E(X, \mathbf{w})$ is calculated with respect to every 
$w \in \mathbf{w}$, for all $\mathbf{x}_n \in X$. 
\\\\
We start with one input pattern $\mathbf{x_n}$ and rewrite the error function as 
\begin{equation}
    \begin{split}
    E_n &= \frac{1}{2} \left(y(\mathbf{x_n}, \mathbf{w}) - \mathbf{t_n} \right)^2  \\
     &= \frac{1}{2} \sum_{j=1} (w_{kj}z_{j} - t_{nk})^2
    \end{split}
\end{equation} 
The calculation starts with the gradient of $E_n$ with respect to each $w_{kj}$ in 
the output layer (2) then continues backwards to layer (1) for $w_{ji}$. 
This method can extend to $l$-layer networks where $l = (1, \ldots, L)$ and 
$L \subseteq \R$.
\\\\
Observe that
\begin{equation}
    \frac{\partial E_n}{\partial w_{kj}} = \frac{\partial E_n}{\partial a_k} \\
    \frac{\partial a_k}{\partial w_{kj}}
\end{equation}
We start by calculating the partial derivative of $E_n$ with respect to the 
activation $a_k$. Recall that $a_k = \sum_{k} w_{kj}{z_j}$. By the chain rule:
\begin{equation}
    \begin{split}
    \frac{\partial E_n}{\partial a_k} &= (h(a_k) - t_{nk})h'(a_k) \\
    &= h'(a_k)(\hat{y}_n - t_{nk})
    \end{split}
\end{equation}
We introduce a new notation to call this partial derivative an \textit{error}
\begin{equation}
    \delta_{k} \equiv \frac{\partial E_n}{\partial a_k} 
\end{equation}
Next we calculate the partial derivate of $a_k$ with respect to $w_{kj}$
\begin{equation}
    \begin{split}
    \frac{\partial a_k}{\partial w_{kj}} &= \frac{\partial}{\partial w_{kj}} \left( \sum_{k} w_{kj}z_{k} \right) \\
    &= z_{k}
    \end{split}
\end{equation} 
With which we can write
\begin{equation}
    \frac{\partial{E_n}}{\partial w_{kj}} = \delta_{k}z_{k}
\end{equation}
The procedure will continue in the same way for the remainder of the layers and their 
units, where we calculate the errors $\delta$ for the units in the layer and multiply 
error of that unit by its activation $z$. For layer (1) (input layer) we need to calculate
\begin{equation}
    \frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j} \\
    \frac{\partial a_j}{\partial w_{ji}}
\end{equation}
starting with $\delta_{j}$ or $\frac{\partial E_{n}}{\partial a_{j}}$. 
Observe that 
\begin{equation}
    \frac{\partial E_n}{\partial a_j} = \sum_{k} \\
    \frac{\partial E_n}{\partial a_{k}} \frac{\partial a_{k}}{\partial a_{j}}
\end{equation} where k is the number of outputs (here, k=1 for the continued
regression example).
\\\\
We calculated $\frac{\partial E_{n}}{\partial a_{k}}$ above. Continue with 
\begin{equation}
    \begin{split}
    \frac{\partial a_k}{\partial a_{j}} &= \frac{\partial}{\partial a_{j}} \left( \sum_{k} w_{kj}h(a_{j}) \right) \\
    &= h'(a_{j})w_{kj}
    \end{split}
\end{equation} 
We can finish calculating the error $\delta_{j}$ for equation 1.18
\begin{equation}
    \begin{split}
    \frac{\partial E_n}{\partial a_k} \frac{\partial a_k}{\partial a_{j}} &= h'(a_k)(\hat{y}_n - t_{nk}) h'(a_{j}) w_{kj} \\
    &= \frac{\partial E_n}{\partial a_j} \\
    & = \delta_{j}
    \end{split}
\end{equation} 
Thus we obtain the \textit{backpropagation} formula
\begin{equation}
    \delta_{j} = h'(a_{j}) \sum_{k} w_{kj} \delta_{k}
\end{equation} where the error for a unit $j$ is the result of backpropagating the errors 
in the units later in the network.
\\\\
Calculating the gradient is backpropgating the errors. As we have seen, this
procedure begins with a forward propagation of the input vectors $x_{n}$ to
calculate the activations of all units. Then, it involves calculating the 
errors $\delta_{k}$ in the output layer. Using $\delta_{k}$ we can calculate
$\delta_{j}$ for the hidden units in previous layers. With all errors 
$\delta$, the gradient is calculated by multiplying the error by the 
activations $a$ transformed by their non-linear function $h$ where $h(a) = z$.
\subsection{Recurrent Neural Networks}
ANNs can be constructed as \textit{directed graphs}, formally defined as $G = (V, E)$ where 
$V$ is the set of vertices $\set{v_1, \ldots, v_n}$ and $E$ is the set of edges 
$\set{(u,v) | u, v \in V}$. We show neural networks are directed becuause the edges are a 
set of ordered pairs. In comparison, an undirected graph would have edges 
$\set{ \{ u,v \} | u, v \in V}$.
In terms appropriate to neural networks, $V$ corresponds to our hidden units $\set{z}$ 
and output units and $E$ corresponds to the parameters $\set{w}$.
\\\\
The 2-layer network we constructed above was a \textit{directed acyclic graph}.
$G \text{ is acyclic if } \forall v \in V, \text{ there does not exist a cycle
containing } v$. This means that for $\forall (u,v) \in E \text{, } u \neq v$.
\\\\
ANNs can contain cycles however. A type of ANN that contains cycles is a 
\textit{recurrent neural network} (RNN) \cite{RumHinWil:86}. An RNN is recurrent in that 
information persists in the network by being passed from one forward propagation step to 
the next. This ability to incorporate past network data makes RNNs useful for simulating 
dynamical systems.
\\\\
RNNs model past network data as $\mathbf{h}_{t}$ or the \textit{hidden state}
\begin{align}
    &\mathbf{h}_t = f_{h}(\mathbf{x}_t, \mathbf{h}_{t-1}) \\
    &\mathbf{y}_t = f_{o}(\mathbf{h}_t)
\end{align} where $f_{h}$ is a transition function parameterized by $\theta_{h}$ 
and $f_{o}$ is an output function parameterized by $\theta_{o}$ \cite{PasGulChoBen:13}.
The transition function can be a non-linear function such as the rectified linear unit
or the sigmoid function.
\\\\
Datasets used with RNNs may include $T_n$ many input patterns $\mathbf{x}^{(n)}$
where $T_n \in \R$ is the number of timesteps for which there is data for the 
datapoint 
\begin{equation}
    \set{(\mathbf{x}_{1}^{(n)}, \mathbf{y}_{1}^{(n)}), \ldots, 
    (\mathbf{x}_{Tn}^{(n)}, \mathbf{y}_{Tn}^{(n)}) }_{n=1}^N 
\end{equation}
The cost function is
\begin{equation}
    E(\theta) = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T} \\
    d(\mathbf{y}_t^{(n)}, f_o(\mathbf{h}_t^{(n)}))
\end{equation} where $\theta$ is the parameters of the network, and 
$d(\mathbf{a}, \mathbf{b})$ is the divergence measure such as Euclidean distance used 
in the above sum of squares error. 
\\\\
The parameters $\theta$ are updated with a variant of backpropagation that works with
sequential data called \textit{backpropagation through time (BPTT)} 
\cite{Wer:90}. This method ``unrolls'' the RNN into each a computational graph one
time step at a time. This unrolled RNN is equivalent to a deep neural network where the
same parameters re-appear throughout the network per timestep. Backpropagation through 
time sums the gradient with respect to each parameter for all times the parameter 
appears in the network.
\\\\
RNNs are prone to challenges during training including \textit{exploding gradient} and 
\textit{vanishing gradient}. During training with BPTT the gradients can become very 
large (i.e., exploding) or very small (i.e., vanishing). Calculating the errors involves
multiplying the errors from later layers by the activations in earlier layers as defined
above. RNNs can have long sequences in the unrolled network, meaning many multiplication
operations over the gradients. Multiplying large or small numbers many times will lead to
very large numbers and very small numbers, respectively.
\\\\
A large gradient will cause large weight updates in the gradient update step, such as 
in gradient descent optimization, which will make training unstable. A small gradient 
will cause negligent or no weight updates such that no learning happens and hidden unit 
activation trend to 0. These activations are called \textit{dead neurons} where 
``neuron'' is another word for a hidden unit.

\subsection{Long Short-Term Memory Networks}
An extension of the RNN is the Long Short-Term Memory Network (LSTM), intended 
to address the exploding and vanishing gradient problems or ``error back-flow 
problems''\cite{HocSch:97}. LSTMs introduce additional calculations called ``gates'' 
within the cells of an RNN. These gates control how much information is retained or
discarded in each timestep. Each cell has state $C_t$ and the gates responsible for
modifying $C_t$ across $T_n$ for $(x_{1n}, \ldots, x_{Tn})$.
\\\\
The \textit{forget gate} controls the amount of information retained from the previous
unit $h_{t-1}$ and the input $x_t$ to include in this state $C_t$
\begin{equation}
    f_t = \sigma (W_f \cdot [h_{t-1}, x_t] + b_f)
\end{equation} where $W_f$ is a weight matrix and $b_f$ is the bias term. The sigmoid
is used as it outputs a value in $[0, 1]$, with $0$ meaning to discard all previous 
network data and $1$ meaning to keep all previous network data.
\\\\
The \textit{input gate} controls the amount of information to be included from the input
$x_t$ 
\begin{equation}
    i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i) 
\end{equation} where $W_i$ is the weight matrix and $b_i$ is the bias term for this gate,
respectively.
\\\\
The output of the forget gate and input gate are composed as a proposal vector
that would be added element-wise to $C_t$. 
\begin{align}
    \widetilde{C}_t &= \tanh{(W_C \cdot [h_{t-1}, x_t] + b_C)}
\end{align} where $\widetilde{C}_t$ holds the amount of information to include from 
$x_t$ and $h_{t-1}$. The $\tanh$ function is the hyperbolic tangent function 
$\frac{e^{2x} - 1}{e^{2x} + 1}$. It is used to transform $x$ to a value within 
$[-1, 1]$ that results in more stable gradient calculations.
\\\\
The cell state $C_t$ is the sum of the two values we have constructed: some amount of 
the previous state $C_{t-1}$ and some amount of the proposed $\widetilde{C}_t$.
\begin{equation}
    C_t = f_t * C_{t-1} + i_t * \widetilde{C_t}
\end{equation}
The final calculation what to output - the new hidden state $h_t$. The cell calculates 
how much of the new cell state $C_t$ to output for this timesteep.
\begin{align}
    o_t &= \sigma (W_o \cdot [h_{t-1}, x_t] + b_o) \\
    h_t &= o_t * \tanh(C_t)
\end{align} 

\section{Reinforcement Learning}
An agent exhibiting reinforcement learning (RL) learns from interacting with an 
environment. 
The environment is represented in terms of states it can take on based on the agent's 
actions. 
This environment gives the agent a numerical reward signal based on the state-action pair.
The agent continues to take actions within the environment, trying to maximize its
cumulative reward.
\\\\
In reinforcement learning, the learning objective of the agent then is to learn 
a \textit{policy} $\pi$, a map from each state $s \in S$ and action $a \in A_{s}$ to 
the probability of $\pi(a|s)$ of taking action $a$ in state $s$ that maximizes cumulative
reward over timesteps $t$.
\\\\
Reinforcement learning is considered a distinct type of learning to \textit{supervised
learning} and \textit{unsupervised learning}. The learning considered thus far has been 
\textit{supervised learning} or learning from labeled examples where the ground truth is known. 
In this supervised context, the agent or model is given the answer after it acts. The 
model's task is instead to learn from labeled data such that it can generalize
or approximate to unseen examples where a label does not exist. 
RL is not supervied learning because no answer is ever provided to the agent; the agent
needs to discover its' own answer. 
RL is also not \textit{unsupervised learning}, where the objective is to find patterns in 
unlabeled data; while the agent may build a model of the environment it interacts with as
a kind of pattern recognition, the objective of RL is to maximize the numerical reward
signal rather than to discover hidden structure.
\\\\
A reason reinforcement learning is used over supervised learning is the answer may not 
be known for a sufficiently complex task. Another reason is its often impractical to 
provided a full set of representative examples of all states the agent may experience.
\subsection{Markov Decision Processes}
Consider an agent that interacts with the environment over timesteps 
$t \in \mathbb{R}_{\geq 0}$. For each timestep $t=0,1,2,\ldots$ the agent is in a state
$s \in S$ where it can take an action $a \in A_s$ and recieve the reward signal $r_{t+1}$ 
as it transitions from state $s=s_t$ to state $s'=s_{t+1}$. This sequence would look
something like
\begin{equation}
    S_0, A_0, R_1, S_1, A_1, R_2, \ldots
\end{equation}
The state is the information the agent has about the environment. It is based on this
state that the agent takes an action and enters a new state -- and this
process of taking an action from a state to a new state repeats. Therefore, the state
needs to encode information that allows a decision in the context of how the agent is doing 
doing in the environment.
\\\\
This information is more than the immediate sensations provided by the environment but must
include relevant information about the sequence thus far (i.e., the history) of the agent's 
interactions with the environment. A state is said to be \textit{Markov} if it encodes
all previous states' information as relevant to take the next action in the current state.
\\\\
In other words, $\forall s \in S$, $s$ incorporates information (history) of the seqeuence
to the current state.
The probability of the next state $s_{t+1}$ only depends on $s_{t}$ and $a_s$. 
The past history of states and action transitions is not needed.
This condition is the \textit{Markov property}. 
\\\\
When a state is Markov, it has complete information on the dynamics of the 
environment
\begin{equation}
    p(s', r | s, a) = P\set{R_{t+1}=r, S_{t+1}=s'|S_t, A_t}
\end{equation} where the probability of the environment continuing to state 
$s'$ depends only on this state $s_{t}$ and action $a_{t}$. 
For this to be true, $\forall s \in S$, $s$ has a history of all seqeuences 
possible from that state. 
\\\\
RL problems that have the Markov property can be modelled as Markov Decision Processes 
(MDP) \cite{Bel:57}.
A markov decision process (MDP) is represented as a 4-tuple $(S, A, P_a, R_a)$ where 
\begin{itemize}
    \item $S$ is the set of states or \textit{state space}
    \item $A=\set{A_s | s \in S}$ is the set of actions or \textit{action space}
    \item $P$ is $P_a(s, s') = P(s_{t+1} = s' | s, a)$ or \textit{transition function}
    \item $R_a(s, s')=\set{r | r \in \R}$ is the reward upon transition from state s to state s' or \textit{reward function}
\end{itemize}
The goal of the agent is to maximize the reward signal over timesteps $t$ where
$t \in T, \quad T \subseteq \R$. The reward value $r_t \in \R$ is rewarded by the 
environment to the agent every timestep.
This accummulated value is called the \textit{return}. 
\begin{equation}
    G_t = R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_{T}
\end{equation}
This equation for the return works if the time horizon of the agent's experience is finite.
This is the case when experience have a termination condition that concludes the agent's 
trajectory. A learning experience that ends with a finite $T$ is called an \textit{episode}
as in \textit{episodic learning}. This type of learning fits tasks that have a natural
endpoint, such as a car parking itself in a valid spot.
A learning experience without a boundary like this is called a \textit{continuing task}.
However, if $T = \infty$ then the the return could be infinite. \\\\
Another problem with the above formulation for return is it provides equal weighting to
all rewards. 
A \textit{discount factor} is often added to the calculation to produce a finite sum
\begin{align}
    G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + \ldots + \gamma^{T-1}R_{T} \\
    &= \sum_{k=0}^\infty \gamma^{k} R_{t+k+1}
\end{align}
where the discount factor $\gamma$ is typically a number between $0$ and $1$. The 
addition of a discount factor transforms the reward contribution to the return such that
a reward $k$ time steps into the future is only worth $\gamma^{k-1}$ as much. The reward
is in this sense ``discounted''. 
\\\\
Values closer to $0$ will make the agent ``myopic'' in the sense the agent only takes into
account the immediate reward and zeroes all subseqeuent rewards after $s_{t+1}$. 
In this configuration, the agent learns only from the next action it takes and is unable 
to learn sequences of actions that lead to some future state. 
\\\\
Values closer to $1$, on the other hand, increasingly weigh future actions further
in the trajectory. A value of $1$, as discussed above, would mean each action is weighed
equally; therefore, values closer to $1$ such $0.95 \text{ or } 0.90$ provide more weight to 
actions near $T$.
\\\\
The discount factor is tuned to reach a balance between a focus on near-term rewards and 
longer-term rewards. It helps address the problem of \textit{temporal credit assignment} 
or the difficulty attributing credit to past action(s) for an observed return. 
Part of the challenge is the delay from the timestep $t$ an action$a_t$ is taken at and 
when the return is calculated $T$. 
In other words, the environment may pass through multiple timesteps before the effect of 
an action is observed.
The problem can be framed as figuring out the influence of each action on the return. 
Using a discount factor spreads the credit across timesteps as a measure of the reward and
how far into the future actions are from the current timestep.
\subsection{Value Functions}
As an agent interacts within its environment taking actions, the agent needs a way to
decide what next action $a_t$ to take in its state $s_t$. The value is decided in 
terms of the expected return discussed above. 
\\\\
There are two kinds of functions to approximate value depending on whether the input
is the state $s_t$ or the state-action pair $(s_t, a_t)$. 
\\\\
The value of a state is 
\begin{equation}
    v_{\pi}(s) = \mathbb{E}_{\pi}[G_{t}|S_{t}=s] = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma_{k}R_{t+k+1w}|S_{t}=s \right]
\end{equation} where $\mathbb{E}\left[ \cdot \right]$ is the expected value of a random variable 
provided the agent follows policy $\pi$ at timestep $t$.

\subsection{Policy Gradient Methods}
\subsection{PPO}
\subsection{Recurrent Policy}
\section{Meta-Learning}
Meta-learning or ``learning to learn'' is a general framework of using
information learned from one task for future tasks. One definition extends the 
idea of reuse for future tasks by specifying that performance on future tasks
should improve with each task learned \cite{ThrunPratt:98}. Another definition...
\subsection{Few shot learning}
