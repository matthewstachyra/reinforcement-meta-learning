\chapter{Background}
\label{ch1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\minitoc 

\section{Artificial Neural Networks}
Artificial neural networks (ANNs) are non-linear computational models that approximate a target function   $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, where $n$ and $m$ are integers \cite{Bis:06}. Given a set $X = \{ (x_1, y_1), ..., (x_N, y_N) \} \subset \R^n \times \R^m$ of input-output pairs of size $N$ the model is trained to approximate $f$ such that $f(x_i) = y_i \forall i \in \{1, 2, \ldots, N\}$. By the Universal Approximation theorem, a neural network's approximation of a continuous function is theoretically guaranteed to be as precise as it needs to be given the network has at least one hidden layer with some finite number of nodes \cite{HorStiWhi:89}. \\\\
Consider the problem of predicting the value of one or more continuous target variables \mathbf{t} provided a D-dimensional vector \mathbf{x} of input variables, or what is called regression. Given a training set consisting of $N$ observations $\set{ \mathbf{x}_1, ..., \mathbf{x}_n }$ and values $\{t_n\}_{n=1}^N$, the objective is to predict the output value for any input $x$ as close as possible to the provided target value \mathbf{t}. The regression function is a linear combination over the input variables 
\begin{equation} 
    y(\mathbf{x},\mathbf{w}) = w_0 + w_1x_1 + ... + w_Dx_D
\end{equation}
where $\mathbf{x}$ has $D$ dimensions, $\mathbf{x} = (x_1, ..., x_D)^T$ and $w \in \R^{D+1}$ represents the parameters of the function, $w = (w_0, ..., w_D)$. This function is limited to being a linear function over the input vector $\mathbf{x}$. Non-linear basis functions $\phi$ on the input variables make the function $y\left(\mathbf{x}, \mathbf{w}\right)$ non-linear.
\begin{equation} 
    y(\mathbf{x},\mathbf{w}) = w_0 + \sum_{i=1}^{N} w_i \phi_i\left(x_i\right)
\end{equation}
The basic neural network model extends this by making the basis functions $\phi\left(x\right)$ depend on learnable parameters along with the weights $w$.The neural network performs a series of linear transformations. We start with $M$ linear combinations of the input variables where $j = 1, ..., M$.
\begin{equation}
    a_j = \sum_{i=1}^{D} w_{ji}x_i + w_{j0} \phi\left(x_i\right)
\end{equation}
The \textit{activation} $a_j$  is transformed using non-linear activation function $h$.
\begin{equation}
    z_j = h\left(a_j\right)
\end{equation}
This transformation corresponds to a \textit{layer} in a neural network. The network has two layers - one input layer over the input variables $x_1, ..., x_D$  and one output layer that takes the activation from the input layer's transformation through an activation function suited to the provided target variables $t_1,...,t_N$. The parameters for the input layer are represented with a superscript (1) and the parameters for the output layer are represented with a superscript (2).
\begin{equation}
    z_j = h\left( \sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)} \right)
\end{equation}
\begin{equation}
     a_k = \sum_{j=1}^{M} w_{kj}^{(2)}z_j + w_{k0}^{(2)} 
\end{equation}
Equations (1.5) and (1.6) combined present the equation for a \textit{feed-forward} pass through a 2-layer ANN.
\begin{equation}
    y_{k}(\mathbf{x}, \mathbf{w}) =  \left( \sum_{j=1}^{M}  w_{kj}^{(2)}h \left( \sum_{i=1}^{D} w_{ji}^{(1)} + w_{j0}^{(1)} \phi\left(x_i\right)\right) + w_{k0}^{(2)} \right)
\end{equation}


% \subsection{ANNs as directed graphs}
% ANNs are directed graphs. 
 
% 
% In this view, the nodes of a graph (called "neurons") are non-linear functions that operate on the inputs to it. The edges feeding into a node contain parameters that are multiplied against
% In neural networks, the directed graph is composed of so called "layers" which are parameterized non-linear functions that applies to the input to the network and then to the output of the preceding layer. 
% More specifically, the graph's nodes (also called "neurons") are non-linear "activation" functions and the edges are the parameters (also called "weights") of the network.
% An input x, which may be denoted as (x1, ..., xn) is multiplied by the parameters (w1, ..., wn)^T and added a bias term b to output z. 
% This is passed to a non-linear "activation" function in the neuron or node a(z) to produce y. 
% This affine function comes out to be y = a(z) = w^T . x + b. It is often represented as a sum. E_1_n (wi .xi + b).

% As mentioned earlier, non-linearity is introduced into the network with activation functions that are applied to the immediately preceding linear transformation. A common activation function due to its breadth of applicable use cases is the rectified linear unit, a piecewise linear function that is linear for positive inputs and 0 for negative inputs: f(x) = max(0,x). Another common activation function is the sigmoid function represented as: f(x) = 1 / 1 + e^-x. It bounds the input x to (0, 1). This makes it a common choice for tasks where the output is used to represent a probability. 

% \begin{figure}
% \centering
% \includegraphics[width=0.8\columnwidth]{}
% \caption[Short description for list of figures]{This figure is taken from \cite{Sca:16}.}
% \label{fig-magnitude}
% \end{figure}%




\subsection{Training a neural network}
The goal of learning for a neural network is to optimize the weights of the network such that the loss function $E(X, \mathbf{w})$ takes the lowest value. Continuing with the previous example for regression, we look at the sum-of-squares error function 
\begin{equation}
    E(X, \mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N} \norm{y\left(\mathbf{x}_n, \mathbf{w}  \right) - \mathbf{t}_n}^2
\end{equation}
There is typically not an analytical solution and iterative procedures are used to minimize the loss function $E$. The steps taken are
\begin{equation}
    \mathbf{w}^{(\tau+1)} = \mathbf{w^{(\tau)}} + \Delta \mathbf{w}^{(\tau)}
\end{equation} where $\tau$ is the iteration step. An approach for the weight update step with $\Delta \mathbf{w}^{(\tau)}$ is to use gradient information where the gradient is of $E(X, \mathbf{w})$ with respect to the parameters $\mathbf{w}$. The weights are updated in the direction of steepest error function decrease or in the $- \nabla E(X, \mathbf{w})$ direction. Equation 1.9 thus becomes
\begin{equation}
     \mathbf{w}^{(\tau+1)} = \mathbf{w^{(\tau)}} + \alpha \nabla E\left( X, \mathbf{w}^{(\tau)} \right)
\end{equation} where $\alpha > 0$ is the learning rate controlling the size of update step taken. This iterative procedure is called 
\textit{gradient descent optimization} \cite{RumHinWil:86}. The gradient must be calculated with respect to every layer's weights in the 
neural network. Consider first the case of one weight $w_{ji}$ where $j$ corresponds to the parameter in the layer and $i$ corresponds to the 
component in the input. Provided the error function for 1 input $E_n = \frac{1}{2} \sum_{k} (y_{nk} - t_{nk})^2$, the derivative of 
$E_n$ with respect to $w_{ji}$ is 
\begin{equation}
    \frac{\partial E_n}{\partial w_{ji}} = (y_{nj} - t_{nj})x_{ni}
\end{equation}
However, in a neural network there is more than 1 weight and more than 1 layer. Backpropagation seeks to calculate the derivative of the 
error with respect to every weight in the network from the output units backwards through all hidden units. Equation 1.11 can be rewritten as 
\begin{equation}
        \frac{\partial E_n}{\partial w_{ji}} =  \frac{\partial E_n}{\partial a_j} \frac{\partial a_j}{\partial w_{ji}}
\end{equation} where $a_j$ is the weighted sum of previous units with weight $w_{ji}$. This term is called an \textit{error} where here it is the error for unit j.
\begin{equation}
    \delta_{j} \equiv \frac{\partial E_n}{\partial a_j} 
\end{equation}
And because $\frac{\partial a_j}{\partial w_{ji}} = z_{i}$ we can write equation 1.12 as
\begin{equation}
    \frac{\partial E_n}{\partial w_{ji}} =  \delta_{j} z_{i}
\end{equation}












\section{Title section}
\section{Notation}
\section{Published material}


% OPTIONAL: COPY THEM IN ANY NEW CHAPTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

