\chapter{Background}
\label{ch1}

\minitoc 

\section{Artificial Neural Networks}
Artificial neural networks (ANNs) are non-linear computational models that approximate 
a target function   $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, where $n$ and $m$ are 
integers \cite{Bis:06}. Given a set $X = \{ (\mathbf{x}_1, \mathbf{y}_1), \ldots, (\mathbf{x}_N,
 \mathbf{y}_N) \} \subset \R^n \times \R^m$ of input-output pairs of size $N$ the model is 
 trained to approximate $f$ such that $f(\mathbf{x}_i) = \mathbf{y}_i \forall i \in \{1, 2, 
 \ldots, N\}$. By the Universal Approximation theorem, a neural network's approximation of a 
 continuous function is theoretically guaranteed to be as precise as it needs to be given the 
 network has at least one hidden layer with some finite number of nodes \cite{HorStiWhi:89}. 

\subsection{Starting from linear regression}
Consider the problem of predicting the value of one or more continuous target variables 
$\mathbf{t} \subset \R^m$ provided a $D$-dimensional vector $\mathbf{x}_n$ of input 
variables, or what is called regression. Given a set consisting of $N$ observation and 
value pairs $\set{(\mathbf{x}_{n}, \mathbf{t}_{n})}_{n=1}^N$, the objective is to predict 
the value for any input vector $\mathbf{x}_n$ such that it is as as close as possible 
to the provided target value $\mathbf{t}_n$. 
\\\\
One approach is linear regression, or a linear combination over the components of an input
pattern $\mathbf{x}_n$
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = w_0 + w_1x_1 + \ldots + w_Dx_D
\end{equation}
where $\mathbf{x}_n$ has $D$ dimensions, $\mathbf{x}_n = (x_1, \ldots, x_D)^T$ and $w \in \R^{D+1}$ 
represents the parameters of the function, $w = (w_0, \ldots, w_D)$ and $D$ is extended to $D+1$ 
for the bias weight $w_0$. 
\\\\
As is, this regression function is limited to being a linear function over the input vector 
$\mathbf{x}_n$. Non-linear basis functions $\phi$ on the input variables make the function 
$y\left(\mathbf{x}_n, \mathbf{w}\right)$ non-linear for an input $\mathbf{x}_n$:
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = w_0 + \sum_{i=1}^{D} w_i \phi_i\left(x_i\right)
\end{equation}
This equation can be simplified further if we define a useful basis function for the bias
$\phi_0 (\mathbf{x}) =1$ such that
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = \sum_{i=0}^{D} w_i \phi_i\left(x_i\right)
\end{equation}
Despite producing non linear outputs over the input $\mathbf{x}$ this is \textit{linear
regression} because it is linear with respect to $\mathbf{w}$.

\subsection{Constructing neural networks}
Basic ANNs can be seen as an extension to linear regression where the basis functions become 
parameterized. The basis functions continue to be non-linear functions over the linear 
combination of the input, but now the output of the basis function is dependent on the 
learned coefficients $\set{w_j}$. In this construction, basis functions are known as 
\textit{activation} functions $h$ in the context of neural networks. 
\\\\
We start by rewriting equation 1.2 as a linear combinations over the input variables to 
produce $a$ or the \textit{activation}. 
\begin{equation}
    a = \sum_{i=1}^{D} w_{i}x_i + w_{0} \phi\left(x_i\right)
\end{equation}
The value $a$  is transformed using a non-linear activation function $h$. This 
transformation produces $z$ and is referred to as a \textit{hidden unit}. 
\begin{equation}
    z = h\left(a\right)
\end{equation}
The coefficients $\set{w_j}$ parameterizing this non-linear transformation
are referred to as a \textit{layer}.
\\\\
An ANN has a minimum of two layers - an input layer and output layer. 
However, ANNs are not limited to 2 layers.
ANNs can have $l$ many layers where $l \in [2, +\infty)$. 
Networks with $>2$ layers are referred to as \textit{deep neural networks}.
For the purposes of the background, we will continue with the simple 2-layer case to
establish preliminaries.
\\\\
The input layer operates on an input $(x_1, \ldots, x_D)$ to produce \textit{activations}
$a_j = (a_1, \ldots, a_M)$, where $M$ denotes the number of parameters $\set{w_j}$ in the 
input layer. 
The parameters for the input layer are represented with a superscript (1) and the parameters 
for the output layer will be represented with a superscript (2).
\begin{equation}
    a_j = \sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}
\end{equation}
The activations are passed through a non-linear activation $h$
\begin{equation}
    z_j = h\left(a_{j}\right)
\end{equation}
The output layer then transforms the hidden units $z_j$ to produce output unit activations 
$a_k$ where $k \in (1, \ldots, K)$ and $K$ is the number of outputs expected for this problem
(i.e., appropriate to the target variable $\mathbf{t}_i$ for $\mathbf{x}_i$).
\begin{equation}
     a_k = \sum_{j=1}^{M} w_{kj}^{(2)}z_j + w_{k0}^{(2)} 
\end{equation}
The activation $a_k$ is transformed by a different non-linear activation function that is 
appropriate for $K$. Here this activation function is represented as $\sigma$. A common choice 
of activation function $h$ for non-output layers is the rectified linear unit 
$h(a) = \min(0, a)$. A common choice of activation function for $\sigma$ is the sigmoid 
function $\sigma(a) = \frac{1}{1 + e^{-a}}$ for classification problems and the identity $y_k = a_k$ 
for simple regression problems.
We now present the equation for a \textit{feed-forward} pass through a 2-layer ANN.
\begin{equation}
    y_{k}(\mathbf{x}_n, \mathbf{w}) =  \sigma \left( \sum_{j=1}^{M}  w_{kj}^{(2)}h \left( \sum_{i=1}^{D} w_{ji}^{(1)} + w_{j0}^{(1)} \phi\left(x_i\right)\right) + w_{k0}^{(2)} \right)
\end{equation}
A neural network then is a non-linear function over an input $\mathbf{x_n}$ to an output $y_k$ 
that seeks to approximate $\mathbf{t_n}$ and is controlled by a set of adaptable parameters
$\mathbf{w}$.
\subsection{Training a neural network}
The goal of learning for a neural network is to optimize the parameters of the network such 
that the loss function $E(X, \mathbf{w})$ takes the lowest value. Continuing with the 
previous example for regression, we look at the sum-of-squares error function 
\begin{equation}
    E(X, \mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N} \norm{y\left(\mathbf{x}_n, \mathbf{w}  \right) - \mathbf{t}_n}^2
\end{equation}
There is typically not an analytical solution and iterative procedures are used to 
minimize the loss function $E$. The steps taken are
\begin{equation}
    \mathbf{w}^{(\tau+1)} = \mathbf{w^{(\tau)}} + \Delta \mathbf{w}^{(\tau)}
\end{equation} where $\tau$ is the iteration step. An approach for the weight update step 
with $\Delta \mathbf{w}^{(\tau)}$ is to use the gradient of $E(X, \mathbf{w})$ with 
respect to the parameters $\mathbf{w}$. The weights are updated in the direction of 
steepest error function decrease or in the $- \nabla E(X, \mathbf{w})$ direction.
\begin{equation}
     \mathbf{w}^{(\tau+1)} = \mathbf{w^{(\tau)}} + \alpha \nabla E\left( X, \mathbf{w}^{(\tau)} \right)
\end{equation} where $\alpha > 0$ is the learning rate controlling the size of update step 
taken. This iterative procedure is called \textit{gradient descent optimization} 
\cite{RumHinWil:86}. 
\subsection{Error function derivatives}
The gradient $\nabla E(X, \mathbf{w})$ is calculated with respect to every 
$w \in \mathbf{w}$, for all $\mathbf{x}_n \in X$. 
\\\\
We start with one input pattern $\mathbf{x_n}$ and rewrite the error function as 
\begin{equation}
    \begin{split}
    E_n &= \frac{1}{2} \left(y(\mathbf{x_n}, \mathbf{w}) - \mathbf{t_n} \right)^2  \\
     &= \frac{1}{2} \sum_{j=1} (w_{kj}z_{j} - t_{nk})^2
    \end{split}
\end{equation} 
The calculation starts with the gradient of $E_n$ with respect to each $w_{kj}$ in 
the output layer (2) then continues backwards to layer (1) for $w_{ji}$. 
This method can extend to $l$-layer networks where $l = (1, \ldots, L)$ and 
$L \subseteq \R$.
\\\\
Observe that
\begin{equation}
    \frac{\partial E_n}{\partial w_{kj}} = \frac{\partial E_n}{\partial a_k} \\
    \frac{\partial a_k}{\partial w_{kj}}
\end{equation}
We start by calculating the partial derivative of $E_n$ with respect to the 
activation $a_k$. Recall that $a_k = \sum_{k} w_{kj}{z_j}$. By the chain rule:
\begin{equation}
    \begin{split}
    \frac{\partial E_n}{\partial a_k} &= (h(a_k) - t_{nk})h'(a_k) \\
    &= h'(a_k)(\hat{y}_n - t_{nk})
    \end{split}
\end{equation}
We introduce a new notation to call this partial derivative an \textit{error}
\begin{equation}
    \delta_{k} \equiv \frac{\partial E_n}{\partial a_k} 
\end{equation}
Next we calculate the partial derivate of $a_k$ with respect to $w_{kj}$
\begin{equation}
    \begin{split}
    \frac{\partial a_k}{\partial w_{kj}} &= \frac{\partial}{\partial w_{kj}} \left( \sum_{k} w_{kj}z_{k} \right) \\
    &= z_{k}
    \end{split}
\end{equation} 
With which we can write
\begin{equation}
    \frac{\partial{E_n}}{\partial w_{kj}} = \delta_{k}z_{k}
\end{equation}
The procedure will continue in the same way for the remainder of the layers and their 
units, where we calculate the errors $\delta$ for the units in the layer and multiply 
error of that unit by its activation $z$. For layer (1) (input layer) we need to calculate
\begin{equation}
    \frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j} \\
    \frac{\partial a_j}{\partial w_{ji}}
\end{equation}
starting with $\delta_{j}$ or $\frac{\partial E_{n}}{\partial a_{j}}$. 
Observe that 
\begin{equation}
    \frac{\partial E_n}{\partial a_j} = \sum_{k} \\
    \frac{\partial E_n}{\partial a_{k}} \frac{\partial a_{k}}{\partial a_{j}}
\end{equation} where k is the number of outputs (here, k=1 for the continued
regression example).
\\\\
We calculated $\frac{\partial E_{n}}{\partial a_{k}}$ above. Continue with 
\begin{equation}
    \begin{split}
    \frac{\partial a_k}{\partial a_{j}} &= \frac{\partial}{\partial a_{j}} \left( \sum_{k} w_{kj}h(a_{j}) \right) \\
    &= h'(a_{j})w_{kj}
    \end{split}
\end{equation} 
We can finish calculating the error $\delta_{j}$ for equation 1.18
\begin{equation}
    \begin{split}
    \frac{\partial E_n}{\partial a_k} \frac{\partial a_k}{\partial a_{j}} &= h'(a_k)(\hat{y}_n - t_{nk}) h'(a_{j}) w_{kj} \\
    &= \frac{\partial E_n}{\partial a_j} \\
    & = \delta_{j}
    \end{split}
\end{equation} 
Thus we obtain the \textit{backpropagation} formula
\begin{equation}
    \delta_{j} = h'(a_{j}) \sum_{k} w_{kj} \delta_{k}
\end{equation} where the error for a unit $j$ is the result of backpropagating the errors 
in the units later in the network.
\\\\
Calculating the gradient is backpropgating the errors. As we have seen, this
procedure begins with a forward propagation of the input vectors $x_{n}$ to
calculate the activations of all units. Then, it involves calculating the 
errors $\delta_{k}$ in the output layer. Using $\delta_{k}$ we can calculate
$\delta_{j}$ for the hidden units in previous layers. With all errors 
$\delta$, the gradient is calculated by multiplying the error by the 
activations $a$ transformed by their non-linear function $h$ where $h(a) = z$.
\subsection{Recurrent Neural Networks}
ANNs can be constructed as \textit{directed graphs}, formally defined as $G = (V, E)$ where 
$V$ is the set of vertices $\set{v_1, \ldots, v_n}$ and $E$ is the set of edges 
$\set{(u,v) | u, v \in V}$. We show neural networks are directed becuause the edges are a 
set of ordered pairs. In comparison, an undirected graph would have edges 
$\set{ \{ u,v \} | u, v \in V}$.
In terms appropriate to neural networks, $V$ corresponds to our hidden units $\set{z}$ 
and output units and $E$ corresponds to the parameters $\set{w}$.
\\\\
The 2-layer network we constructed above was a \textit{directed acyclic graph}.
$G \text{ is acyclic if } \forall v \in V, \text{ there does not exist a cycle
containing } v$. This means that for $\forall (u,v) \in E \text{, } u \neq v$.
\\\\
ANNs can contain cycles however. A type of ANN that contains cycles is a 
\textit{recurrent neural network} (RNN) \cite{RumHinWil:86}. An RNN is recurrent in that 
information persists in the network by being passed from one forward propagation step to 
the next. This ability to incorporate past network data makes RNNs useful for simulating 
dynamical systems.
\\\\
RNNs model past network data as $\mathbf{h}_{t}$ or the \textit{hidden state}
\begin{align}
    &\mathbf{h}_t = f_{h}(\mathbf{x}_t, \mathbf{h}_{t-1}) \\
    &\mathbf{y}_t = f_{o}(\mathbf{h}_t)
\end{align} where $f_{h}$ is a transition function parameterized by $\theta_{h}$ 
and $f_{o}$ is an output function parameterized by $\theta_{o}$ \cite{PasGulChoBen:13}.
The transition function can be a non-linear function such as the rectified linear unit
or the sigmoid function.
\\\\
Datasets used with RNNs may include $T_n$ many input patterns $\mathbf{x}^{(n)}$
where $T_n \in \R$ is the number of timesteps for which there is data for the 
datapoint 
\begin{equation}
    \set{(\mathbf{x}_{1}^{(n)}, \mathbf{y}_{1}^{(n)}), \ldots, 
    (\mathbf{x}_{Tn}^{(n)}, \mathbf{y}_{Tn}^{(n)}) }_{n=1}^N 
\end{equation}
The cost function is
\begin{equation}
    E(\theta) = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T} \\
    d(\mathbf{y}_t^{(n)}, f_o(\mathbf{h}_t^{(n)}))
\end{equation} where $\theta$ is the parameters of the network, and 
$d(\mathbf{a}, \mathbf{b})$ is the divergence measure such as Euclidean distance used 
in the above sum of squares error. 
\\\\
The parameters $\theta$ are updated with a variant of backpropagation that works with
sequential data called \textit{backpropagation through time (BPTT)} 
\cite{Wer:90}. This method ``unrolls'' the RNN into each a computational graph one
time step at a time. This unrolled RNN is equivalent to a deep neural network where the
same parameters re-appear throughout the network per timestep. Backpropagation through 
time sums the gradient with respect to each parameter for all times the parameter 
appears in the network.
\\\\
RNNs are prone to challenges during training including \textit{exploding gradient} and 
\textit{vanishing gradient}. During training with BPTT the gradients can become very 
large (i.e., exploding) or very small (i.e., vanishing). Calculating the errors involves
multiplying the errors from later layers by the activations in earlier layers as defined
above. RNNs can have long sequences in the unrolled network, meaning many multiplication
operations over the gradients. Multiplying large or small numbers many times will lead to
very large numbers and very small numbers, respectively.
\\\\
A large gradient will cause large weight updates in the gradient update step, such as 
in gradient descent optimization, which will make training unstable. A small gradient 
will cause negligent or no weight updates such that no learning happens and hidden unit 
activation trend to 0. These activations are called \textit{dead neurons} where 
``neuron'' is another word for a hidden unit.

\subsection{Long Short-Term Memory Networks}
An extension of the RNN is the Long Short-Term Memory Network (LSTM), intended 
to address the exploding and vanishing gradient problems or ``error back-flow 
problems''\cite{HocSch:97}. LSTMs introduce additional calculations called ``gates'' 
within the cells of an RNN. These gates control how much information is retained or
discarded in each timestep. Each cell has state $C_t$ and the gates responsible for
modifying $C_t$ across $T_n$ for $(x_{1n}, \ldots, x_{Tn})$.
\\\\
The \textit{forget gate} controls the amount of information retained from the previous
unit $h_{t-1}$ and the input $x_t$ to include in this state $C_t$
\begin{equation}
    f_t = \sigma (W_f \cdot [h_{t-1}, x_t] + b_f)
\end{equation} where $W_f$ is a weight matrix and $b_f$ is the bias term. The sigmoid
is used as it outputs a value in $[0, 1]$, with $0$ meaning to discard all previous 
network data and $1$ meaning to keep all previous network data.
\\\\
The \textit{input gate} controls the amount of information to be included from the input
$x_t$ 
\begin{equation}
    i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i) 
\end{equation} where $W_i$ is the weight matrix and $b_i$ is the bias term for this gate,
respectively.
\\\\
The output of the forget gate and input gate are composed as a proposal vector
that would be added element-wise to $C_t$. 
\begin{align}
    \widetilde{C}_t &= \tanh{(W_C \cdot [h_{t-1}, x_t] + b_C)}
\end{align} where $\widetilde{C}_t$ holds the amount of information to include from 
$x_t$ and $h_{t-1}$. The $\tanh$ function is the hyperbolic tangent function 
$\frac{e^{2x} - 1}{e^{2x} + 1}$. It is used to transform $x$ to a value within 
$[-1, 1]$ that results in more stable gradient calculations.
\\\\
The cell state $C_t$ is the sum of the two values we have constructed: some amount of 
the previous state $C_{t-1}$ and some amount of the proposed $\widetilde{C}_t$.
\begin{equation}
    C_t = f_t * C_{t-1} + i_t * \widetilde{C_t}
\end{equation}
The final calculation what to output - the new hidden state $h_t$. The cell calculates 
how much of the new cell state $C_t$ to output for this timesteep.
\begin{align}
    o_t &= \sigma (W_o \cdot [h_{t-1}, x_t] + b_o) \\
    h_t &= o_t * \tanh(C_t)
\end{align} 

\section{Reinforcement Learning}
An agent exhibiting reinforcement learning (RL) learns from actions it takes in an 
environment that gives the agent a numerical reward signal for the agent's actions. 
The environment is represented in terms of states it can take on based on the agent's 
actions. 
The action taken in a state yields a new state together with some reward.
The goal of the agent then is to learn a \textit{policy}, a map from states to actions
for the environment, that maximizes the cumulative reward over time for the agent.
As the agent learns this policy, the agent needs to decide when to take an action it 
knows (i.e, it has taken and knows the value of) and an action it doesn't know \cite{SutBar:18}.
\\\\
Reinforcement learning is considered a distinct type of learning to \textit{supervised
learning} and \textit{unsupervised learning}. The learning considered thus far has been 
\textit{supervised learning} or learning from labeled examples where the ground truth is known. 
In this supervised context, the agent or model is given the answer after it acts. The 
model's task is instead to learn from labeled data such that it can generalize
or approximate to unseen examples where a label does not exist. 
RL is not supervied learning because no answer is ever provided to the agent; the agent
needs to discover its' own answer. 
RL is also not \textit{unsupervised learning}, where the objective is to find patterns in 
unlabeled data; while the agent may build a model of the environment it interacts with as
a kind of pattern recognition, the objective of RL is to maximize the numerical reward
signal rather than to discover hidden structure.
\\\\
A reason reinforcement learning is used over supervised learning is the answer may not 
be known for a sufficiently complex task. Another reason is its often impractical to 
provided a full set of representative examples of all states the agent may experience.
\subsection{Markov Decision Processes}
The problems solved by RL are often formalized as a markov decision process 
(MDP) \cite{Bel:57}.
A markov decision process (MDP) is often used to formalize and model the problems and is
represented as a 4-tuple $(S, A, P_a, R_a)$ where 
\begin{itemize}
    \item $S$ is the set of states or \textit{state space}
    \item $A=\set{A_s | s \in S}$ is the set of actions or \textit{action space}
    \item $P$ is $P_a(s, s') = P(s_{t+1} = s' | s, a)$ or \textit{transition function}
    \item $R_a(s, s')=\set{r | r \in \R}$ is the reward upon transition from state s to state s' or \textit{reward function}
\end{itemize}
Consider an agent that interacts with the environment over timesteps 
$t \in \mathbb{R}_{\geq 0}$. For each timestep $t=0,1,2,\ldots$ the agent is in a state
$s \in S$ where it can take an action $a \in A_s$ and recieve the reward signal $r_{t+1}$ 
as it transitions from state $s=s_t$ to state $s'=s_{t+1}$. This sequences would look
something like
\begin{equation}
    S_0, A_0, R_1, S_1, A_1, R_2, \ldots
\end{equation}
In a finite MDP where there is a finite number of states in $S$, actions in $A$, and rewards
in $R$, the random variables for $S$ and $R$ have some probability of occuring in the 
environment. 
\\\\
Consider that transition function has complete information on the dynamics of the 
environment. For this to be true, $\forall s \in S$, $s$ has a history of all seqeuences 
possible from that state. The probability of the next state $s_{t+1}$ only depends 
on $s_{t}$ and $a_s$. The past history of states and action transitions is not needed.
This condition is the \textit{Markov property}.
\subsection{Policy Gradient Methods}
\subsection{PPO}
\subsection{Recurrent Policy}
\section{Meta Learning}
\subsection{Few shot learning}
