\chapter{Background}
\label{ch1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\minitoc 

\section{Artificial Neural Networks}
Artificial neural networks (ANNs) are non-linear computational models that approximate 
a target function   $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, where $n$ and $m$ are 
integers \cite{Bis:06}. Given a set $X = \{ (\mathbf{x}_1, \mathbf{y}_1), \ldots, (\mathbf{x}_N,
 \mathbf{y}_N) \} \subset \R^n \times \R^m$ of input-output pairs of size $N$ the model is 
 trained to approximate $f$ such that $f(\mathbf{x}_i) = \mathbf{y}_i \forall i \in \{1, 2, 
 \ldots, N\}$. By the Universal Approximation theorem, a neural network's approximation of a 
 continuous function is theoretically guaranteed to be as precise as it needs to be given the 
 network has at least one hidden layer with some finite number of nodes \cite{HorStiWhi:89}. 

\subsection{Starting from linear regression}

Consider the problem of predicting the value of one or more continuous target variables 
$\mathbf{t} \subset \R^m$ provided a D-dimensional vector $\mathbf{x}$ of input variables, or 
what is called regression. Given a training set consisting of $N$ observations 
$\{\mathbf{x}_n\}_{n=1}^N$ and values $\{\mathbf{t}_n\}_{n=1}^N$, the objective is to predict 
the output value for any input vector $\mathbf{x}$ as close as possible to the provided target 
variable $\mathbf{t}$. 
\\\\
One approach is linear regression, or a linear combination over the input variables 
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = w_0 + w_1x_1 + \ldots + w_Dx_D
\end{equation}
where $\mathbf{x}_n$ has $D$ dimensions, $\mathbf{x}_n = (x_1, \ldots, x_D)^T$ and $w \in \R^{D+1}$ 
represents the parameters of the function, $w = (w_0, \ldots, w_D)$ and $D$ is extended to $D+1$ 
for the bias weight $w_0$. 
\\\\
As is, this regression function is limited to being a linear function over the input vector 
$\mathbf{x}_n$. Non-linear basis functions $\phi$ on the input variables make the function 
$y\left(\mathbf{x}_n, \mathbf{w}\right)$ non-linear for an input $\mathbf{x}_n$:
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = w_0 + \sum_{i=1}^{D} w_i \phi_i\left(x_i\right)
\end{equation}
This equation can be simplified further if we define a useful basis function for the bias
$\phi_0 (\mathbf{x}) =1$ such that
\begin{equation} 
    y(\mathbf{x}_n,\mathbf{w}) = \sum_{i=0}^{D} w_i \phi_i\left(x_i\right)
\end{equation}
Despite producing non linear outputs over the input $\mathbf{x}$ this is \textit{linear
regression} because it is linear with respect to $\mathbf{w}$.

\subsection{Constructing neural networks}

Basic ANNs can be seen as an extension to linear regression where the basis functions become 
parameterized. The basis functions continue to be non-linear functions over the linear 
combination of the input, but now the output of the basis function is dependent on the 
learned coefficients $\set{w_j}$. In this construction, basis functions are known as 
\textit{activation} functions $h$ in the context of neural networks. 
\\\\
We start by rewriting equation 1.2 as a linear combinations over the input variables to 
produce $a$ or the \textit{activation}. 
\begin{equation}
    a = \sum_{i=1}^{D} w_{i}x_i + w_{0} \phi\left(x_i\right)
\end{equation}
The value $a$  is transformed using a non-linear activation function $h$. This 
transformation produces $z$ and is referred to as a \textit{hidden unit}. 
\begin{equation}
    z = h\left(a\right)
\end{equation}
The coefficients $\set{w_j}$ parameterizing this non-linear transformation
are referred to as a \textit{layer}.
\\\\
An ANN has a minimum of two layers - an input layer and output layer. 
However, ANNs are not limited to 2 layers.
ANNs can have $l$ many layers where $l \in [2, +\infty)$. 
Networks with $>2$ layers are referred to as \textit{deep neural networks}.
For the purposes of the background, we will continue with the simple 2-layer case to
establish preliminaries.
\\\\
The input layer operates on an input $(x_1, \ldots, x_D)$ to produce \textit{activations}
$a_j = (a_1, \ldots, a_M)$, where $M$ denotes the number of parameters $\set{w_j}$ in the 
input layer. 
The parameters for the input layer are represented with a superscript (1) and the parameters 
for the output layer will be represented with a superscript (2).
\begin{equation}
    a_j = \sum_{i=1}^{D} w_{ji}^{(1)}x_i + w_{j0}^{(1)}
\end{equation}
The activations are passed through a non-linear activation $h$
\begin{equation}
    z_j = h\left(a_{j}\right)
\end{equation}
The output layer then transforms the hidden units $z_j$ to produce output unit activations 
$a_k$ where $k \in (1, \ldots, K)$ and $K$ is the number of outputs expected for this problem
(i.e., appropriate to the target variable $\mathbf{t}_i$ for $\mathbf{x}_i$).
\begin{equation}
     a_k = \sum_{j=1}^{M} w_{kj}^{(2)}z_j + w_{k0}^{(2)} 
\end{equation}
The activation $a_k$ is transformed by a different non-linear activation function that is 
appropriate for $K$. Here this activation function is represented as $\sigma$. A common choice 
of activation function $h$ for non-output layers is the rectified linear unit 
$h(a) = \min(0, a)$. A common choice of activation function for $\sigma$ is the sigmoid 
function $\sigma(a) = \frac{1}{1 + e^{-a}}$ for classification problems and the identity $y_k = a_k$ 
for simple regression problems.
We now present the equation for a \textit{feed-forward} pass through a 2-layer ANN.
\begin{equation}
    y_{k}(\mathbf{x}_n, \mathbf{w}) =  \sigma \left( \sum_{j=1}^{M}  w_{kj}^{(2)}h \left( \sum_{i=1}^{D} w_{ji}^{(1)} + w_{j0}^{(1)} \phi\left(x_i\right)\right) + w_{k0}^{(2)} \right)
\end{equation}
A neural network then is a non-linear function over an input $\mathbf{x_n}$ to an output $y_k$ 
that seeks to approximate $\mathbf{t_n}$ and is controlled by a set of adaptable parameters
$\mathbf{w}$.
\subsection{Training a neural network}
The goal of learning for a neural network is to optimize the parameters of the network such 
that the loss function $E(X, \mathbf{w})$ takes the lowest value. Continuing with the 
previous example for regression, we look at the sum-of-squares error function 
\begin{equation}
    E(X, \mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N} \norm{y\left(\mathbf{x}_n, \mathbf{w}  \right) - \mathbf{t}_n}^2
\end{equation}
There is typically not an analytical solution and iterative procedures are used to 
minimize the loss function $E$. The steps taken are
\begin{equation}
    \mathbf{w}^{(\tau+1)} = \mathbf{w^{(\tau)}} + \Delta \mathbf{w}^{(\tau)}
\end{equation} where $\tau$ is the iteration step. An approach for the weight update step 
with $\Delta \mathbf{w}^{(\tau)}$ is to use the gradient of $E(X, \mathbf{w})$ with 
respect to the parameters $\mathbf{w}$. The weights are updated in the direction of 
steepest error function decrease or in the $- \nabla E(X, \mathbf{w})$ direction.
\begin{equation}
     \mathbf{w}^{(\tau+1)} = \mathbf{w^{(\tau)}} + \alpha \nabla E\left( X, \mathbf{w}^{(\tau)} \right)
\end{equation} where $\alpha > 0$ is the learning rate controlling the size of update step 
taken. This iterative procedure is called \textit{gradient descent optimization} 
\cite{RumHinWil:86}. 
\subsection{Error function derivatives}
The gradient $\nabla E(X, \mathbf{w})$ is calculated with respect to every 
$w \in \mathbf{w}$, for all $\mathbf{x}_n \in X$. 
\\\\
We start with one input pattern $\mathbf{x_n}$ and rewrite the error function as 
\begin{equation}
    \begin{split}
    E_n &= \frac{1}{2} \left(y(\mathbf{x_n}, \mathbf{w}) - \mathbf{t_n} \right)^2  \\
     &= \frac{1}{2} \sum_{j=1} (w_{kj}z_{j} - t_{nk})^2
    \end{split}
\end{equation} 
The calculation starts with the gradient of $E_n$ with respect to each $w_{kj}$ in 
the output layer (2) then continues backwards to layer (1) for $w_{ji}$. 
This method can extend to $l$-layer networks where $l = (1, \ldots, L)$ and 
$L \subseteq \R$.
\\\\
Observe that
\begin{equation}
    \frac{\partial E_n}{\partial w_{kj}} = \frac{\partial E_n}{\partial a_k} \\
    \frac{\partial a_k}{\partial w_{kj}}
\end{equation}
We start by calculating the partial derivative of $E_n$ with respect to the 
activation $a_k$. Recall that $a_k = \sum_{k} w_{kj}{z_j}$. By the chain rule:
\begin{equation}
    \begin{split}
    \frac{\partial E_n}{\partial a_k} &= (h(a_k) - t_{nk})h'(a_k) \\
    &= h'(a_k)(\hat{y}_n - t_{nk})
    \end{split}
\end{equation}
We introduce a new notation to call this partial derivative an \textit{error}
\begin{equation}
    \delta_{k} \equiv \frac{\partial E_n}{\partial a_k} 
\end{equation}
Next we calculate the partial derivate of $a_k$ with respect to $w_{kj}$
\begin{equation}
    \begin{split}
    \frac{\partial a_k}{\partial w_{kj}} &= \frac{\partial}{\partial w_{kj}} \left( \sum_{k} w_{kj}z_{k} \right) \\
    &= z_{k}
    \end{split}
\end{equation} 
With which we can write
\begin{equation}
    \frac{\partial{E_n}}{\partial w_{kj}} = \delta_{k}z_{k}
\end{equation}
The procedure will continue in the same way for the remainder of the layers and their 
units, where we calculate the errors $\delta$ for the units in the layer and multiply 
error of that unit by its activation $z$. For layer (1) (input layer) we need to calculate
\begin{equation}
    \frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j} \\
    \frac{\partial a_j}{\partial w_{ji}}
\end{equation}
starting with $\delta_{j}$ or $\frac{\partial E_{n}}{\partial a_{j}}$. 
Observe that 
\begin{equation}
    \frac{\partial E_n}{\partial a_j} = \sum_{k} \\
    \frac{\partial E_n}{\partial a_{k}} \frac{\partial a_{k}}{\partial a_{j}}
\end{equation} where k is the number of outputs (here, k=1 for the continued
regression example).
\\\\
We calculated $\frac{\partial E_{n}}{\partial a_{k}}$ above. Continue with 
\begin{equation}
    \begin{split}
    \frac{\partial a_k}{\partial a_{j}} &= \frac{\partial}{\partial a_{j}} \left( \sum_{k} w_{kj}h(a_{j}) \right) \\
    &= h'(a_{j})w_{kj}
    \end{split}
\end{equation} 
We can finish calculating the error $\delta_{j}$ for equation 1.18
\begin{equation}
    \begin{split}
    \frac{\partial E_n}{\partial a_k} \frac{\partial a_k}{\partial a_{j}} &= h'(a_k)(\hat{y}_n - t_{nk}) h'(a_{j}) w_{kj} \\
    &= \frac{\partial E_n}{\partial a_j} \\
    & = \delta_{j}
    \end{split}
\end{equation} 
Thus we obtain the \textit{backpropagation} formula
\begin{equation}
    \delta_{j} = h'(a_{j}) \sum_{k} w_{kj} \delta_{k}
\end{equation} where the error for a unit $j$ is the result of backpropagating the errors 
in the units later in the network.
\\\\
Calculating the gradient is backpropgating the errors. As we have seen, this
procedure begins with a forward propagation of the input vectors $x_{n}$ to
calculate the activations of all units. Then, it involves calculating the 
errors $\delta_{k}$ in the output layer. Using $\delta_{k}$ we can calculate
$\delta_{j}$ for the hidden units in previous layers. With all errors 
$\delta$, the gradient is calculated by multiplying the error by the 
activations $a$ transformed by their non-linear function $h$ where $h(a) = z$.
\\\\
\subsection{Recurrent Neural Networks}
ANNs can be constructed as \textit{directed graphs}, formally defined as $G = (V, E)$ where 
$V$ is the set of vertices $\set{v_1, \ldots, v_n}$ and $E$ is the set of edges 
$\set{(u,v) | u, v \in V}$. We show neural networks are directed becuause the edges are a 
set of ordered pairs. In comparison, an undirected graph would have edges 
$\set{ \{ u,v \} | u, v \in V}$.
In terms appropriate to neural networks, $V$ corresponds to our hidden units $\set{z}$ 
and output units and $E$ corresponds to the parameters $\set{w}$.
\\\\
The 2-layer network we constructed above was a \textit{directed acyclic graph}.
$G \text{ is acyclic if } \forall v \in V, \text{ there does not exist a cycle
containing } v$. This means that for $\forall (u,v) \in E \text{, } u \neq v$.
\\\\
ANNs can contain cycles however. A type of neural network that contains cycles is a 
\textit{recurrent neural network} (RNN). An RNN is recurrent in that its output is 
part of forward propagation in the next timestep. 

% In this view, the nodes of a graph (called "neurons") are non-linear functions that 
% operate on the inputs to it. The edges feeding into a node contain parameters that are multiplied against
% In neural networks, the directed graph is composed of so called "layers" which are parameterized non-linear functions that applies to the input to the network and then to the output of the preceding layer. 
% More specifically, the graph's nodes (also called "neurons") are non-linear "activation" functions and the edges are the parameters (also called "weights") of the network.
% An input x, which may be denoted as (x1, ..., xn) is multiplied by the parameters (w1, ..., wn)^T and added a bias term b to output z. 
% This is passed to a non-linear "activation" function in the neuron or node a(z) to produce y. 
% This affine function comes out to be y = a(z) = w^T . x + b. It is often represented as a sum. E_1_n (wi .xi + b).
% As mentioned earlier, non-linearity is introduced into the network with activation functions that are applied to the immediately preceding linear transformation. A common activation function due to its breadth of applicable use cases is the rectified linear unit, a piecewise linear function that is linear for positive inputs and 0 for negative inputs: f(x) = max(0,x). Another common activation function is the sigmoid function represented as: f(x) = 1 / 1 + e^-x. It bounds the input x to (0, 1). This makes it a common choice for tasks where the output is used to represent a probability. 

\subsection{Vanishing Gradient Problem}
\section{Reinforcement Learning}
\subsection{Markov Decision Processes}
\subsection{Policy Gradient Methods}
\subsection{PPO}
\section{Meta Learning}