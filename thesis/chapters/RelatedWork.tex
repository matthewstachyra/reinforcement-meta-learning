\chapter{Related work}
\label{ch2}

\minitoc 
The REML algorithm proposed in this thesis is deep reinforcement 
learning for meta-learning. 
Related work includes the research 
where reinforcement learning is applied to meta-learning, and more
generally, popular meta-learning algorithms in recent years. 
REML is
unlike most meta-learning research in its use of RL as the meta-learning agent,
rather than adapting a meta-learning agent to RL. Algorithms that have used
RL as the meta-learning agent are typically limited to RL tasks (CITE). REML is 
unique in adapting parameters with RL for non-RL tasks. For this reason, REML is 
\textit{Reinforcement} Meta-Learning and not Meta Reinforcement Learning. 
\\\\ 
REML has parallels to neural architecture search (NAS) in searching a space for
a configuration to build a neural network with. The difference is that the configuration
is hyperparameters in NAS versus parameters in REML. Neural architecture search with
reinforcement learning is a specific method similar its use of in the use of an RL learner
to generate networks where the loss of these generated networks is the reward 
signal to the RL learner \cite{ZophQuoc:16}. The authors represent the model description 
as a variable length string that is generated by an RNN. 
\\\\
Model Agnostic Meta-Learning (MAML) shares with REML the ability to generalize to different
learning tasks including regression, classification, and reinforcement learning 
\cite{FinAbbLev:17}. A notable difference is MAML uses a single set of parameters that
it adapts to new tasks and calculates the Hessian to update the global parameters with
information on how these parameters changed for each new task. The intent was to enable 
the architecture to adapt with few gradient steps, to multiple tasks. 



\section{Title section 2.1}



\subsection{If needed}



\subsection{If needed}



\section{Title section 2.2}

\subsection{If needed}


\subsection{If needed}


\section{Title section 2.3}


\subsection{If needed}



\subsection{If needed}

