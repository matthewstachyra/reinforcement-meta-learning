{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import gymnasium\n",
    "from typing import (\n",
    "    Type,\n",
    "    List,\n",
    "    Tuple,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See if there is transfer from learning regression models from a couple different sine curves to yet a different sine curve.\n",
    "- We know there is transfer is (1) it takes less to train and (2) if the model chooses to reuse layers rather than create new ones.\n",
    "- We first train regression models without the meta-learner (because the 'meta' ability cannot do anything without pre-trained weights).\n",
    "- The meta-learner will then output actions to build a model for unseen data.\n",
    "- Actions will be continuous, a vector of probabilities corresponding to which layer to add from the layer pool. One of these probabilities corresponds to adding no layer and just training the architecture as it is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 41,\n",
       " 'device': 'cuda',\n",
       " 'pretrain': False,\n",
       " 'episodic': False,\n",
       " 'epochs': 1,\n",
       " 'timesteps': 500,\n",
       " 'n_x': 100,\n",
       " 'n_tasks': 6,\n",
       " 'in_features': 1,\n",
       " 'out_features': 1,\n",
       " 'n_pool_hidden_layers': 18,\n",
       " 'n_hidden_layers_per_network': 3,\n",
       " 'n_layers_per_network': 5,\n",
       " 'n_nodes_per_layer': 32,\n",
       " 'pool_layer_type': torch.nn.modules.linear.Linear,\n",
       " 'batch_size': 100,\n",
       " 'learning_rate': 0.05,\n",
       " 'action_cache_size': 5,\n",
       " 'num_workers': 0,\n",
       " 'loss_fn': MSELoss(),\n",
       " 'sb3_model': 'RecurrentPPO',\n",
       " 'sb3_policy': 'MlpLstmPolicy',\n",
       " 'log_dir': 'wandb',\n",
       " 'action_space_shape': (32,)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_config = {\n",
    "    'seed' : 41,\n",
    "    'device' : 'cuda',\n",
    "    'pretrain' : False,\n",
    "    'episodic' : False,\n",
    "    'epochs' : 1,\n",
    "    'timesteps' : 500,\n",
    "    'n_x' : 100,\n",
    "    'n_tasks' : 6,\n",
    "    'in_features' : 1,\n",
    "    'out_features' : 1,\n",
    "    'n_pool_hidden_layers' : 10,\n",
    "    'n_hidden_layers_per_network' : 3,\n",
    "    'n_layers_per_network' : 5,\n",
    "    'n_nodes_per_layer' : 32,\n",
    "    'pool_layer_type' : torch.nn.Linear,\n",
    "    'batch_size' : 100,\n",
    "    'learning_rate' : 0.05,\n",
    "    'action_cache_size' : 5,\n",
    "    'num_workers' : 0,\n",
    "    'loss_fn' : torch.nn.MSELoss(),\n",
    "    'sb3_model' : 'RecurrentPPO',\n",
    "    'sb3_policy' : 'MlpLstmPolicy',\n",
    "    'log_dir' : 'wandb',\n",
    "    }\n",
    "config = default_config\n",
    "config['n_pool_hidden_layers'] = config['n_tasks'] * config['n_hidden_layers_per_network']\n",
    "config['action_space_shape'] = (config['n_nodes_per_layer'], )\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmattstachyra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\stach\\Documents\\masters-thesis\\src\\bounded-continuous\\wandb\\run-20231120_174512-06rtubmj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/06rtubmj' target=\"_blank\">volcanic-hill-205</a></strong> to <a href='https://wandb.ai/mattstachyra/reinforcement-meta-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mattstachyra/reinforcement-meta-learning' target=\"_blank\">https://wandb.ai/mattstachyra/reinforcement-meta-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/06rtubmj' target=\"_blank\">https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/06rtubmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/06rtubmj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x27b430253c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project='reinforcement-meta-learning',\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Meta-Learning (REML) / \"Learning to Learn by Gradient Descent as a Markov Decision Process\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sinusoidal curve regression as in MAML 2018 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tasks created.\n"
     ]
    }
   ],
   "source": [
    "lower_bound = torch.tensor(-5).float()\n",
    "upper_bound = torch.tensor(5).float()\n",
    "X = np.linspace(lower_bound, upper_bound, config['n_x'])\n",
    "amplitude_range = torch.tensor([0.1, 5.0]).float()\n",
    "phase_range = torch.tensor([0, math.pi]).float()\n",
    "amps = torch.from_numpy(np.linspace(amplitude_range[0], amplitude_range[1], config['n_tasks'])).float()\n",
    "phases = torch.from_numpy(np.linspace(phase_range[0], phase_range[1], config['n_tasks'])).float()\n",
    "tasks_data = torch.tensor(np.array([ \n",
    "        X\n",
    "        for _ in range(config['n_tasks'])\n",
    "        ])).float()\n",
    "tasks_targets = torch.tensor(np.array([\n",
    "        [((a * np.sin(x)) + p).float()\n",
    "        for x in X] \n",
    "        for a, p in zip(amps, phases)\n",
    "        ])).float()\n",
    "tasks_info = [\n",
    "        {'i' : i, \n",
    "         'amp' : a, \n",
    "         'phase_shift' : p, \n",
    "         'lower_bound' : lower_bound, \n",
    "         'upper_bound' : upper_bound, \n",
    "         'amplitude_range_lower_bound' : amplitude_range[0], \n",
    "         'amplitude_range_upper_bound' : amplitude_range[1], \n",
    "         'phase_range_lower_bound' : phase_range[0],\n",
    "         'phase_range_lower_bound' : phase_range[1]}\n",
    "        for i, (a, p) in enumerate(zip(amps, phases))\n",
    "]\n",
    "print(f'[INFO] Tasks created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 100])\n",
      "torch.float32\n",
      "torch.Size([6, 100])\n",
      "torch.float32\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(tasks_data.shape)\n",
    "print(tasks_data.dtype)\n",
    "print(tasks_targets.shape)\n",
    "print(tasks_targets.dtype)\n",
    "print(len(tasks_info))\n",
    "print(len(tasks_info[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(config['in_features'], 32),  \n",
    "            torch.nn.Linear(32, 32), \n",
    "            torch.nn.Linear(32, 32),  \n",
    "            torch.nn.Linear(32, 32),  \n",
    "            torch.nn.Linear(32, config['out_features'])  \n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = torch.nn.functional.relu(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerNetworkAction(Enum):\n",
    "    TRAIN = 0\n",
    "    ADD = 1\n",
    "    ERROR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerNetworkTask(Dataset):\n",
    "    def __init__(self, data, targets, info):\n",
    "        self.data = data \n",
    "        self.targets = targets\n",
    "        self.info = info\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.data) == config['n_x'], '[ERROR] Length should be the same as n_x.'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert self.data[index].dtype == torch.float32, f'[ERROR] Expected type torch.float32, got type: {self.data[index].dtype}'\n",
    "        assert self.targets[index].dtype == torch.float32, f'[ERROR] Expected type torch.float32, got type: {self.targets[index].dtype}'\n",
    "        sample = {\n",
    "            'x' : self.data[index],\n",
    "            'y' : self.targets[index],\n",
    "            'info' : self.info\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'[INFO] InnerNetworkTask(data={self.data}, targets={self.targets}, info={self.info})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_and_gradients(layers):\n",
    "    num_layers = len(layers)\n",
    "    max_num_hidden_layers = config['n_hidden_layers_per_network']\n",
    "    num_hidden_layers = num_layers - 2\n",
    "    hidden_layers = layers[1:-1]\n",
    "    params = [layer.weight.detach() for layer in hidden_layers]\n",
    "    gradients = [layer.weight.grad for layer in hidden_layers]\n",
    "    if num_hidden_layers < config['n_hidden_layers_per_network']:\n",
    "        zero_pad = [torch.zeros((config['n_nodes_per_layer'], config['n_nodes_per_layer']), dtype=torch.float32)] * (max_num_hidden_layers - num_hidden_layers)\n",
    "        zero_pad_tensor = torch.stack(zero_pad)\n",
    "        if len(params) > 0 and len(gradients) > 0:\n",
    "            params = torch.stack(params)\n",
    "            gradients = torch.stack(gradients)\n",
    "            params = torch.cat((params, zero_pad_tensor))\n",
    "            gradients = torch.cat((gradients, zero_pad_tensor)) \n",
    "        else:\n",
    "            params = zero_pad_tensor\n",
    "            gradients = zero_pad_tensor\n",
    "    else:\n",
    "        params = torch.stack(params)\n",
    "        gradients = torch.stack(gradients)\n",
    "    assert params.shape==(max_num_hidden_layers, config['n_nodes_per_layer'], config['n_nodes_per_layer']), f\"[ERROR] Expected params shape={max_num_hidden_layers, config['n_nodes_per_layer'], config['n_nodes_per_layer']}, got {params.shape}\"\n",
    "    return params.view(-1), gradients.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_space(latent_space):\n",
    "    flattened_latent_space = latent_space.view(-1)\n",
    "    flattened_size = flattened_latent_space.numel()\n",
    "    target_size = config['batch_size'] * config['n_nodes_per_layer']\n",
    "    if flattened_size < target_size:\n",
    "        num_elements_to_pad = target_size - flattened_size\n",
    "        padding_tensor = torch.zeros(num_elements_to_pad)\n",
    "        padded_tensor = torch.cat((flattened_latent_space, padding_tensor), dim=0)\n",
    "        return padded_tensor\n",
    "    else:\n",
    "        return flattened_latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerNetwork(gymnasium.Env, torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                task: InnerNetworkTask,\n",
    "                epoch: int=0,\n",
    "                in_features: int=config['in_features'],\n",
    "                out_features: int=config['out_features'],\n",
    "                learning_rate: float=config['learning_rate'],\n",
    "                batch_size: int=config['batch_size'],\n",
    "                action_cache_size: float=config['action_cache_size'],\n",
    "                num_workers: int=config['num_workers'],\n",
    "                shuffle: bool=True,\n",
    "                ):\n",
    "        super(InnerNetwork, self).__init__()\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.task = task\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.action_cache_size = action_cache_size\n",
    "        self.num_workers = num_workers\n",
    "        self.prev = defaultdict(lambda: None)\n",
    "        self.curr = defaultdict(lambda: None)\n",
    "        self.data_loader = DataLoader(task, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "        self.data_iter = iter(self.data_loader)\n",
    "        self.initial_layer = torch.nn.Linear(in_features, config['n_nodes_per_layer'])\n",
    "        self.final_layer = torch.nn.Linear(config['n_nodes_per_layer'], out_features)\n",
    "        self.layers = torch.nn.ModuleList([self.initial_layer, self.final_layer]) \n",
    "        [torch.nn.init.xavier_uniform_(layer.weight) for layer in self.layers]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.opt = torch.optim.Adam(self.layers.parameters(), lr=self.learning_rate)\n",
    "        self.actions_taken = [InnerNetworkAction.TRAIN] * config['action_cache_size']\n",
    "        self.timestep = 0\n",
    "        self.cum_loss = 0\n",
    "        self.cum_reward = 0\n",
    "        self.errors = 0\n",
    "        self.rewards_per_episode = []\n",
    "        self.steps_per_episode = []\n",
    "        self.eval()\n",
    "        self.next_batch()\n",
    "        self.run_inner_network()\n",
    "        self.observation_space = gymnasium.spaces.box.Box(low=float('-inf'), high=float('inf'), shape=self.build_state().shape)\n",
    "        self.action_space = gymnasium.spaces.box.Box(low=-1, high=1, shape=config['action_space_shape'])\n",
    "        self.termination = False\n",
    "\n",
    "    def step(self, action: np.int64) -> Tuple[torch.Tensor, float, bool, dict]: \n",
    "        assert action.shape == (), f'[ERROR] Expected action shape () for scalar {self.action_space.n}, got: {action.shape}'\n",
    "        assert action.dtype == np.int64, f'[ERROR] Expected np.int64 dtype, got: {action.dtype}'\n",
    "\n",
    "        self.timestep += 1\n",
    "        self.train()\n",
    "        self.next_batch()\n",
    "        self.update_inner_network()\n",
    "        self.run_inner_network()\n",
    "        s_prime = self.build_state()\n",
    "        reward = self.reward()\n",
    "        self.update_internal_trackers()\n",
    "        self.log()\n",
    "\n",
    "        return (\n",
    "            s_prime,\n",
    "            reward, \n",
    "            self.termination,\n",
    "            False,\n",
    "            {}\n",
    "        )\n",
    "    \n",
    "    def update_inner_network(self, action: np.int64) -> None:\n",
    "        self.curr['action_type'] = InnerNetworkAction.ADD if action < self.layer_pool.size else InnerNetworkAction.TRAIN\n",
    "        if self.curr['action_type']==InnerNetworkAction.ADD:\n",
    "            new_layer = torch.nn.Linear(in_features=config['n_nodes_per_layer'], out_features=['n_nodes_per_layer'])\n",
    "            linear_layer.weight.data = torch.tensor(action, dtype=torch.float32)\n",
    "            final_layer = self.layers.pop(-1) \n",
    "            final_layer_index = self.layers_pool_indices.pop(-1)\n",
    "            self.layers.append(new_layer)\n",
    "            self.layers.append(final_layer) \n",
    "        \n",
    "        self.curr['action_type'] = InnerNetworkAction.ADD if action < self.layer_pool.size else InnerNetworkAction.TRAIN\n",
    "        self.actions_taken.append(self.curr['action_type']) \n",
    "        self.termination = True if config['episodic'] and len(self.env.layers)==config['n_layers_per_network'] else False\n",
    "            \n",
    "    def next_batch(self, throw_exception=False) -> None:\n",
    "        self.prev = self.curr\n",
    "        self.curr = defaultdict(lambda: None)\n",
    "\n",
    "        if (throw_exception):\n",
    "            batch = next(self.data_iter)\n",
    "            self.curr['x'] = batch['x'].view(-1, 1)\n",
    "            self.curr['y'] = batch['y'].view(-1, 1)\n",
    "            self.curr['info'] = batch['info']\n",
    "        else: \n",
    "            try:\n",
    "                batch = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_loader = DataLoader(self.task, batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)\n",
    "                self.data_iter = iter(self.data_loader)\n",
    "                batch = next(self.data_iter)\n",
    "            finally:\n",
    "                self.curr['x'] = batch['x'].view(-1 ,1)\n",
    "                self.curr['y'] = batch['y'].view(-1, 1)\n",
    "                self.curr['info'] = batch['info']\n",
    "    \n",
    "    def run_inner_network(self) -> None: \n",
    "        if self.training:\n",
    "            self.train()\n",
    "            if self.curr['action_type']==InnerNetworkAction.ADD and len(self.layers) < config['n_layers_per_network']:\n",
    "                self.opt = torch.optim.Adam(self.layers.parameters(), lr=self.learning_rate) \n",
    "            self.opt.zero_grad()\n",
    "            self.forward(self.curr['x'])\n",
    "            loss = self.curr['loss']\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "        else:\n",
    "            self.forward(self.curr['x'])\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        for i in range(len(self.layers) - 1): \n",
    "            x = torch.nn.functional.relu(self.layers[i](x))\n",
    "        self.curr['latent_space'] = x\n",
    "        self.curr['y_hat'] = self.layers[-1](x) \n",
    "        y = self.curr['y']\n",
    "        self.curr['loss'] = self.loss_fn(y, self.curr['y_hat'])\n",
    "        return self.curr['y_hat']\n",
    "    \n",
    "    def build_state(self) -> np.ndarray:\n",
    "        task_info = torch.tensor([self.task.info['amp'], self.task.info['phase_shift']]).squeeze()\n",
    "        loss = torch.Tensor([self.curr['loss']])\n",
    "        latent_space = get_latent_space(self.curr['latent_space'])\n",
    "        _, gradients = get_params_and_gradients(self.layers)\n",
    "        h = torch.tensor([action_enum.value for action_enum in self.actions_taken[-self.action_cache_size:]])\n",
    "        \n",
    "        return torch.concat((\n",
    "            task_info,\n",
    "            latent_space,\n",
    "            loss,\n",
    "            gradients,\n",
    "            h\n",
    "        ), dim=0).detach().numpy()\n",
    "    \n",
    "    def reward(self) -> torch.Tensor:\n",
    "        prev_loss = self.prev['loss'] or None\n",
    "        curr_loss = self.curr['loss']\n",
    "        loss_delta = prev_loss - curr_loss if prev_loss is not None else curr_loss\n",
    "        if (self.curr['action_type'] == InnerNetworkAction.ERROR):\n",
    "            reward = -5\n",
    "        elif (self.curr['action_type'] == InnerNetworkAction.TRAIN):\n",
    "            reward = 1 if loss_delta > 0 else -1\n",
    "        else:\n",
    "            reward = 1 * (0.99 ** self.timestep) if loss_delta > 0 else -1 * (0.99 ** self.timestep)\n",
    "        self.curr['reward'] = reward\n",
    "        return reward\n",
    "\n",
    "    def update_internal_trackers(self) -> None:\n",
    "        self.cum_loss += self.curr['loss']\n",
    "        self.cum_reward += self.curr['reward']\n",
    "        if config['episodic'] and self.curr['action_type']==InnerNetworkAction.ERROR:\n",
    "            self.steps_per_episode.append(self.timestep)\n",
    "            self.rewards_per_episode.append(self.cum_reward)\n",
    "            self.errors += 1\n",
    "\n",
    "    def log(self):\n",
    "        # task_num = str(self.curr['info']['i'].item())\n",
    "        task_num = 0\n",
    "        if self.timestep%100==0 and not config['episodic']:\n",
    "            wandb.log({ f'running_loss_task{task_num}_per_100steps' : self.cum_loss})\n",
    "            wandb.log({ f'running_reward_task{task_num}_per_100steps' : self.cum_reward})\n",
    "            self.cum_loss = 0\n",
    "            self.cum_reward = 0\n",
    "        wandb.log({ f'action_types_task{task_num}_per_step' : wandb.Histogram(torch.tensor([e.value for e in self.actions_taken]))})\n",
    "        wandb.log({ f'num_layers_task{task_num}_per_step' : len(self.layers) })\n",
    "\n",
    "    def reset(self, seed=None) -> np.ndarray:\n",
    "        # print(f'[INFO] Reset at {self.timestep}')\n",
    "        self.timestep = 0\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        self.eval()\n",
    "        self.next_batch()\n",
    "        self.run_inner_network()\n",
    "        return self.build_state(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REML:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tasks: List[InnerNetworkTask],\n",
    "        model=config['sb3_model'],\n",
    "        policy=config['sb3_policy'],\n",
    "        epochs: int=config['epochs'],\n",
    "        timesteps: int=config['timesteps'],\n",
    "        device: str=config['device'],\n",
    "        log_dir: str=f\"./{config['log_dir']}/{config['sb3_model']}_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        ):\n",
    "        self.tasks = tasks\n",
    "        if config['sb3_model']=='PPO':\n",
    "            model = PPO\n",
    "        elif config['sb3_model']=='RecurrentPPO':\n",
    "            model = RecurrentPPO\n",
    "        elif config['sb3_model']=='DQN':\n",
    "            model = DQN\n",
    "        dummy_env = self.make_env(tasks[0])\n",
    "        self.model = model(policy, dummy_env, tensorboard_log=log_dir, n_epochs=5, batch_size=32, n_steps=32)\n",
    "        self.policy = policy\n",
    "        self.epochs = epochs\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device # TODO is to check whether cuda is used as assumed\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f'REML(model={self.model}, policy={self.policy})'\n",
    "    \n",
    "    def make_env(self, task, epoch=None) -> gymnasium.Env:\n",
    "        return gymnasium.wrappers.NormalizeObservation(InnerNetwork(task, epoch=epoch))\n",
    "\n",
    "    def train(self):\n",
    "        # wraps stablebaselines learn() so we call it n * m times\n",
    "        # n is the number of epochs where we run all m tasks\n",
    "        # we use the same policy, swapping out envs for the n tasks, m times. \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'[INFO] Epoch={epoch + 1}/{self.epochs}')\n",
    "            for i, task in enumerate(self.tasks): \n",
    "                self.task = task\n",
    "                print(f'[INFO] Task={i+1}/{len(self.tasks)}')\n",
    "\n",
    "                # each task gets its own network\n",
    "                self.env = self.make_env(self.task, epoch=epoch)\n",
    "                self.model.set_env(self.env)\n",
    "                self.model.learn(total_timesteps=self.timesteps)\n",
    "\n",
    "                # wandb \n",
    "                if config['episodic']:\n",
    "                    wandb.log({ f'average_reward_per_episode_task{i}_per_epoch' : sum(self.env.rewards_per_episode) / len(self.env.rewards_per_episode) }) \n",
    "                    wandb.log({ f'average_steps_per_episode_task{i}_per_epoch' : sum(self.env.steps_per_episode) / len(self.env.steps_per_episode) }) \n",
    "                    wandb.log({ f'errors_per_epoch_task{i}_per_epoch' : self.env.errors })\n",
    "\n",
    "                # sine curves\n",
    "                self.generate_sine_curve(epoch=epoch, task=i, image=True, title='training_sine_curves', args={'label' : f'task_{i}'})\n",
    "                plt.plot(self.task.data, self.task.targets, linestyle='--', label='ground truth')\n",
    "    \n",
    "    def evaluate_loss_curves(self, steps=100) -> dict:\n",
    "        # generates loss curve over 'steps' per task\n",
    "\n",
    "        lossperstep_bytask = defaultdict(lambda: [])\n",
    "\n",
    "        for task in self.tasks: \n",
    "            env = gymnasium.wrappers.NormalizeObservation(InnerNetwork(task, self.layer_pool))\n",
    "            self.model.set_env(env, force_reset=False)\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            while len(env.layers) < config['n_layers_per_network']:\n",
    "                action, _ = self.model.predict(obs)\n",
    "                obs, _, _, _, _ = env.step(action)\n",
    "\n",
    "            for _ in range(steps):\n",
    "                action, _ = self.model.predict(obs)\n",
    "                obs, _, _, _, _ = env.step(action)\n",
    "                env.next_batch()\n",
    "                yhats = env.forward(env.curr['x'])\n",
    "                loss = env.loss_fn(yhats, env.curr['y'] )\n",
    "                lossperstep_bytask[task].append(loss)\n",
    "\n",
    "        return lossperstep_bytask\n",
    "\n",
    "    def generate_sine_curve(self, env=None, data=None, epoch=None, task=None, image=False, new_figures=False, title=None, args=defaultdict()) -> List:\n",
    "        # generates sine curve after 'env.layers' is full, with option to set env, limit to \n",
    "        # subset of env data (for few shot evaluation), and to create png\n",
    "\n",
    "        if env is not None:\n",
    "            self.env = env\n",
    "            self.model.set_env(env, force_reset=False)\n",
    "\n",
    "        self.env.eval()\n",
    "        obs, _ = self.env.reset()\n",
    "        \n",
    "        while len(self.env.layers)!=config['n_layers_per_network']:\n",
    "            action, _ = self.model.predict(obs)\n",
    "            obs, _, _, _, _ = self.env.step(action)\n",
    "        \n",
    "        # if data is specified, wrap in new task\n",
    "        # if data is not specified, the iterator is used over set\n",
    "        if data is not None:\n",
    "            dataset = InnerNetworkTask(data=data[:, 0].clone(), targets=data[:, 1].clone(), info=self.task.info)\n",
    "        else: \n",
    "            dataset = self.task\n",
    "\n",
    "        xs = dataset.data.clone()\n",
    "        xs = xs.view(len(xs), 1)\n",
    "        for i in range(len(self.env.layers) - 1): \n",
    "            xs = torch.nn.functional.relu(self.env.layers[i](xs))\n",
    "        yhats = self.env.layers[-1](xs) \n",
    "\n",
    "        if new_figures:\n",
    "            plt.figure()\n",
    "        plot_title = title if title!=None else f'sine_curve_epoch_{epoch}_task_{task}' if epoch!=None and task!=None else 'sine_curve'\n",
    "        plot_path = f'{self.log_dir}/{plot_title}.png'  \n",
    "        plt.plot(dataset.data, [yhat.detach().numpy() for yhat in yhats], **args)\n",
    "        # plt.plot(dataset.data, dataset.targets, label='ground truth', linestyle='--')\n",
    "        plt.title(plot_title)\n",
    "        plt.legend()\n",
    "\n",
    "        if image:\n",
    "            plt.savefig(plot_path)\n",
    "            wandb.log({plot_title: wandb.Image(plot_path)})\n",
    "       \n",
    "        xs, yhats = dataset.data, [yhat.detach().numpy() for yhat in yhats]\n",
    "        return xs, yhats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [InnerNetworkTask(data=tasks_data[i], targets=tasks_targets[i], info=tasks_info[i]) for i in range(config['n_tasks'])]\n",
    "eval_task = random.choice(list(tasks))\n",
    "training_tasks = list(set(tasks) - {eval_task})\n",
    "reml = REML(tasks=training_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REML(model=<sb3_contrib.ppo_recurrent.ppo_recurrent.RecurrentPPO object at 0x0000027B49E04DC0>, policy=MlpLstmPolicy)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reml.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 41,\n",
       " 'device': 'cuda',\n",
       " 'pretrain': False,\n",
       " 'episodic': False,\n",
       " 'epochs': 1,\n",
       " 'timesteps': 500,\n",
       " 'n_x': 100,\n",
       " 'n_tasks': 6,\n",
       " 'in_features': 1,\n",
       " 'out_features': 1,\n",
       " 'n_pool_hidden_layers': 18,\n",
       " 'n_hidden_layers_per_network': 3,\n",
       " 'n_layers_per_network': 5,\n",
       " 'n_nodes_per_layer': 32,\n",
       " 'pool_layer_type': torch.nn.modules.linear.Linear,\n",
       " 'batch_size': 100,\n",
       " 'learning_rate': 0.05,\n",
       " 'action_cache_size': 5,\n",
       " 'num_workers': 0,\n",
       " 'loss_fn': MSELoss(),\n",
       " 'sb3_model': 'RecurrentPPO',\n",
       " 'sb3_policy': 'MlpLstmPolicy',\n",
       " 'log_dir': 'wandb',\n",
       " 'action_space_shape': (32,)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch=1/1\n",
      "[INFO] Task=1/5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stach\\Documents\\masters-thesis\\src\\bounded-continuous\\bounded-continuous-regression.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m reml\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmeta_\u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m'\u001b[39m\u001b[39msb3_model\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m reml\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave(path)\n",
      "\u001b[1;32mc:\\Users\\stach\\Documents\\masters-thesis\\src\\bounded-continuous\\bounded-continuous-regression.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_env(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask, epoch\u001b[39m=\u001b[39mepoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mset_env(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimesteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# wandb \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mepisodic\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\stach\\miniconda3\\envs\\masters-thesis\\lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:469\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    466\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    468\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 469\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    471\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    472\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stach\\miniconda3\\envs\\masters-thesis\\lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:254\u001b[0m, in \u001b[0;36mRecurrentPPO.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    252\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 254\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    258\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stach\\miniconda3\\envs\\masters-thesis\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\stach\\miniconda3\\envs\\masters-thesis\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\stach\\miniconda3\\envs\\masters-thesis\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\stach\\miniconda3\\envs\\masters-thesis\\lib\\site-packages\\gymnasium\\wrappers\\normalize.py:76\u001b[0m, in \u001b[0;36mNormalizeObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and normalizes the observation.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     obs, rews, terminateds, truncateds, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_vector_env:\n\u001b[0;32m     78\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(obs)\n",
      "\u001b[1;32mc:\\Users\\stach\\Documents\\masters-thesis\\src\\bounded-continuous\\bounded-continuous-regression.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: np\u001b[39m.\u001b[39mint64) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]: \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39massert\u001b[39;00m action\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[ERROR] Expected action shape () for scalar \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mn\u001b[39m}\u001b[39;00m\u001b[39m, got: \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39massert\u001b[39;00m action\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mint64, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[ERROR] Expected np.int64 dtype, got: \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stach/Documents/masters-thesis/src/bounded-continuous/bounded-continuous-regression.ipynb#X30sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestep \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "reml.train()\n",
    "path = f\"meta_{config['sb3_model']}_{datetime.datetime.now().strftime('%H-%M')}\"\n",
    "reml.model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### speed of training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loss for meta trained network per step\n",
    "reml = REML(layer_pool=pool, tasks=tasks)\n",
    "reml.model.load(path)\n",
    "meta_lossperstep_bytask = reml.evaluate_loss_curves()\n",
    "\n",
    "# get loss for vanilla network per step\n",
    "vanilla_lossperstep_bytask = {}\n",
    "for task, x, y in zip(tasks, tasks_data, tasks_targets):\n",
    "    model = RegressionModel()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    vanilla_lossperstep_bytask[task] = []\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(x.view(-1, 1))\n",
    "        loss = criterion(outputs, y.view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        vanilla_lossperstep_bytask[task].append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(zip(tasks_data, tasks_targets)):\n",
    "    task = tasks[i]\n",
    "    vanilla_label = f'Vanilla network: Task {i+1}' \n",
    "    meta_label = f'Meta-trained network: Task {i+1}' \n",
    "    meta_loss_vals = [val.detach().numpy() for val in meta_lossperstep_bytask[task]]\n",
    "    vanilla_loss_vals = [val.detach().numpy() for val in vanilla_lossperstep_bytask[task]]\n",
    "    plt.plot(range(100), vanilla_loss_vals, label=vanilla_label)\n",
    "    plt.plot(range(100), meta_loss_vals, label=meta_label)\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### few shot learning tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseen task\n",
    "# choose 5 and 10 datapoints on it, K = 5, K = 10\n",
    "k = 5\n",
    "k_pairs = torch.tensor(random.sample(list(zip(eval_task.data, eval_task.targets)), k))\n",
    "print(len(k_pairs))\n",
    "print(k_pairs)\n",
    "\n",
    "# network still recieves the same 100 x values {-5, ..., 5}\n",
    "# the difference is that a target value is only provided for 5 or 10 of these 100 values\n",
    "# start by generating the 'pre-update' curve \n",
    "# next pass k datapoints and do 1 gradient update step to get the curve\n",
    "# next do 9 more gradient update steps (total 10) to get next curve\n",
    "# plot 'pre-update', '1 grad step', '10 grad steps' curves together with the 'k_pairs' data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env with eval task\n",
    "reml = REML(layer_pool=pool, tasks=tasks)\n",
    "reml.model.load(path)\n",
    "env = reml.make_env(eval_task, pool)\n",
    "reml.model.set_env(env)\n",
    "reml.env = env\n",
    "reml.task = eval_task\n",
    "\n",
    "# ground truth\n",
    "plt.plot(eval_task.data, eval_task.targets, linestyle='--', label='ground truth')\n",
    "\n",
    "# pre-update\n",
    "reml.generate_sine_curve(env=env, args={'linestyle' : '--', 'label': 'pre-update'})\n",
    "\n",
    "# k points \n",
    "plt.scatter(k_pairs[:, 0], k_pairs[:, 1], marker='^', color='b', label='k points')\n",
    "\n",
    "# train for 1 grad step\n",
    "reml.env.batch_size = k\n",
    "obs, _ = reml.env.reset()\n",
    "for _ in range(1):\n",
    "    action, _ = reml.model.predict(obs)\n",
    "    obs, _, _, _, _ = reml.env.step(action)\n",
    "reml.generate_sine_curve(env=env, args={'label' :'1 grad step'})\n",
    "    \n",
    "# train for 10 grad steps\n",
    "obs, _ = reml.env.reset()\n",
    "for _ in range(10):\n",
    "    action, _ = reml.model.predict(obs)\n",
    "    obs, _, _, _, _ = reml.env.step(action)\n",
    "reml.generate_sine_curve(env=env, args={'label' : '10 grad steps'})\n",
    "plt.title('REML, k=5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "k_pairs = torch.tensor(random.sample(list(zip(eval_task.data, eval_task.targets)), k))\n",
    "print(len(k_pairs))\n",
    "print(k_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reml = REML(layer_pool=pool, tasks=tasks)\n",
    "reml.model.load(path)\n",
    "env = reml.make_env(eval_task)\n",
    "reml.model.set_env(env)\n",
    "reml.env = env\n",
    "reml.task = eval_task\n",
    "\n",
    "# ground truth\n",
    "plt.plot(eval_task.data, eval_task.targets, linestyle='--', label='ground truth')\n",
    "\n",
    "# pre-update\n",
    "reml.generate_sine_curve(env=env, args={'linestyle' : '--', 'label': 'pre-update'})\n",
    "\n",
    "# k points \n",
    "plt.scatter(k_pairs[:, 0], k_pairs[:, 1], marker='^', color='b', label='k points')\n",
    "\n",
    "# train for 1 grad step\n",
    "reml.env.batch_size = k\n",
    "obs, _ = reml.env.reset()\n",
    "for _ in range(1):\n",
    "    action, _ = reml.model.predict(obs)\n",
    "    obs, _, _, _, _ = reml.env.step(action)\n",
    "reml.generate_sine_curve(env=env, args={'label' :'1 grad step'})\n",
    "    \n",
    "# train for 10 grad steps\n",
    "obs, _ = reml.env.reset()\n",
    "for _ in range(10):\n",
    "    action, _ = reml.model.predict(obs)\n",
    "    obs, _, _, _, _ = reml.env.step(action)\n",
    "reml.generate_sine_curve(env=env, args={'label' : '10 grad steps'})\n",
    "plt.title('REML, k=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## episodic learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['episodic'] = True\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [InnerNetworkTask(data=tasks_data[i], targets=tasks_targets[i], info=tasks_info[i]) for i in range(config['n_tasks'])]\n",
    "eval_task = random.choice(list(tasks))\n",
    "training_tasks = list(set(tasks) - {eval_task})\n",
    "pool = LayerPool(layers=layers) if config['pretrain'] else LayerPool(layers=None)\n",
    "reml = REML(layer_pool=pool, tasks=training_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reml.train()\n",
    "path = f\"meta_episodic_{config['sb3_model']}_{datetime.datetime.now().strftime('%H-%M')}\"\n",
    "reml.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loss for meta trained network per step\n",
    "reml = REML(layer_pool=pool, tasks=tasks)\n",
    "reml.model.load(path)\n",
    "meta_lossperstep_bytask = reml.evaluate_loss_curves()\n",
    "\n",
    "# get loss for vanilla network per step\n",
    "vanilla_lossperstep_bytask = {}\n",
    "for task, x, y in zip(tasks, tasks_data, tasks_targets):\n",
    "    model = RegressionModel()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    model.train()\n",
    "    vanilla_lossperstep_bytask[task] = []\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(x.view(-1, 1))\n",
    "        loss = criterion(outputs, y.view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        vanilla_lossperstep_bytask[task].append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(zip(tasks_data, tasks_targets)):\n",
    "    task = tasks[i]\n",
    "    vanilla_label = f'Vanilla network: Task {i+1}' \n",
    "    meta_label = f'Meta-trained network: Task {i+1}' \n",
    "    meta_loss_vals = [val.detach().numpy() for val in meta_lossperstep_bytask[task]]\n",
    "    vanilla_loss_vals = [val.detach().numpy() for val in vanilla_lossperstep_bytask[task]]\n",
    "    plt.plot(range(100), vanilla_loss_vals, label=vanilla_label)\n",
    "    plt.plot(range(100), meta_loss_vals, label=meta_label)\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "k_pairs = torch.tensor(random.sample(list(zip(eval_task.data, eval_task.targets)), k))\n",
    "print(len(k_pairs))\n",
    "print(k_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env with eval task\n",
    "reml = REML(layer_pool=pool, tasks=tasks)\n",
    "reml.model.load(path)\n",
    "env = reml.make_env(eval_task, pool)\n",
    "reml.model.set_env(env)\n",
    "reml.env = env\n",
    "reml.task = eval_task\n",
    "\n",
    "# ground truth\n",
    "plt.plot(eval_task.data, eval_task.targets, linestyle='--', label='ground truth')\n",
    "\n",
    "# pre-update\n",
    "reml.generate_sine_curve(env=env, args={'linestyle' : '--', 'label': 'pre-update'})\n",
    "\n",
    "# k points \n",
    "plt.scatter(k_pairs[:, 0], k_pairs[:, 1], marker='^', color='b', label='k points')\n",
    "\n",
    "# train for 1 grad step\n",
    "reml.env.batch_size = k\n",
    "obs, _ = reml.env.reset()\n",
    "for _ in range(1):\n",
    "    action, _ = reml.model.predict(obs)\n",
    "    obs, _, _, _, _ = reml.env.step(action)\n",
    "reml.generate_sine_curve(env=env, args={'label' :'1 grad step'})\n",
    "    \n",
    "# train for 10 grad steps\n",
    "obs, _ = reml.env.reset()\n",
    "for _ in range(10):\n",
    "    action, _ = reml.model.predict(obs)\n",
    "    obs, _, _, _, _ = reml.env.step(action)\n",
    "reml.generate_sine_curve(env=env, args={'label' : '10 grad steps'})\n",
    "plt.title('REML, k=5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
