{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from enum import Enum\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Module\n",
    "import tqdm\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "from gym.utils.env_checker import check_env\n",
    "from typing import (\n",
    "    Type,\n",
    "    OrderedDict,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Callable,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "0.21.0\n"
     ]
    }
   ],
   "source": [
    "print(stable_baselines3.__version__)\n",
    "print(gym.__version__) # should be 0.21.0 to be compatible with stable_baselines3 1.8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED': 123,\n",
       " 'DEVICE': 'cpu',\n",
       " 'EPOCHS': 2,\n",
       " 'TIMESTEPS': 5000,\n",
       " 'N_X': 100,\n",
       " 'N_TASKS': 5,\n",
       " 'POOL_N_LAYERS': 100,\n",
       " 'N_NODES_PER_LAYER': 32,\n",
       " 'POOL_LAYER_TYPE': torch.nn.modules.linear.Linear,\n",
       " 'ACTION_SPACE_SHAPE': (3,),\n",
       " 'EPSILON': 0.1,\n",
       " 'BATCH_SIZE': 1,\n",
       " 'LEARNING_RATE': 0.05,\n",
       " 'ACTION_CACHE_SIZE': 5,\n",
       " 'NUM_WORKERS': 0,\n",
       " 'LOSS_FN': MSELoss(),\n",
       " 'SB3_MODEL': stable_baselines3.ppo.ppo.PPO,\n",
       " 'SB3_POLICY': 'MlpPolicy'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'SEED' :123,\n",
    "    'DEVICE' : 'cpu',\n",
    "    'EPOCHS' : 2,\n",
    "    'TIMESTEPS' : 5000,\n",
    "    'N_X' : 100,\n",
    "    'N_TASKS' : 5,\n",
    "    'POOL_N_LAYERS' : 100,\n",
    "    'N_NODES_PER_LAYER' : 32,\n",
    "    'POOL_LAYER_TYPE' : torch.nn.Linear,\n",
    "    'ACTION_SPACE_SHAPE' : (3,),\n",
    "    'EPSILON' : 0.1,\n",
    "    'BATCH_SIZE' : 1,\n",
    "    'LEARNING_RATE' : 0.05,\n",
    "    'ACTION_CACHE_SIZE' : 5,\n",
    "    'NUM_WORKERS' : 0,\n",
    "    'LOSS_FN' : torch.nn.MSELoss(),\n",
    "    'SB3_MODEL' : PPO,\n",
    "    'SB3_POLICY' : 'MlpPolicy',\n",
    "    }\n",
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Meta-Learning (REML) / \"Learning to Learn by Gradient Descent as a Markov Deicision Process\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layer pool\n",
    "- inner network -- composed of layers from layer pool\n",
    "- outer network (meta learner) -- responsible for parameters and hyperparameters of inner network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, \n",
    "                type: Type[torch.nn.Module]=config['POOL_LAYER_TYPE']):\n",
    "        self.type = type\n",
    "        self.params = type\n",
    "        self.used = False\n",
    "        self.times_used = 0\n",
    "\n",
    "class LayerPool:\n",
    "    # pool of uniform Layer objects each with the same type and shape\n",
    "    def __init__(self, \n",
    "                size: int=config['POOL_N_LAYERS'], \n",
    "                layer_type: Type[torch.nn.Module]=config['POOL_LAYER_TYPE'],\n",
    "                num_nodes_per_layer: int=config['N_NODES_PER_LAYER']):\n",
    "        self.size = size\n",
    "        self.layer_type = layer_type\n",
    "        self.num_nodes_per_layer = num_nodes_per_layer\n",
    "\n",
    "        # each layer that is used gets updated (i.e., their parameters change and the copy in \n",
    "        # this layer pool is updated), except for the first and last layers which are unique\n",
    "        # for each task\n",
    "        self.layers = {\n",
    "            i : Layer(\n",
    "                type=self.layer_type(in_features=num_nodes_per_layer, out_features=num_nodes_per_layer)\n",
    "                )\n",
    "            for i in range(size)}\n",
    "\n",
    "        [torch.nn.init.xavier_uniform_(layer.params.weight) for layer in self.layers.values()]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"LayerPool(size={self.size}, layer_type={config['POOL_LAYER_TYPE']}, num_nodes_per_layer={config['N_NODES_PER_LAYER']}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerNetworkAction(Enum):\n",
    "    UNAVAILABLE = 0\n",
    "    ADD = 1\n",
    "    DELETE = 2\n",
    "    TRAIN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerNetworkTask(Dataset):\n",
    "    def __init__(self, data, targets, info):\n",
    "        self.data = data \n",
    "        self.targets = targets\n",
    "        self.info = info\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.data) == config['N_X'], '[ERROR] Length should be the same as N_X.'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert self.data[index].dtype == torch.float32, f'[ERROR] Expected type torch.float32, got type: {self.data[index].dtype}'\n",
    "        assert self.targets[index].dtype == torch.float32, f'[ERROR] Expected type torch.float32, got type: {self.targets[index].dtype}'\n",
    "        sample = {\n",
    "            'x' : self.data[index],\n",
    "            'y' : self.targets[index],\n",
    "            'info' : self.info[index]\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'[INFO] InnerNetworkTask(data={self.data, self.targets}, info={self.info})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerNetwork(gym.Env, Module):\n",
    "    def __init__(self, \n",
    "                task: InnerNetworkTask,\n",
    "                layer_pool: LayerPool,\n",
    "                learning_rate: float=config['LEARNING_RATE'],\n",
    "                batch_size: int=config['BATCH_SIZE'],\n",
    "                epsilon: float=config['EPSILON'],\n",
    "                action_cache_size: float=config['ACTION_CACHE_SIZE'],\n",
    "                num_workers: int=config['NUM_WORKERS'],\n",
    "                shuffle: bool=True,\n",
    "                log_dir: str='runs',\n",
    "                ):\n",
    "        super(InnerNetwork, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.layer_pool = layer_pool\n",
    "        self.task = task\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.data_loader = DataLoader(task, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "        self.data_iter = iter(self.data_loader)\n",
    "        self.prev = defaultdict(lambda: None)\n",
    "        self.curr = defaultdict(lambda: None)\n",
    "        self.initial_layer = torch.nn.Linear(1, self.layer_pool.num_nodes_per_layer) # TODO is to have param with input_shape\n",
    "        self.final_layer = torch.nn.Linear(self.layer_pool.num_nodes_per_layer, 1) # TODO is to have param with output_shape\n",
    "        torch.nn.init.xavier_uniform_(self.final_layer.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.initial_layer.weight)\n",
    "        self.layers = torch.nn.ModuleList([self.initial_layer, self.final_layer])\n",
    "        self.pool_indices = [] \n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.opt = torch.optim.Adam(self.layers.parameters(), lr=learning_rate)\n",
    "        self.action_cache_size = action_cache_size\n",
    "        self.actions_taken = [InnerNetworkAction.UNAVAILABLE] * config['ACTION_CACHE_SIZE']\n",
    "\n",
    "        # logging variables\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "        self.timestep = 0\n",
    "        self.reset_num = 0\n",
    "\n",
    "        # state and action spaces\n",
    "        self.state = self.reset()\n",
    "        state_shape = self.build_state().shape\n",
    "        self.observation_space = Box(low=float('-inf'), high=float('inf'), shape=state_shape) # TODO is to normalize\n",
    "        self.action_space = Discrete(self.layer_pool.size * 2 + 1)\n",
    "\n",
    "        # print(f'[INFO] Number of available actions={self.action_space.n}')\n",
    "        # print(f'[INFO] Range of action indices={}')\n",
    "\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[torch.Tensor, float, bool, dict]: \n",
    "        assert action.shape == (), f'[ERROR] Expected action shape () for scalar {self.action_space.n}, got: {action.shape}'\n",
    "        assert action.dtype == np.int64, f'[ERROR] Expected np.int64 dtype, got: {action.dtype}'\n",
    "        self.prev = self.curr\n",
    "        self.curr = defaultdict(lambda: None)\n",
    "        action_index = self.epsilon_greedy(action)\n",
    "        self.update_inner_network(action_index)\n",
    "        self.forward_inner_network()\n",
    "\n",
    "        # log\n",
    "        task_str = str(self.curr['info']['i'].item())\n",
    "        self.writer.add_scalar(f'loss/timestep_task_{task_str}', self.curr['loss'], global_step=self.timestep) \n",
    "        self.writer.add_scalar(f'num_layers/timestep_task_{task_str}', len(self.layers), global_step=self.timestep) \n",
    "        if (len(self.pool_indices)!=0):\n",
    "            self.writer.add_histogram(f'pool_indices/timestep_task_{task_str}', torch.tensor(self.pool_indices).long(), global_step=self.timestep) \n",
    "        self.writer.add_histogram(f'action_types/timestep_task{task_str}', torch.tensor([e.value for e in self.actions_taken]).long(), global_step=self.timestep) \n",
    "\n",
    "        self.timestep += 1\n",
    "        s_prime = self.build_state()\n",
    "        reward = self.reward()\n",
    "        if (self.curr['action_taken']==InnerNetworkAction.TRAIN):\n",
    "            print(f'[INFO] TRAINED')\n",
    "        # print(f'[INFO] Timestep={self.timestep}')\n",
    "        return (\n",
    "            s_prime,\n",
    "            reward, \n",
    "            False, \n",
    "            {}\n",
    "        )\n",
    "    \n",
    "    def epsilon_greedy(self, action: np.int64) -> int:\n",
    "        if random.random() <= self.epsilon: action_index = random.randint(0, self.action_space.n - 1)\n",
    "        else: action_index = action\n",
    "        # print(f'[INFO] Action index {action_index} (range: 0 - {self.action_space.n - 1})')\n",
    "        return action_index\n",
    "    \n",
    "    def update_inner_network(self, action_index: int) -> None:\n",
    "        if (action_index == 0): \n",
    "            action_type = InnerNetworkAction.TRAIN\n",
    "        elif (action_index > 0 and action_index < self.layer_pool.size): \n",
    "            action_type = InnerNetworkAction.ADD\n",
    "        else:\n",
    "            action_type = InnerNetworkAction.DELETE\n",
    "\n",
    "        if (action_type == InnerNetworkAction.TRAIN):\n",
    "            # print('[INFO] \"Train\" action taken by inner network.')\n",
    "            self.actions_taken.append(InnerNetworkAction.TRAIN)\n",
    "            self.curr['action_type'] = InnerNetworkAction.TRAIN\n",
    "\n",
    "        if (action_type == InnerNetworkAction.ADD):\n",
    "            # print('[INFO] \"Add\" action taken by inner network.')\n",
    "            self.pool_indices.append(action_index)\n",
    "            next_layer = self.layer_pool.layers[action_index].params\n",
    "            final_layer = self.layers.pop(-1)\n",
    "            self.layers.append(next_layer)  \n",
    "            self.layers.append(final_layer) \n",
    "            self.actions_taken.append(InnerNetworkAction.ADD)\n",
    "            self.curr['action_type'] = InnerNetworkAction.ADD\n",
    "\n",
    "        if (action_type == InnerNetworkAction.DELETE):\n",
    "            # print('[INFO] \"Delete\" action taken by inner network.')\n",
    "            if (action_index not in self.pool_indices):\n",
    "                self.actions_taken.append(InnerNetworkAction.UNAVAILABLE)\n",
    "                return\n",
    "            adjusted_pool_index = action_index = self.layer_pool.size\n",
    "            self.pool_indices.remove(adjusted_pool_index)\n",
    "            layer_to_delete = self.layer_pool.layers[adjusted_pool_index].params\n",
    "            network_index = self.layers.index(layer_to_delete)\n",
    "            assert layer_to_delete == self.layers[network_index], '[ERROR] Wrong layer would be deleted from inner network params.'\n",
    "            self.layers.pop(network_index)\n",
    "            self.actions_taken.append(InnerNetworkAction.DELETE)\n",
    "            self.curr['action_type'] = InnerNetworkAction.DELETE\n",
    "\n",
    "    def next_batch(self, throw_exception=False) -> dict:\n",
    "        if (throw_exception):\n",
    "            return next(self.data_iter)\n",
    "        else: \n",
    "            try:\n",
    "                batch = next(self.data_iter)\n",
    "                return batch\n",
    "            except StopIteration:\n",
    "                self.data_loader = DataLoader(self.task, batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)\n",
    "                self.data_iter = iter(self.data_loader)\n",
    "                return next(self.data_iter)\n",
    "\n",
    "    def forward_inner_network(self) -> None: \n",
    "        batch = self.next_batch()\n",
    "        self.curr['x'] = batch['x']\n",
    "        self.curr['y'] = batch['y'] \n",
    "        self.curr['info'] = batch['info']\n",
    "        # set model to train or eval\n",
    "        if self.curr['action_type'] == InnerNetworkAction.TRAIN:\n",
    "            self.train() # needs to be called before forward pass for gradient information to be saved\n",
    "            self.opt = torch.optim.Adam(self.layers.parameters(), lr=self.learning_rate) \n",
    "            self.opt.zero_grad()\n",
    "        else:\n",
    "            self.eval()\n",
    "        # forward pass\n",
    "        x = self.curr['x']\n",
    "        for i in range(len(self.layers) - 1): x = torch.nn.functional.relu(self.layers[i](x))\n",
    "        self.curr['latent_space'] = x\n",
    "        self.curr['y_hat'] = self.layers[-1](x) \n",
    "        self.curr['loss'] = self.loss_fn(self.curr['y'], self.curr['y_hat'])\n",
    "        assert self.curr['latent_space'].dtype == torch.float32\n",
    "        assert self.curr['y_hat'].dtype == torch.float32\n",
    "        # update params if 'train'\n",
    "        if (self.curr['action_type'] == InnerNetworkAction.TRAIN):\n",
    "            print(f'[INFO] Trained network.')\n",
    "            loss = self.curr['loss']\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "\n",
    "    def build_state(self) -> np.ndarray:\n",
    "        num_add_actions = torch.tensor(len(list(filter(lambda e : e == InnerNetworkAction.ADD, self.actions_taken)))).unsqueeze(0)\n",
    "        num_delete_actions = torch.tensor(len(list(filter(lambda e : e == InnerNetworkAction.DELETE, self.actions_taken)))).unsqueeze(0)\n",
    "        num_train_actions = torch.tensor(len(list(filter(lambda e : e == InnerNetworkAction.TRAIN, self.actions_taken)))).unsqueeze(0)\n",
    "        num_layers = torch.tensor(len(self.layers)).unsqueeze(0)\n",
    "        h = torch.tensor([action_enum.value for action_enum in self.actions_taken[-self.action_cache_size:]])\n",
    "        task_info = torch.tensor([float(value) for value in self.curr['info'].values()])\n",
    "        return torch.concat((\n",
    "            task_info,\n",
    "            self.curr['x'],\n",
    "            self.curr['latent_space'],\n",
    "            self.curr['y'],\n",
    "            self.curr['y_hat'],\n",
    "            num_add_actions,\n",
    "            num_delete_actions,\n",
    "            num_train_actions,\n",
    "            num_layers,\n",
    "            h\n",
    "        ), dim=0).detach().numpy()\n",
    "    \n",
    "    def reward(self) -> torch.Tensor:\n",
    "        prev_loss = self.prev['loss']\n",
    "        curr_loss = self.curr['loss']\n",
    "        # assert prev_loss != curr_loss, '[ERROR] Loss values from previous and current run are equal.'\n",
    "        delta_loss = prev_loss - curr_loss\n",
    "        curr_action = self.curr['action_type']\n",
    "        if (curr_action == InnerNetworkAction.ADD):\n",
    "            reward = delta_loss / math.sqrt(len(self.layers))\n",
    "        if (curr_action == InnerNetworkAction.TRAIN or InnerNetworkAction.DELETE):\n",
    "            reward = delta_loss\n",
    "        if (curr_action == InnerNetworkAction.UNAVAILABLE):\n",
    "            reward = -1000\n",
    "        return reward\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.forward_inner_network()\n",
    "        print(f'[INFO] Reset called.')\n",
    "        self.reset_num += 1\n",
    "        return self.build_state()\n",
    "    \n",
    "    def close(self):\n",
    "        print(f'[INFO] Closed writer')\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baby example with just 2 epochs\n",
    "However, 2 epoch is still going through all 20 tasks, generating 20 different networks, using the same 1 meta policy. Also, on each task we loop through the data 1000 times (timesteps / len(X) == 10000 / 100).\n",
    "<br>\n",
    "The benefit of more epochs is we get the meta policy to gnerate 20 different networks again for the 20 tasks, having hopefully learned something from the first run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sinusoidal curves regression as in 2018 MAML paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stach\\AppData\\Local\\Temp\\ipykernel_22992\\2632726700.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  tasks_data = torch.tensor([\n"
     ]
    }
   ],
   "source": [
    "# create tasks\n",
    "X = np.linspace(lower_bound, upper_bound, config['N_X'])\n",
    "lower_bound = torch.tensor(-5).float()\n",
    "upper_bound = torch.tensor(5).float()\n",
    "amplitude_range = torch.tensor([0.1, 5.0]).float()\n",
    "phase_range = torch.tensor([0, math.pi]).float()\n",
    "amps = torch.from_numpy(np.linspace(amplitude_range[0], amplitude_range[1], config['N_TASKS'])).float()\n",
    "phases = torch.from_numpy(np.linspace(phase_range[0], phase_range[1], config['N_TASKS'])).float()\n",
    "\n",
    "# (20, 100)\n",
    "tasks_data = torch.tensor([ \n",
    "        X\n",
    "        for _ in range(config['N_TASKS'])\n",
    "        ]).float()\n",
    "tasks_targets = torch.tensor([\n",
    "        [((a * np.sin(x)) + p).float()\n",
    "        for x in X] \n",
    "        for a, p in zip(amps, phases)\n",
    "        ]).float()\n",
    "tasks_info = [\n",
    "        [{'i' : i, \n",
    "         'amp' : a, \n",
    "         'phase_shift' : p, \n",
    "         'lower_bound' : lower_bound, \n",
    "         'upper_bound' : upper_bound, \n",
    "         'amplitude_range_lower_bound' : amplitude_range[0], \n",
    "         'amplitude_range_upper_bound' : amplitude_range[1], \n",
    "         'phase_range_lower_bound' : phase_range[0],\n",
    "         'phase_range_lower_bound' : phase_range[1]}\n",
    "         for _ in X]\n",
    "        for i, (a, p) in enumerate(zip(amps, phases))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100])\n",
      "torch.float32\n",
      "torch.Size([5, 100])\n",
      "torch.float32\n",
      "5\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(tasks_data.shape)\n",
    "print(tasks_data.dtype)\n",
    "print(tasks_targets.shape)\n",
    "print(tasks_targets.dtype)\n",
    "print(len(tasks_info))\n",
    "print(len(tasks_info[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tasks\n",
    "tasks = [InnerNetworkTask(data=tasks_data[i], targets=tasks_targets[i], info=tasks_info[i]) for i in range(config['N_TASKS'])]\n",
    "# create pool\n",
    "pool = LayerPool()\n",
    "# create REML\n",
    "log_dir = f'./runs/ppo_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "model = REML(layer_pool=pool, tasks=tasks, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED': 123,\n",
       " 'DEVICE': 'cpu',\n",
       " 'EPOCHS': 2,\n",
       " 'TIMESTEPS': 5000,\n",
       " 'N_X': 100,\n",
       " 'N_TASKS': 5,\n",
       " 'POOL_N_LAYERS': 100,\n",
       " 'N_NODES_PER_LAYER': 32,\n",
       " 'POOL_LAYER_TYPE': torch.nn.modules.linear.Linear,\n",
       " 'ACTION_SPACE_SHAPE': (3,),\n",
       " 'EPSILON': 0.1,\n",
       " 'BATCH_SIZE': 1,\n",
       " 'LEARNING_RATE': 0.05,\n",
       " 'ACTION_CACHE_SIZE': 5,\n",
       " 'NUM_WORKERS': 0,\n",
       " 'LOSS_FN': MSELoss(),\n",
       " 'SB3_MODEL': stable_baselines3.ppo.ppo.PPO,\n",
       " 'SB3_POLICY': 'MlpPolicy'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch 1/2\n",
      "[INFO] Task num=1/5\n",
      "[INFO] Reset called.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "[INFO] Reset called.\n",
      "Logging to ./runs/ppo_2023-10-10_12-44-57\\task_0_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stach\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\stable_baselines3\\ppo\\ppo.py:148: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 100`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=100 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 189 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 0   |\n",
      "|    total_timesteps | 100 |\n",
      "----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 152        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 200        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01574725 |\n",
      "|    clip_fraction        | 0.0852     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.3       |\n",
      "|    explained_variance   | -7.37      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0637    |\n",
      "|    value_loss           | 0.0232     |\n",
      "----------------------------------------\n",
      "[INFO] Trained network.\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 300         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016126849 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.29       |\n",
      "|    explained_variance   | -0.923      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.107      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 0.00492     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 400         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019992024 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.28       |\n",
      "|    explained_variance   | -0.384      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0991     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 0.00327     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 500         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022225937 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.26       |\n",
      "|    explained_variance   | -0.106      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.111      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n",
      "[INFO] Trained network.\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 600         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025754381 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.24       |\n",
      "|    explained_variance   | -0.00377    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.11       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 0.00181     |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 77        |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 700       |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0213063 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.22     |\n",
      "|    explained_variance   | 0.000356  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0793   |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | -0.0458   |\n",
      "|    value_loss           | 0.00624   |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 800         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020984542 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.2        |\n",
      "|    explained_variance   | -0.000443   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0463     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.00088     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 67         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 900        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02946544 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.19      |\n",
      "|    explained_variance   | -0.000307  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.146     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0687    |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018835116 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 1.33e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0864     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 1100        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019059617 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.13       |\n",
      "|    explained_variance   | -3.11e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0989     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 0.00159     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 1200       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03113236 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.09      |\n",
      "|    explained_variance   | -6.44e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0955    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0585    |\n",
      "|    value_loss           | 0.000757   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 1300        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023894522 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.04       |\n",
      "|    explained_variance   | 3.4e-06     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.116      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n",
      "[INFO] Trained network.\n",
      "[INFO] Trained network.\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 1400        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025768418 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.02       |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 0.000481    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018310899 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0947     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 0.03        |\n",
      "-----------------------------------------\n",
      "[INFO] Trained network.\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 1600        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031482454 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 7.33e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.111      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 0.000523    |\n",
      "-----------------------------------------\n",
      "[INFO] Trained network.\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 1700        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016883645 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5          |\n",
      "|    explained_variance   | 3.99e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0685     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.0334      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 42         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 1800       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02404615 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.97      |\n",
      "|    explained_variance   | -0.0121    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0813    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    value_loss           | 0.0155     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 1900        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023601836 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.92       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0931     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 0.0174      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 38         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 51         |\n",
      "|    total_timesteps      | 2000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02981279 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.88      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 0.00783    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 37          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 2100        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026905622 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.84       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.11       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 0.00512     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 35          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 2200        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020541226 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.82       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0917     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 0.00911     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 34          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 2300        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028674126 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.75       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0976     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 0.00655     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 33          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 2400        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028279055 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.73       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0893     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 0.0058      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 76         |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02448852 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.73      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0815    |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0448    |\n",
      "|    value_loss           | 0.00868    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 2600        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032988694 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.67       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0939     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 0.00402     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 2700        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024283066 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.62       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0867     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 0.00814     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 2800        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027597226 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.59       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0905     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 0.00208     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 2900        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023747113 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.54       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0831     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 106        |\n",
      "|    total_timesteps      | 3000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03220309 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.39      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.114     |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0557    |\n",
      "|    value_loss           | 0.00701    |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 27        |\n",
      "|    iterations           | 31        |\n",
      "|    time_elapsed         | 113       |\n",
      "|    total_timesteps      | 3100      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0295793 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -4.3      |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0573   |\n",
      "|    n_updates            | 300       |\n",
      "|    policy_gradient_loss | -0.0567   |\n",
      "|    value_loss           | 0.00499   |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 3200        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023862548 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0829     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
