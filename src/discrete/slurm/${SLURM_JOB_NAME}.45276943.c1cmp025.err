wandb: Appending key for api.wandb.ai to your netrc file: /cluster/home/mstach01/.netrc
2023-12-24 09:52:36.835484: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.
wandb: Currently logged in as: mattstachyra. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/wandb/run-20231224_095253-1lk048qm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tuning_1224_0952
wandb: ⭐️ View project at https://wandb.ai/mattstachyra/reinforcement-meta-learning
wandb: 🚀 View run at https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/1lk048qm
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  tasks_targets = torch.tensor(np.array([
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tasks_targets = torch.tensor(np.array([
/cluster/home/mstach01/condaenv/mthesis/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 5
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=5 and n_envs=1)
  warnings.warn(
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.086 MB uploaded (0.000 MB deduped)wandb: | 0.086 MB of 0.086 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▂▁▄▁▁▁▁▂▂▂▃▁▁▄▃▂█▄▂▄
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▁▁▁▄▁▂▁▁▂█▁▁▁▁▁▁▁▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▁▂▂▁▂▂▄▃▂▃▂█▁▁▁▂▁▁▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▁▂▂▂▁▂█▁▂▂▁▁▃▁▁▂▂▂▁▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▃▁▁▃▄▁▁▄▆▂█▁▁▁▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▂▂▅▁█▂▁▁▁▁▁▁▁▁▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▂▁▂▂▄█▁▂▂▂▂▁▁▂▁▁▁▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▅▅▅█▁▁▅▁▁▂▁▃▁▁▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▃▄▁▃▁▁▂▁▁▁▃▁▁▂▁▁▂▃█▄
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁█▁▁▄▂▃▁▁▂▂▁▂▂▁▁▂▁▂▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▂▆▁▆▁▁▁▂▂▂▂▁▁▂▂▂█▂▂
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▂▁▂▁▁▁▁▁▁▁▄▁▁█▁▁▂▂
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▁▅▅▂▂▅▂▂▄▁▄▆▁█▆▁▁▅▄▅
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▅▄▁▂▃▆▃▁▆▅▂▁▅▅▅▃▂▅▅
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▄▁▇█▆▆▇▇▄▁▇▄▃▄▁▇▁▃▆▃
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▄▁▂▁▃▁▁▁▄▁▃▁▁▅▁█▂▃▁
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▃▂▇▄▇▂█▂█▂▃▄▁▄▁▂▁▃▂▂
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▄▂▃▁▄▅▃█▅▃▁▁▃▁▁▂▂▂▃
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▁▃▁▁▂▂▂▁▂▁▁▁▁▂█▁▁▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▂▇█▃
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▂▅▂▂▃▁▂▃██
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▃▆▂▁▁▆█▁▅▃▁▃
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▂▅▇▄▄▂▂█▂▄█▄▄
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▂▇▃▃▁▃▁▁▃▁▂▁▁█▁▁▂▁▁▂
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▇▃▃▂▅▂▂▂▁▁▂▄▄▁▂▂▁█▃▂
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▄▂▃▂▄▅▃▁▄█▅▁▁▁▁▃▅▁▁▁
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▃▂▁█▄▂▂▄▆▁▅▄▃▁▄▃▂▁▂
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▂▁▁▁▂▁▅▂▄▁▁▁▃▆▅█▁▁
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▃▅▂▆▂▂▅▆▂▅▁█▄▄▂▁▄▂▁▂
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▄▃▁▁▁█▂▂▂▂▁▁▁▂▁▂▁▁▂▁
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▆▆▇▁█▇▅▁▁▅▄▆▁▇▆▂▂▆▆▁
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▃▁█▁▁▁▁▁▁▃▁▁▁▁▁▁▁▂▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▅▁▄▂█▃▄▁▅▁▃▄▅▂▁▂▅▁▃▄
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁█▂█▆▃█▅▃▆▂
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▄▄▃▃▆█▃▃
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch █▆▃▂▃▂▁▃▁▂▁▂▁▁▁▂▄▂▂▁
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▁▄▄▂▁▃▃▃▂▂▁▃▁▁▁▁▁▁▁█
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▃▃▂▅▃▁▁▂▁▅▆▂▂█▁▆▄▁▄
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▁▄▁▁▆▅▁▇▂▆▄▂▇▁▆█▂▃▃
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▁▅▄▇▇▇▇▃▅▅▄▅▇▆▅▆██▅▁
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▃▂▄▃▂▃█▂▂▂▂▂▂▂▂
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▂▂▁▂▂█▁▁▁▁▁▁▁
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▃▂▁▂▂▃█▂▄▄▂▂▃▂▂▁▃█▂▂
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▆▆▆▆▆▅▆▆▅▆▆▆█▁▅▆▆▆▆▆
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▂▇▅█▁█▂▂▂▁▁▁▁▁▁▁
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▁▃▃▄█▆▄▃▄▄▃▃▃▃▃▃▃▃
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▂▃▂▃▂▃█▄▃▂▃▄▃▃▂▂▃▂▁▂
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▃▂█▃▅▃▃█▃▅▇███▃█▄▁▃▄
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▂▂▂█▃▃▂▂▁▂▂▁▂▂▂▂▂▂▂
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▁▁▁
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▆▃▄▇▅▅▆▆▃▆▂▇▆█▁▆▆▆▃▆
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▆█▁▅▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁█▅▁█▇▇▇██▇████▇██▇█
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▂▁▂▁▂▁▆▁▂▇▁█▄█▁
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▄▄▅▂▁▅▇▅█▄▅▂▅▃▅▅▅▅▅▅
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▁▅▂▄▄▃█▃▂▂▂▄▂▂▁▃▂▂
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▂▁▂▂▁▂▃▂▂▂▂▂▂▂▂█▂▂▂
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ████████▆█▃▇▄▇██▆▆▁▇
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ██████████▇▅▇▇▇█▇▇▁▆
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ████████▂▁▁▁▁▁▁▁▁▁▁▁
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ███████▁▁▁▁▁▁▆▁▁▁▁▁▁
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▁▂▄▂▃▃▄▄▃▃▂▄▄█▂▂▃▃▃▄
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▃▃▁▃▃▃▃▃██▃▃▃█▃▃▆▆▃▃
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▂▃▃▃▄▃▄▄▄▃▃▃▃▇█▃▃▃
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▆▆▇▇▁▇▇▇▇▇▇█▆▇▇█▇▇▇▇
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▂▂▂▁▃▂▃▂▁▂▂█▅▇▂▂
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▅▅▆▄▅▅▅▆▆▆▆▆▁▅█▇▇▅▇▆
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▃▃▃██▄▅▃▃▃▃▃▃▃▃▃▃▃
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▆▁▇█▇█▇▇▆▆▆▇▃▆▆▆▆▆▇
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▁▆█▆▃▃▁▆▆▄▁▃▆▆▁▁▁▁▆▆
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▁█▁▅▁▁▁█▁█▁▁▂▁█▁▁█▁▁
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ██████████▂▅▂▃▁▁▁▁▁▁
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ██████████▇█▁▁▁▁▁▁▁▁
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▆▃▄▄█▂█▄█▄▃▆▆▄▂▄▄█
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▆▅▅▃▆▃▆▃▁▃▆▃▆█▆▆▆▆▆▅
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▂▁▂▂▂▂▂▂█▃▄▂█▂▅▄▂▄
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▄▂▆▆▅▄▆▂▆▄▆▆▃▆▆█▆▆▆
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch █▅▄▃▂▂▂▂▂▂▂▂▃▂▂▂▁▂▁▁
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch █▇▅▄▄▄▂▃▃▁▂▃▃▂▁▃▁▂▃▃
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▆▅▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▆▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch █▅▃▂▃▃▂▂▁▂▁▂▂▁▁▂▁▃▁▁
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch █▆▄▄▃▃▂▃▄▂▄▃▃▃▁▁▅▂▃▃
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▃▃▂▂▂▄▄▄▃▃▃▁▁▁▁▁▁
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▆▄▃▂▂▂▂▄▄▄▃▃▂▁▁▁▁▁▁
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch █▄▃▃▃▃▃▂▃▂▂▂▁▂▁▃▃▂▂▃
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch █▆▅▄▄▅▄▃▃▃▂▁▂▂▂▃▂▁▂▂
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▆▄▂▂▂▂▂▄▃▂▂▁▁▁▁▁▁▅▂
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▅▄▂▂▂▂▂▂▃▂▂▁▁▁▁▁▁█▂▁
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch █▄▄▃▂▃▂▂▃▁▂▁▁▂▃▂▃▂▂▃
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch █▅▃▄▄▃▄▂▂▂▂▃▂▁▅▄▂▃▅▇
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▄▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch █▅▄▂▃▄▃▂▁▂▁▃▃▃▂▂▃▄▂▂
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch █▄▄▄▃▄▃▃▄▃▂▅▂▅▃▁▄▂▂▃
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▅▅▄▃▂▃▃▃▃▃▁▃▁▁▁▃▂▂▂
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▅▅▄▃▃▃▃▃▃▂▁▂▁▁▁▂▂▃▁
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▁▁▄▅▃▁█▄▇███▇▅████
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▂▂▁▁▁▃▂▁▆▄▇█▇▇▆▃▇███
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▁▂▂▃▄▂████████████
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▃▁▂▂▃▃█████████████
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch █▅▃▃▃▂▂▂▂▂▂▂▁▂▁▂▁▁▁▂
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch █▅▄▃▂▂▂▂▂▂▂▁▁▁▂▁▂▂▁▁
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▃▂▂▁▂▂▂▂▁▁▁▁▂▁▁▁▁
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▆▅▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch █▅▄▂▂▂▂▂▂▂▁▂▃▂▂▂▁▁▁▂
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch █▆▅▃▃▃▃▂▂▁▂▁▂▂▁▂▁▂▃▂
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▃▂▁▁▂▁▂▁▁▃▂▂▁▁▂▁▃█▁▁
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▄▃▁▂▁▂▂▁▁▁█▂▁▄█▃▃▁▆▄
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▃▃▃▂▂▂▄▂▂▁▇▇▇███████
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▅▂▂▃▂▂▂▃▁▅▇████████
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch █▆▅▄▂▁▂▃▁▂▂▂▁▁▂▁▂▁▂▂
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch █▆▄▅▃▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▆▄▃▂▂▂▂▂▁▁▂▁▂▂▁▁▂▁
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▆▄▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 0.87286
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 46.31058
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 3.15114
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 27.68068
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2.45869
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 23.85815
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 0.52878
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 63.50159
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.5827
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 31.66288
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 10.99468
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 34.82032
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2.52678
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 7.54601
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 1.62717
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 8.50146
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2.71411
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 24.16835
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 2604.44773
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 10488.79413
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 16028.59078
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 43903.39859
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 3.18015
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 10.98589
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 9.45674
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 15.32883
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0.84921
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 22.37113
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 496.67918
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 5106.86145
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 87.35784
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 19.57082
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 30.18905
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch -1
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch -3.17498
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.13258
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.1865
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.12321
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.16027
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch -2
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch -1.3306
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.08043
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.21384
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 0.3555
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch -0.4814
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.19615
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.05103
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch -2
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 0.12419
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0.25422
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.15686
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch -311.64982
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch -576.36864
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -2047.18875
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -2047.11655
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch -0.31059
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch -2
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -1
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch -0.62923
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.04139
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch -1.98134
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -498.17169
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -498.12644
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch -1.07547
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 8.74295
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0.03296
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 810
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 839
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 25
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 34
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 787
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 819
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 11
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 11
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 816
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 801
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 92
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 34
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 826
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 906
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 6
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 2
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 779
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 782
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 97
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 26
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 2039
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 2030
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2047
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 2047
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 207
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 206
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 48
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 44
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 797
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 793
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 13
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 13
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 200
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 211
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 498
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 498
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 209
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 208
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 39
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 56
wandb: 
wandb: 🚀 View run tuning_1224_0952 at: https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/1lk048qm
wandb: ️⚡ View job at https://wandb.ai/mattstachyra/reinforcement-meta-learning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzY1Nzc3Ng==/version_details/v54
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231224_095253-1lk048qm/logs
