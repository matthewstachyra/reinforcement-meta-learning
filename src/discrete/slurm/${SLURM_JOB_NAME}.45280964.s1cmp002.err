wandb: Appending key for api.wandb.ai to your netrc file: /cluster/home/mstach01/.netrc
2023-12-25 22:15:27.286390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  tasks_targets = torch.tensor(np.array([
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tasks_targets = torch.tensor(np.array([
wandb: Currently logged in as: mattstachyra. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/wandb/run-20231225_221533-oi3i2q25
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tune_1225_2215
wandb: ⭐️ View project at https://wandb.ai/mattstachyra/reinforcement-meta-learning
wandb: 🚀 View run at https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/oi3i2q25
/cluster/home/mstach01/condaenv/mthesis/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 5
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=5 and n_envs=1)
  warnings.warn(
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▂▁▁▁▂▁▁▂▄▁█▄▆▁▂▂▃▄▅▃
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▂▁▁▁▁▁▁▁▁█▁▁▁▁▁▂▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▂▂▃▁▁▂▃▂▂▃▁▇▂▃▂▁▂█
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch ▂▁▁▁▁▁▁▂▁▁▁▁▅▃▂▁█▂▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▃▂█▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▂▁▁▁▁▁▃█▂▃▁▁▂▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▂▁▂▂▂▁▄▁▃▂▂▁▄█▃▁▃█▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▂▁▂▂▁▁▂▁▂▁▁▂█▁▁▂▁▃▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▂▂▂▁▂▂▁▂▃▁▃▃▅█▁▁▂█▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▁▂▁▂▇▁▄▂▁▁▂▅▆▁▄█▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▁▁▂▂▁▁▁▇▅▂▁▄█▁▃▃▇▂▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▂▁▁▂▁▁▁▁▁▂█▂▁▁▁▂▁▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▃▁▁▁▃█▁▁▁▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▆▂▁█
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▄▂▁▁▁▁▁█▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▂▁▁▁▁▁▅▁█▁▁▁▁▁▁▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▅▂▁▆▂▁▁█▄▁▂█▂▁▂▁▁▂▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▂▁▁▂▁▄▁▁▁█▁▁▁▁▁▁▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▃▂▁▄▅▂▂▅▃█▂▂▃▂▂▂▂▃▁▃
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁█▁▂▁▁▂█▂▁▁▁▁▁▁▁▁▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▂▃▂▁▃█▅▁▇▁▁▁▁▁▁▁▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▂▁▁▁▁▁▁▁▁█▄▁▁▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁█▄▁▃▁▁▁▂▂▅▆▂▁▂▂▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▂▂▁▂▂▁▁▁▁▁▁▁▂█▁▁▃▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch ▂▁▁▂▂▂▁▁▁▁▂▃▁▁▁▁▂▁▂█
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch ▁▂▃▂▃▁▄▁▃▁▁█▁▁▂▁▁▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁█▁▁▁▁▁▁▁▂▁▁▁▁▃▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▃▁▂▁▂▁▁█▁▁▁▁▁▁▁▂▁▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▁▂▃█▃▁▃▁▂▂▂▂▂▂▂▂▃▃▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▅▂█▆▂▂▂▁▃▃▃▁▁▂▃▂▁▃▂
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▄▂▅█▂▄▂▄▄▂▃▃▃▁▁▁▂▃▁▂
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▄▅▁▁▆▂▂▁█▃▁▁▁▁▁▁▁▁▂▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▁▂█▁▁▃▆▁▃▂▂▁▁▁▂▃▃▃▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▂▁▁▁▁▁▂▁█▁▁▅▁▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▂▂▁▁▅▁▁▁▁▇▆▁▁▂▁▁▁█
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch ▁▂▂▁▁▁▂▁▁▁▅▁▁▁▁▁▂█▁▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch ▂▂▂▁▂▁▁▂▁▁▁█▁▂▁▄▁▃▁▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁█▁▁▁▁▄▄▁▁█▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▂▁█▁▁▁▁▃▆▆▁▁▁▁▂
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▆▆▁▃▅▅▅▁▃▃▁▂▂▁▃▁▁▃
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▃▃▆▃▁▆█▂▄▄▃▆▄▂▂▃▃▁▄
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▄▃▇▁▄▂▅▃▅█▂▁▂▂▁▂▃▃▄
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▄▅██▁▂▂▄▄▂▃▂▂▁▃▂▃▄▄▁
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▃▃█▂▃▃▅▃▄▂▁▂▃▃▃▂▃▂▂▁
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁█▅▂▇▆▁▂▆█▅▁▅▅▁▄▄▅
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁█
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▂▁▃▂▁▂▁▁▁▁█▁▁▂▁▁▂▅▆▁
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▃▁▁▁▃▁▁▁▁▁█▁▁▁▁
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▂▂▂▁▁▁▁▇█▁▁▂▁▁▁▁
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁█▁▁▁▁
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁█▃▁▁▁▁▂
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▁▁▁█▂▂▂▁▁▂▂▅▅▅▂▁▂▁
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▅▂▂█▅▁▂▄▃▃▂▃▃▃▃▄▄▄▁▅
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▆▁▁▁█▃▁▂▁▁▁▁▄▁▁▂▁▁▁
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▃▅▂▂▆▂▃▂▂▂▂▂▁▂▂▅▂█▂▂
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▁██▁▃▇▁▁▁▁▁▅▁▁▁▁▁▁
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▂▂▂▁▂▁▁▁▁█▁▁▁▁▁▁▁▁▂
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁█▁▄▁▁▁▃▄▇▁▁▂
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▇▂▁▁▁▁█▇▆▄▁
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▂▁▁▁▁▃▂▂▁▁▁█▁▂▂▄▂
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▄▁▁▁▃▂▁▁▁▁▄█▁▂▃▁
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▃▂▂▁▁▁▂▅▁█▁▃
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▂▂▁▁▁█▃▂▄▂▂
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂█▁▅▁▂▂▂▃▁▂▂▁▁▁▁▂▁▁
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▄▅▁▁▁▄▅▄▄▅▄▄▅▃▄▄▄▄▄█
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▆█▁▁▇▃▃▇▄▅▄▆▄▃▄▃▅▃▃
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁█▁▁▇▃▃▄▄▄▆▄▄▃▄▄▄▄▄
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁█▁▁▁▃▂▂▂▆▂▅▃▃█▂▄▁▂▂
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁█▂▁▂▂▁▂▂▁▂▁▂▄▂▁▂▃▂▁
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▃▁▁▁█▁▁▁▂▃▃▃▂▂▂▂▃▂▃▂
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▃▂▄▂▄▂▁▁▁▁█▂▂▁▁▁▁▁▁▂
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch ▅▄▁▁█▂▃▁▄▁▁▁▁▁▁▁▁▁▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch ▂▁▂▁▂█▂▂▂▂▁▂▄▁▂▂▂▂▂▂
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch ▂▁▁▁█▁▁▂▁▁▁▁▁█▁▁▁▁▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▁▄▁▆▄▂▂▇▃▁▂▁▁▇▁█▂▆▇▂
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch ▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂█
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch ▁▃▁▁▁▁▁▂▂▂▁▂▁▆█▂▇▃▃▃
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch ▂▃▁▄▁▁▁█▃▁▄▁▁▁▁▂▂▁▁▁
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▂▁▁▂▃▁▂█▁▃▁▃▁▃▁▁▁▁
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁█
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▃
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁█
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch ▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch ▁▂▄▁▁▁▃▃▁█▁▁▁▁▁▁▁▁▁▁
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▁█▁▁▁▁▁▂▁▃▁▂▁▁▄▁▁▄
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▄▃▁▁▃▁▁▂▂▂▃▂▃▂▁▁▄▁▁█
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch ▂█▁▃▂▂▁▁▂▂▁▁▆▁▄▃▁▂▅▁
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch ▁█▁▂▁▂▄▂▂▅▁▂▆▂▁▂▁▁▂▂
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▄▁▃█▅▁▃▁▁▁▁▂▁▁▁▁▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch ▁▂▁▁▁▁▁▁▅▁▁▁▁▁█▂▃▁▁▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁█▃▂▂▂▃▂▂▂▁▁▃▄▃▂▁▂▃
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▆▃▅▅▃▁▄▃▄▄▃▆▄▁▄▄▁█▇
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▅▁▇█▅▅▆▅▂▁▅▃▂▃▅▁▁▂▃▆
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▆▃▄▃▁▁▅▅▂▄█▅▁▄▁▁▇▂▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▂▁█▁▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▂
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▃▄▆▅▇▂▁▆▇▃▅▃▁█▅▁▅▆▆▁
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▃▂▁▂▁██▅▄▂▄▂▂▁▁▂▂▂▁▁
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▂▂▁▃▄▂█▃▁▂▁▁▃▃▁▁▂▁▁
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▂▁▁▁▂▁▅█▃▁▁▁▁▁▁▁▁▁▁▁
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▃▃▃▂▁█▁▂▃▂▂▁▁▂▂▁▁▁▁▂
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▂▂▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▂▁▄█▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▃▄▁▁█
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▅▂▂▄▃▄▄█▄▄▇▁█▁▁▁▁
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▂▂▁▁▂▁▁▂▁█▁
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▁▄▁▂▁▁▂▂▁▁▁█▁▁▂▁▆▁▁
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▁▁▁▂▁▁▂▁▁▁█▁▃▂▃▁▁▃
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▁▁▂▅▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▁▁▁▁▁▃▂▁▁▁▁▁▁█▁▁▁▂
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▂▂▁▂▁▁▂▁▁▁▁▃▁█
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▂▁▁▂▂▁▁█▃▁▁▅▁▃▁▁▄▃▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▃▁▁▃▁▂▂▁▂▃▃▂▁▁▂█▂▂▅
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▂▁▇▅▇▂▂▁▂▄▁▂▂▆█▁▁▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▁▂▁▅▂▂▂▁▄▂▂▁▁█▂▁▁▁▇
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▂▂▂▂▂▂▂▂▃▂▂▃█▂▂▂▂▁▃▃
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch ▂▄▄▄▄▃▃▃▄▃▁▃█▄▄▂▄▄▅▇
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch ▂▂▂▁▂▂▂▂▂▂▁▂▂▄▁▁▂▂▂█
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch ▂▂▁▂▂▂▁▂▂▂▁▁█▂▂▂▄▃▂▂
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch ▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁█▂
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▂▁▁▁▁▁▃▆▅█▁▂▆▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▁▂▂▂▂▂▃▂▇▁▂▂▂█▅▂▁█▂
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▂▂▂▃▂▂▂▂▂▂▂▂▇▂▂█▂▇▄
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▂▁▁▁▁▂█▁▁▂▃▂
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▁▁▁▁▃▁▂▁▁▁▁▃▅▁▄█▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▂▂▂▂▂▂▂▁▂▂▂▂█▂▃▂▇▂▂
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▂▂▂▂▂▂▂▂▂▂▂▃█▁▂▂▂█▂▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch ▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂█▂▃▃▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch ▂▂▂▂▂▂▄▃▃▂▂▂▂▂▂▂█▃▂▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch ▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂█▂▂▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁█▁▆▁▁▁▁▁▁▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▂▁▁▁▃▃▁▂█▁▁▁▁▁▁▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▂▂▁▂▂▂▃▂▂▂▂█▂▂▂▂▂▂▂▂
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▂▂▅▁▄██▄▂▂▅▁▂▁▂▁▁▂▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▃▃▃▃▂▃▂▃█▃▃▂▃▁▃▂▃▃▃▃
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁▁▁▂█▂▁▃▁▁▁▁▁▁▁▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▂▁█▄▁▁▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▁▁▁▃▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▁▁▃▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂█
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▂▄▁▄█▂▄▃▄▁█▄▄▁▂▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁█▁▁▁▁▁▁▁▅▁▁▁▁▄▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▂▂▂▃▂▂█▂▂▂▂▂▂▂▂▂▂▂
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▇█▇▇▁▁███▇▇▇▇▇▇▇▇▇▆█
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▅▁▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▆▇▃▁▇▂▇▃█▇▆▇▇▇▇▇▇▇▇▇
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▆▇▇▇▁▇▇▇█▇▇▇▇▇▇▇▇▇▇▇
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▆▆█▆▆▆▇▆▁▆▆▆▆▆▆▆▆▆▆
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁█▁▁
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▂▁▁▁▁█▂▁▁▂▁▁▁▆
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂█▁▁
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁█▁▂▁▄▁▅▁▁
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch ▁▂▂▂▂▂▂▂▁█▂▂▂▃▄█▂▂▆▂
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch ▁▂▂▂▂▂▂▃▂▂▁▂█▃▅▃▂▂▃▂
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▃▂█▃▃▄▃▁▃▃▃▃▃▃▃▃▃▃▁
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▇█▆▇▇▆▆▇▆▆▆▁▆▆▇▆▆▇▆
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁██▇█████▇▁██████▇▇▇
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂██▆▂█▇▆▇█▇█▇█▇█▇▁▆█
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▆▇█▆▇▆▆▆▁▆▇▆▆▆▆▆▆▆▆▇
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ██████▁█████████████
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁█▁▁▁
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▄▂▁
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▂▇▁▁▁▂▁█▄▁▁▁
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▆█▁▁▁▁▁▁▁
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch ████████████▁██▄████
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▁▁▁▁▃
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁█▂▂▂▂▂▃▂▃▃▂▁▁▁▁
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▆▆█▆▆▆▇▇▇▇▇▇▆▆▅▁▅▆▄
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▆▆▆▆▆█▇▆▇▇▇▆▆▆▆▅▁▅▅▅
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▁▆▆▆▆
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ███████▁███████▇████
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▄▄▄▄▄▄▄▄▄▄█▄▄▄▄▁▄▄▄▄
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▆▆▆▆▆▆▆▅▆▄▃▆▅▇▂█▂▆▆▁
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch █████████▁▇████▂▁▂▅█
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch ███▅████▄▂▃███▂▇▁▄▁▇
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch ████▆███▇▄▇███▂▁▇▃▂▇
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch ████████▂▁▁███▇▁▅▂▇▂
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch █████████▃▁███▇▁▃▁▁▅
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ██▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▁███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▁███▆███▅▅▅▅▇▇▇▅▅▅▅▅
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▁▃▃▃▃▃▅▅▃▅█▃▃▅▅▅▅▃▇▃
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁█▂▅█▅█▅▅▅▁▁▁▅
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch ▁█▅█▅▁▅▅▅▁█▅▂█▁▁▅▁▁▅
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch ▃▆▁▃▅▁▆▁▁▁▁▁▁█▆▁▁▁▁▁
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch ▁████▁▁▁▂▅█▅▅▅▅▁▅▅▅▁
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▄▁▂▂▁
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▂▁▁▁▁▁▁
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▂▁█▄▁▂▁▂▂
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch ▅▅▅▅▇██▅▅█▆▇▅▇▇▁▇▇█▇
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch ▄▁▁▁▁▁▁▄▁▃▄▅▂█▂▃▃▂▁▂
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ████████████████▇██▁
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ████████████████▁██▄
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch ██████████▁████████▂
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch █▁████████████████▆█
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch ▅▄▂▅██▄▅█▄▁██▁██▁▁▁█
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch ███████████████▆██▁█
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▁▂▂▁▁▁▂▁▁▂▁▁▁▂▁▂▁█
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▃▃▅▃▁▅▅▁▁▁▃▁▂▁▅▅▁▃▃█
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch ▄▆▆▄▄▁▅▅▄▆██▅▆▅▄▅▅▅▆
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch █▁▅▂█▂▂▂▂▅█▂▂▂█▅█▅▂▅
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch ▅▁▁▅▁▁▁█▁▁▅▁▁▁█▁█▁█▁
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch ▃▆▆█▃▃▆█▃▆█▆█▆▆▁▄▃▆▆
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▅▇█▇▇▇▇▅▇▇▇▇▇▅▁▅▇▇▇▆
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▅█▅▅▆█▅▆▆▅▆▃▅█▅▅█▁▂
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁█████▁████████████▁
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▇█▇███▇▇█▇▁▇█▇██▇██
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ███████▁█▁██████████
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ████████▄████▁██████
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▁▂▁▆▄█▆▂▁▁▁▁▁▁▁▁▁▁
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▂▁▄▃▂█▁▁▁▁▁▁▁▁▁▁▁▁
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▂▁▂█▁▁▁▁▁▁▁▁▁▁▁▁
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▄▅▄▃█▂▂▁▁▂▂▁▁▁▁▁▂▂▁
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▃▅▅▅▃█▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁▂█▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▃▃▁▃▃▃▃▃▃▃▃▃█▃▃▃▃▆▃▃
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▃▁▃▂▃▃▃▃▃▃▃▃▄▃█▃▃▃▃▇
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▂▄▁
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁█▁▁▁▂
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▂▂▂▂▂▁▂▃▂▂▃▂▂▃▂▃▂█
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▂▃▂▁▄▄▂▁█▁▃▂▂█▇▂
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▄▅▅▄▅▅▃▅▅▆▅▅▅▅▅█▅▅▄
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▆▆▅▆▅▇▁▆▆▆▆▇▆▆▇▆█▆▆▆
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▃▃▁▃▁▃▄▄▃▅▃▄▃▃█▃▃▃▃▄
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch █▄▃▃▃▃▃▃▃▃▃▂▂▂▃▃▂▁▁▄
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch █▅▄▃▃▃▃▃▂▂▃▂▂▃▂▂▂▃▁▂
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch █▃▃▃▃▂▂▂▃▂▂▃▁▂▁▁▁▂▂▂
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch █▅▅▃▃▂▂▂▃▂▂▂▁▃▃▂▁▃▁▄
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch █▆▄▃▄▃▃▃▂▃▁▅▃▄▄▂▅▁▂▅
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch █▅▃▃▄▂▃▂▃▄▁▂▃▄▂▅▂▃▁▅
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▃▂▁▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▄▂▂▃▃▃▂▃▂▂▂▂▁▂▂▂▁▂▂
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▄▃▂▂▃▂▂▃▃▂▂▂▂▃▃▂▂▂▁
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▃▃▃▃▃▃▂▂▂▂▂▁▂▃▃▁▁▂▂
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▅▃▂▃▂▃▁▂▂▁▂▂▂▃▃▂▁▂▁
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▅▄▄▃▂▂▃▂▄▃▂▂▄▃▄▃▃▁▂
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch █▅▅▄▅▄▄▄▂▄▃▃█▂▂▁▃▂▇▂
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▆▅▅▅▅▅▅▄▄▄▃█▄▄▁▄▁▁▂▆
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch ▆▅▆▅▅▅▄▃▄▄▃█▄▂▃▃▃▃▁█
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch ▅▄▄▄▄▄▃▂▃▃▅▆▃▂▂▃▃▂▁█
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch ▆▅▅▅▅▅▄▄▅▄▄▆█▁▃▄▄▄▄▆
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch ▇▆▆▆▆▆▅▆▆▅▅▆▅▄▄▅▅▃▁█
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▃▃▃▃▃▂▃▃▃▃▅▃▂▂▂▂▁▁▁
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▃▃▃▂▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▄▄▄▃▃▄▄▄▃▃▃▄▂▂▂▂▁▁▁
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▄▅▄▄▅▅▃▄▄▃▄▄▃▃▂▂▁▁▁
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▅▅▄▃▄▅▃▄▅▄▄▃▃▂▂▂▁▁▁
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▇▆▇▇▆▆▇▇▅▇▆▆▅▄▃▂▂▁▁
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch █▆▅▅▄▄▁▃▅▅▅▅▅▂▁▃▅▅▅▄
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch █▇▇▅▅▄▇▄▅▆▆▆▆▁▃▅▆▅▅▄
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch ██▆▇▅▄▁▃▆▇▇▇▇▃▂▆▆▆▆▆
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch ██▆▅▅▄▁▅▇▆▇▆▇▄▃▆▆▅▆▅
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch █▇▅▇▃▁▃▇▆▇▇▇▇▄▇▆▇▇▇▄
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch ▇▇▄▇▄█▁▆▇▇▆█▇▂▅▆▇▆▆▅
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▄▂▃▂▃▃▂▂▂▂▂▂▃▂▂▂▂▁▁
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▄▂▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▁▁
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▄▃▃▃▃▃▄▃▄▃▃▄▃▃▂▂▂▁▁
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▅▄▃▃▃▄▃▃▅▄▃▄▂▃▂▂▁▁▁
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▃▄▄▄▄▃▃▄▃▅▃▃▃▂▂▂▁▁▁
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▄▄▄▄▄▄▃▄▄▅▃▄▃▃▂▃▁▁▁
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch █▄▃▃▃▃▃▃▃▁▂▃▂▃▅▄▄▄▄▄
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch █▃▃▃▃▃▃▁▁▂▁▃▃▃▄▃▄▄▅▅
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch █▅▃▄▃▄▁▁▁▁▂▃▁▄▆▄▅▅▄▅
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch █▄▅▅▃▄▃▂▁▂▄▄▃▅▅▅▅▅▅▆
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch █▆▆▅▂▄▃▅▁▂▃▃▅▅▇▇▆▇▆█
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch ▆▄▅▄▂▄▃▁▃▁▃▄▅▂▅▄▇▅▆█
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▅▃▃▃▃▄▂▃▃▂▃▂▃▂▂▂▁▁▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▄▃▃▃▃▃▃▃▂▃▂▃▃▂▂▂▂▂▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▃▃▃▃▃▃▃▄▃▃▂▃▂▂▁▂▁▁▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▄▄▃▃▄▄▄▄▃▃▃▂▂▂▂▁▂▁▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▃▃▄▄▃▃▃▃▃▄▃▃▂▂▃▃▁▂▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▅▄▄▄▄▄▃▃▄▄▃▃▂▂▂▃▁▁▁
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▄▃▃▂▂▂▂▂▂▂▄▃▄▃▂▂▁▃▇█
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▆▃▃▄▂▁▃▂▂▃▄▄▃▄▅▃▅▃█▅
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch ▄▃▂▂▃▁▂▂▂▄▂▃▃▂▄▃▂▂█▃
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch ▃▃▂▃▂▂▂▁▂▂▂▃█▃▅▄▂▄▃▃
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch ▄▂▃▂▄▅▃▁▃▅▅▃▆▄▄▅▂▃▄█
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch ▂▂▂▁▁▁▂▂▂▃▂▂▂▂▂▆█▂▂▃
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▇▃▂▁▁▅▁▁▁▁▁▁▁▁▁▁█▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▃▃▁▁▅▁▁▁▁▁▁▁▁▁▁▆▂▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▇▄▂▁▂▂▁▁▁▁▁▁▁▁▁▁█▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▃▁▁▁▂▁▁▂▁▁▁▁▁▁▁█▁▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▂▁▁▆▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▁▂▂▄▁▂▆▆▇██▅█▃██████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▅▄▇▁▄▂▃▃▂█▇▅▅▃▇█████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch ▂▂▃▇▅▁▄▂▆██▂▃▂██████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch ▁▂▃▄▇▂▂▁▇██▃▃▂██████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch ▄▃▄▄▆▂▁▃███▃▅▆██████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch ▄▄▃▄▆▂▁▃███▃▄▆██████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁██▆████████████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ██▇▁▇███████████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▆██▁▄███████████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▅█▃▄███████████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁█▆▂▂███████████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁██▄████████████████
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch █▃▃▃▁▁▂▁▄▁▁▁▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch █▄▆▃▂▂▂▂▂▂▁▂▃▂▂▂▂▂▂▂
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch █▆▆▃▂▁▁▂▁▁▆▁▂▂▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch █▆▂▂▁▄▃▁▂▁▄▁▁▁▁▁▁▁▁▁
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch █▄▃▄▃▃▃▂▃▃▃▂▂▂▃▁▂▁▂▃
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch █▅▅▅▄▄▃▃▃▅▄▁▃▃▅▂▂▁▃▂
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch ▄▄▄▄▃▃▃▁▂▂▂▁▂▃▂▁█▂▂▆
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch █▇▆▆▅▆▅▄▅▇▅▃▆▅▇▁▇▄██
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch ▅▅▅▄▃▄▄▄▄▅▄▄▄▃▃▁▄▅▂█
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch ▃▃▃▂▂▂▂▂▂▂▁▂▂▃▂▁▃▃▃█
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▃▂▃▃▂▆▁▂▁▁▁▃▃▃▃▃▃▃▃█
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▃▂▃▃▁▄▁▁▁▁▁▃▃▃▃▃▅▃▃█
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch ▃▂▃▂▁▁▁▁▁▁█▃▃▄▃▃▄▃▃█
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch ▅▃▅█▁▂▂▂▁▁▁▅▅▅▅▅▅▅▅▅
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch ▅▂▄█▁▁▁▁▁▁▇▄▄▅▄▄▅▄▅▄
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch ▃▃▃▁▁▂▃▁▁▁▃▄▃▃▃█▃▃▇▃
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch █▂▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch █▂▁▁▂▂▂▂▁▁▁▂▁▁▁▂▂▂▁▂
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch █▄▃▃▃▂▂▃▁▂▂▁▂▂▃▂▂▂▂▁
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch █▃▃▃▂▂▂▂▂▂▃▂▂▂▂▁▂▂▁▂
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch █▂▅▅▁▄▃▂▁▃▁▁▂▂▁▁▂▁▁▁
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch █▅▂▂▂▂▁▂▁▂▁▁▂▁▂▂▂▂▁▂
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▃▃▃▂▃▃▂▂▃▂▃▃▂▂▂▂▁▂▁
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▄▂▂▂▂▃▃▂▂▂▂▂▂▂▁▂▁▂▁
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▄▄▃▃▃▃▃▄▃▄▄▃▃▂▂▂▁▂▂
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▄▄▂▄▃▃▃▃▃▃▃▂▂▂▂▁▁▂▁
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▃▄▂▄▄▄▄▁▃▃▂▃▂▂▂▁▁▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▅▄▃▄▄▃▄▃▄▄▃▂▄▂▁▂▂▁▂
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▂▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▆▂▂▂▂▂▁▂▁▁▁▁▁▁▁█▁▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▃▃▂▂▂▁▄▄▄▁▁▁▁▁▂▂▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▄▃▂▃▂▂▂▃▃▂▁▁▁▁▂▁▁▁▁
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▃▄▅▃▂▁▂▁▄▁▄▁▁▁▁█▄▄▄▄
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▇▃▃▂▂▃▃▄▂▂▅▃▁▂▁▇█▆▆▇
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▄█▂▂▂▂▃▂▂▁▁▁▂▁▁▇▇▇▇▇
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▅▂▁▁▂▁▃▁▂▁▁▂▁▁▄▄▄▄▄
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▇▇▆█▃▅▇█▃▁▁▁▅▁▁▇▇▇▇█
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▅▃▄▅█▂▆█▃▇▂▁▄▁▁█████
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▅▃▂▂▂▂▂▂▂▂▁▂▁▁▃▁▁▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▅▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▅▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▅▃▄▂▂▃▂▃▂▂▂▁▁▂▁▁▁▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▅▃▃▃▂▂▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 1338.35383
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 25.93415
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch 35.85688
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch 151.34703
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch 0
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 5.85279
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 9.75999
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 18.25308
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 1.52965
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 13.06766
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 662.40527
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch 19.57052
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch 2678.5336
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch 20.70121
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch 1.13979
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.46306
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 33.16694
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 1.49497
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 4.36299
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch 0.54019
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch 357.09587
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch 0.21218
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch 0.41764
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.4348
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0.99576
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 7.96665
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0.69703
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.85724
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 304.77478
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch 0.80256
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch 16.87456
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch 0.11476
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch 5.38878
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 3.19364
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 22.86858
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 2.80581
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 2.53243
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 569.39018
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch 0.76141
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch 12.34649
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch 0.1776
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch 25.10834
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.49513
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 33.75519
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 1.81986
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 15.76802
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0.45686
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 10.22542
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 2919.47375
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 1061.21503
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch 1626.60882
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch 2178.40899
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch 4818.56805
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch 1046.38483
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 404.40335
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 9197.74934
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 505.06485
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 4784.03238
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 127.72926
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 397.23497
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 0.39086
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 8.48334
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch 0.48172
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch 5.07266
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch 0.24098
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch 0.46044
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 1.30622
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 5.21341
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch 96.84319
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch 58.41798
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch 0.21253
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch 0.6087
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 500.76553
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 848.31185
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch 3017.76614
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch 0
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch 0
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 12.56974
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 102.17028
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch 0.45103
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch 6.94418
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch 0.19551
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch 0.28789
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2.85866
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 28.66875
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 4.62853
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0.69183
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.56306
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 7.66946
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 25.16526
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 214.4528
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 29.40169
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 27.1077
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 386.12874
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0.96744
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 131.03773
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 1.24719
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 63.7462
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 13911.0804
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 1.98694
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch 5.80932
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch 31.47684
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch 0
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.17187
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 2.15456
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 1.67925
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.01709
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch -13.41728
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 49.22818
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch -18
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch -435
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch -129
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch 0.31441
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.08578
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch -0.2099
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -0.02298
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 25.79654
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch -2
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch 155.56028
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch -2
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch -1
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0.08066
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch -0.01395
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch -0.05017
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.01263
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -0.01319
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 96.78604
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch -1
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch -2
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch -1
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch -1.11906
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -1.16522
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.27191
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch -0.0757
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -0.03882
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch -119.67147
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch -2
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch -2
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch -1
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch 20.1689
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.01583
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.36844
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch -0.03777
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch -0.13741
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.00686
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -0.13704
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch -1685.38601
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch -13.48799
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch -242.8026
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch -266.09412
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch -1810.88027
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch -735.81775
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -511.06128
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -511.18448
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch -511.02539
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch -511.09275
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -511.00474
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -511.01216
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch -2
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch -2
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch -1
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch -1
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch -2
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch -2
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch -2
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch -1
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch 54.70181
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch 1.61514
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch -1
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch -1
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch -458.60642
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch -107.01364
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch -306.07104
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch 0
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch 0
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 14.21817
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 1.95002
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch -1
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch -1
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch -2
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch -1
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.16835
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.25051
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch -1.05174
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.00617
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.0853
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.0671
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch -0.15834
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2235.53197
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 5.83168
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 9.79441
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 8.96358
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0.05191
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch -0.71552
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.00052
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.34434
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 841
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 771
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch 792
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch 840
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch 819
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch 832
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 35
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 32
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 22
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 36
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 27
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 31
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 736
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 856
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch 898
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch 999
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch 870
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch 906
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 2
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 2
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 5
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 2
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 810
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 803
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch 812
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch 811
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch 802
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch 812
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 5
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 5
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 1
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 6
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 4
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 1
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 840
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 861
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch 831
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch 846
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch 847
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch 856
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 19
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 9
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 14
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 7
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 8
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 10
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 992
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 860
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch 821
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch 827
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch 891
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch 861
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 8
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 2032
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 2020
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch 2035
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch 1993
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch 2038
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch 2041
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 511
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 511
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 511
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 511
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 511
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 511
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch 205
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 816
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 795
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch 866
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch 841
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch 872
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch 983
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 486
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 465
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch 486
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch 206
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch 200
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch 200
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 203
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 207
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch 203
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch 205
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch 205
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch 206
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 13
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 18
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 23
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 12
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 24
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 28
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 7
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 3
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 16
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 3
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 8
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 101
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 102
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 101
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 100
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 105
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 100
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 5
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 9
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 6
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 15
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 8
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 4
wandb: 
wandb: 🚀 View run tune_1225_2215 at: https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/oi3i2q25
wandb: ️⚡ View job at https://wandb.ai/mattstachyra/reinforcement-meta-learning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzY1Nzc3Ng==/version_details/v57
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231225_221533-oi3i2q25/logs
