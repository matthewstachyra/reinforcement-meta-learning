wandb: Appending key for api.wandb.ai to your netrc file: /cluster/home/mstach01/.netrc
2023-12-23 23:16:22.871113: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
wandb: Currently logged in as: mattstachyra. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/wandb/run-20231223_231629-9vde38bb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tuning_1223_2316
wandb: ⭐️ View project at https://wandb.ai/mattstachyra/reinforcement-meta-learning
wandb: 🚀 View run at https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/9vde38bb
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  tasks_targets = torch.tensor(np.array([
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tasks_targets = torch.tensor(np.array([
wandb: ERROR Error while calling W&B API: dial tcp 35.226.229.132:3307: connect: connection refused (<Response [500]>)
wandb: ERROR Error while calling W&B API: dial tcp 35.226.229.132:3307: connect: connection refused (<Response [500]>)
wandb: Network error (ReadTimeout), entering retry loop.
/cluster/home/mstach01/condaenv/mthesis/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 5
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=5 and n_envs=1)
  warnings.warn(
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: \ 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: / 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: - 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▂█▁▂▂▁▁▂▆▁▄▁▁▁▄▁▁▂▂▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▅▂▃▄▃▁▂▁▂▅▂▃█▂▃█▄▂▃▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▂▁▃▁▁▁▄▁▂▇▁▁▁█▂▁▁▁▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▃▂▄▂▂▂▃▂▁▁▆█▄▃▁▁▁▂▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▁▂▂▄▁▃▂▂█▄▂▂▁▇▃▂▁▅▂▆
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▂▃▁▁▁▁▁▁▁▁▁▃█▁▁▁▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▂▃▂▁▁▂▁▁▁▂▂▂▃▁█▂▂▂▂
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁█
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▂▁▄▂▂▆▃▁▁▂▂▃▂▂▇▆▁▂█▆
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▁▁▅▁▃▁█▂▂▁▁▂▁▁▂▁▁▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂█▃█▅▃▂▇▂▁▃▁▃▅▅▅▁▁▁▂
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▃▃▁▁▂▂▂▁▁▂▁▂▁▁█▁▁▁▁▁
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▂▄▆▁▃▂▁▂▁▁▂▁▃▃▂▁█▂▁▅
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▂▁█▁▃▁▁▁▁▂▁▂▁▂▁▂▁▁
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▁▂▃▁▁▂█▁▁▁▁▁▁▁▁▁▁▁
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▂▂▁▂▁▁▂█▁▃▂▃▁▁▂▃▁
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▂▃▅▃▃▂█▃▁▃▂▄▄▂▁▁▅▂▂▁
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▂▂▁▁█▄▁▆▁▂▁▁▂▁▂▁▂▁
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▄▁▂▆▁█▄▁▅▄▃▄▅▄▄▂▄▄▃▄
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▂▁▃█▁▁▁▁▄▁▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁█
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▆█▁▁▁
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▅▃▁█▁▁▃
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▁▁▁▁▂▁▁▃▂▁▁▁▂▂▂█▁▄
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▃▆▂▄▂▃▄▆▂▁▃▁▄▄█▁▂▁▇▃
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▂▁▁▁▁▁▅▇▇▁▁▃█▇▁▃▃▂
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▅▆▄▂█▁▃▅▇▃▃▃▁▄▄▃▄▁▃
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁█▃▁▂▅▁▁▁▃▁▁▁▁█▅▂▃▁
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▂▁▁▁▁▃▃▅▂▂▁▁▃▂▂▁█▅▇▃
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▄▂▂▁▃▃▃▁▂▄▁▃▁▃▁▂▃▂▃
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▁▄▁▁▁█▄▂▁▂▁▂▂▂▁▁▂▂
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂█▂▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▂▂▂▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▃▂
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁█
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▂▁▁▁▁▁▁▂▃█▄▂▁▁▁▁▂▁
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▂▁▂▁▂▁▁▂▁▁█▅▂▁▁▁▁▁
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▇▇▂▆▃█▂▄▂▁▁▄▂▅▅▇▃▆▅▃
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▂▁▄▂▂▁▂▁▁▃▁▁█▂▃▂▂▂
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▅▅▃▂▄▅▃▇█▁▆▅▃▂▄▂▅▁
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▂▂▃▂▂▂█▂▆▂▂▂▄▂▂▃▁▃
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▁▃▅▄▅▆▅▆▃▁▃▂▇▅▃█▃▄▄▆
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▂▁▁▃▁▁▁█▁▁▁▁▁
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▁▂▁▂▂▂▃▂▂▂▃█▄▂▂▂▂▂▂
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▇▃▃▁▇▁▃▅▆▄▃▃▇▅▃▅▇█▅▃
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▂▄▁▃▁▁▂▃▁▁▁██▁▁▁▁
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▁▃▂▃▃▄▃▃▃▃▂▄▅▃█▁▃▂▃
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁█
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▆█▆▆▇▅▆██▆▆▆▆▇▆▇█▇▁▃
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▂▁▁▁█▁▁▁▁▁▁▁▁▁▁▁
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▂▃█▂▄▄▄▄▃▄▃▃▃▃▄▄▄▃
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▂▂▁▁▂▁▃▁▁█▁▁▁▁▁
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▃▃▁▅▃▄▅▃▅▅▃▅▃▃▂▄█▃▅▄
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▂▁█▁▄▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▂▃▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▂▂▁▂▂▂▂▁▂▃█▂▇▁▆▂▁▁█▂
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▂▂▃▂▃▂▇▃▃▃▂▁▂▂▄▃█▂▂▄
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▂▂▂▁█▄▁▆▁▁▁▁▁▁▁▁▁▁
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▆▆▃█▆▆▇▆▄█▆▁▅▅▅▆▅▅▆▅
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁█▁▁
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ████████▁██████████▇
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▇▇▇▇▇▇▇▇█▇▇█▇▃▁▁▁▇██
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▇▇▇▇▇▇▇▇▇▇▇▇▇▁▁▇▁▇█▁
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▂▂▁▁▁▂▁▁▁▂█▂▂
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▅▁▂▂▂▁▅█▂█▁▂▁█▂█▂▂
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▄▄▄▄▄▄▅▆▅▄▄▅█▆▄▄▄▄
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▄▁▄▄███▇▄▇▇▇█▇▇▇▇█▇
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▂▂▂▂▂▁▃▂▂▂▂▂▂▂▂█▃▁▃▂
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▄▆▆▅▅▄▃▄▃▄▅▅▁▅▄▅█▆▅▅
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▅▆▆▅█▇▆▅▆▄▅▅▅▅▅▅▅▅▅
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▁▆▇▆▆▆▃█▆▆▆▆▆▅▆▆▆▆▆
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▁▃▄▃▄▂▂▄▄▂▂▄▃▃▃▂█▄▄▃
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▇▇▇▇▇▇█▇█▇▁▇██▇▇▇▇▇█
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁█
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▄▄▃▅▅▄▅▅▅▄▃█▄▄▅▅▄▃▁▄
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▂▂▂▂▂▂▁▂▂▁█▂▂▂▂▂▂▂
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▄▄█▃▄▄▆██▄▆▄▃▁▆▆▄▃
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▂▂▁▂▂▂▂▂▂▂▂▂█▂▂▂▂▂
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▇▃▇▅▃▇▅▇▇▅▆▇▄▇█▇▇▇▁▇
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch █▆▄▂▃▂▂▃▂▂▂▂▂▂▁▂▂▁▂▁
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch █▄▃▃▃▁▂▂▃▁▂▂▂▂▂▁▁▁▁▂
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▆▆▅▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch █▄▄▃▃▃▃▃▂▂▂▃▁▂▂▂▂▂▂▁
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch █▄▄▄▂▃▃▃▃▂▂▂▂▁▃▂▁▁▂▁
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▆▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▆▄▃▃▃▃▂▃▃▂▃▂▂▂▂▂▁▁▁
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch █▄▄▃▃▃▃▃▃▃▃▃▃▃▁▃▁▁▂▃
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch █▇▆▅▅▅▅▄▄▅▅▅▅▄▃▂▃▁▄▁
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▆▄▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch █▄▃▃▂▂▂▂▂▃▂▁▁▁▂▂▁▁▂▂
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch █▄▃▂▃▃▃▂▃▂▂▃▂▁▂▁▁▂▁▂
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▆▅▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▅▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch █▃▂▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch █▆▅▃▃▂▁▃▃▂▄▁▂▄▂▂▁▄▃▂
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▅▂▃▃▃▂▂▂▃▂▂▁▁▁▃▂▁▁▁
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▄▃▄▄▄▃▂▂▃▃▂▁▁▁▁▂▁▁▁
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▃▂▄▃▅▄▂▅▁▁▅▂▅▆▅▄▂█▄▅
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▄▄▄▃▂▆▃▄▆▁▃▃▂▅▃▃▃▅▅█
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▃▁▁▄▇▅▅▄▃▃▃▃▃████▃▃▃
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▁▁▅▇▄▄▄▃▃▃▃▃██▃█▃▃█
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch █▅▄▃▃▂▂▂▃▂▂▂▂▂▁▁▂▂▂▂
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch █▃▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▅▄▃▂▂▂▂▁▂▂▂▂▂▁▁▁▂
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▆▅▅▄▂▃▂▂▂▁▂▁▁▁▂▁▂▁▁
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch █▅▄▄▃▃▄▄▃▃▃▂▂▂▃▂▃▁▂▅
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch █▄▅▄▄▄▄▄▄▃▂▃▃▃▃▃▃▂▁▄
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▃▂▂▂▂▂▂▃▂▁▁▁▁▁▁▁▁
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch █▅▃▃▄▃▄▅▃▃▄▄▄▃▃▃▃▁▃▃
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▃▂▂▂▂▁█▁▁▂▃▂▂▃▂▁▁▁▂▁
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇█
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▅▂▄▁▁▁▁▁▁▂▁▁▁▁▁▁▁▃█▂
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch █▅▅▃▃▃▃▂▂▂▂▂▃▁▂▃▁▁▁▂
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ███▅▆▇▅▆▅▅▄▅▃▇▁▁▂▄▄▆
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▆▅▄▃▂▂▂▁▁▂▁▂▁▁▁▁▂▁
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▆▅▄▃▂▂▂▂▁▁▁▁▂▁▁▂▂▁
wandb: 
wandb: Run summary:
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 0.82855
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 3.17906
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 734.37757
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 51.41637
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 3.18794
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 23.22619
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 91.60523
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 42.47191
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 7.4546
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 77.57421
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 16.61787
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 22.7615
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 2.65657
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 16472.38264
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 7.38219
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 18871.9535
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 28.48648
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 20.02573
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2.32895
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 23.36341
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 37.14147
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2.35632
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 15.33731
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 1.96879
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 66.4678
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 245.14685
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 4.07914
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 18.83499
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 9.53077
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 0.04105
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.14747
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 2628.05154
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch -2.19817
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.16042
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.15149
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 62.08057
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch -4.62113
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.04858
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch -0.92907
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.10809
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.14811
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 4.99095
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch -41.94952
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 159.27409
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -2047.05922
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch -0.01121
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch -1.98855
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.11087
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.15069
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch -1.62681
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.12022
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.09745
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch -1.72155
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 1.76144
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 191.56273
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -1
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch -2.03264
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.19738
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 773
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 812
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 34
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 25
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 779
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 780
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 41
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 32
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 820
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 743
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 8
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 7
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 823
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 810
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 13
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 12
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 823
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 801
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 11
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 7
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 1126
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 1623
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 820
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 2047
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 207
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 206
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 52
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 45
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 889
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 842
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 11
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 17
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 205
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 204
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 360
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 116
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 201
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 218
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 39
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 28
wandb: 
wandb: 🚀 View run tuning_1223_2316 at: https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/9vde38bb
wandb: ️⚡ View job at https://wandb.ai/mattstachyra/reinforcement-meta-learning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzY1Nzc3Ng==/version_details/v51
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_231629-9vde38bb/logs
