wandb: Appending key for api.wandb.ai to your netrc file: /cluster/home/mstach01/.netrc
2023-12-25 22:15:27.286390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  tasks_targets = torch.tensor(np.array([
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tasks_targets = torch.tensor(np.array([
wandb: Currently logged in as: mattstachyra. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/wandb/run-20231225_221533-b2a1c3kf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tune_1225_2215
wandb: ⭐️ View project at https://wandb.ai/mattstachyra/reinforcement-meta-learning
wandb: 🚀 View run at https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/b2a1c3kf
/cluster/home/mstach01/condaenv/mthesis/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 5
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=5 and n_envs=1)
  warnings.warn(
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▅█▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▂▄█▃▂▁▁▁▄▁▅▂▁▃▅▁▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch ▄▇▁█▁▂▃█▁█▁▃▁▅▂▂▄▁▄▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▂▂▁█▁▁▁▂▁▂▄▅▁█▃▄▃▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▂▁▁▁▁▂▁▂▃▁▁█▁▂▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch ▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▅▁▁▂█
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▂▂▁▁▂▃▇▃▃▂▁▁▁▂▁▄▅█
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▃▁▂▁▁▂▂▂█▃▃█▁▂▂
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▂▁▁▁▂▂▂▁▅▂▁█▂▁█▁▁▂
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▁▁▂▁▃▁▂▁▁▁▁▃▂▂█▁▂▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▂▂▂▂█▁▁▂▁▂▅▂▁▁▅▃▁▁▂
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▂▁▄▂▁▂▁▂▃▁▁█▄▄▁█▁▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▃▁▁▁▁▁▁█▁▁▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▃▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▄█▁▁▁▁▁▁▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁█▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▂▁▁▁▂▁▂▁▂▂▁▃█▂▂▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▂▁▁▁▁▂▂▃▂▁▁▁▂▃▃█▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▂▂▃▁▇▂▁▃▂▂▁▂▂▂▁█▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▂▁▁▁▂▂▂▁▁▃▁▁▅▁▁▁▄▃█
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▂▁▁▂▁▁▁▃▁▁▁▄▁█▂▃▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▁█▁▅▁▂▃▅▂▆▅▆▁▁▁▁▂▁▁▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁█▁▁▂
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▂▁▁█▁▁▁▁▁▁▂▁▁▂▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▂▁▁▁▁▁▁▁▁▁█▂▁▁▁▁
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch ▂▁▁▁█▁▅▁▁▁▁▁▂▁▁▂█▁▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▂▂▁▂▁▁▂▁▁▅█▁▂▂▂▃
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▂▁▁▁▁▁▁▁▁▁▆█▁▁▁▂
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▂▁▁▂▁▂█▁▁▃▁▁▁▁▁▂▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▃▄▁▄▃▂▁▁▁▁▁▂▅▁▃▂▃█
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▂▁▂▂▂▁▁▁▁▁▁▂▂▁▁▁█▁▁
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▂▂▂▁▁▁▂▁▂▁▁▄▅▂▁▁█▁▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▁▂▁▆▂█▁▂▁▂▁▁▅▁▁▁▃▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁█▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▅█▁▁▄▁▁▂▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch ▁▂▁▁▁▂▃▁█▃▁▁▁▁▁▁▂▁▂▆
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▃▁▁▁▁█▁▄▁▁▅
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▃▂▄▃▂▂▂▃▁▁█▃▂▂▁▁▁▂▁
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▄▅▂▂▃▂▃▁▄▂█▁▁▁▁▁▁▁
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▂▂▄▄▅▁▁█▁▁▁▂▂▂▁▁
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▂▁▁▁▄▃▁▄█▂▁▁▁▁▁▁▁▁▁
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▂▃▃▃▁▂▃▂▁█▂▂▁▁▂▁▃▃▃
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▂▁▁▁▄▂▂▂█▄▂▁▁▁▁▁▁▁▁
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▂▃▂▁▃▂▁▃▂▂▆█▁▁▂▂▃▁▇
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁█▁▁▁
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch ▁▃▁▁▁▂▁▁▂▁█▁▁▁▁▁▁▆▁▂
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▂▁▁▅▁▁▂▂▁▃█▁▃
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch ▁▂▁▁▂█▁▄▁▂▁▂▁▁▁▁▁▁▂▇
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂█▄
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▆█▇
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▃
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃█▄
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▂█▇
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁▂▁▁▁▁▁▁▁▁▂▂▁▁▇▂▆█
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▃▁▂▁▁▁█▂▇▅▂▄▅▄▄
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▁▃▁▁▁▁▁▁▁▁▁▄▃▇▂█▃▅▂▁
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▄▂█▂▃▁▂▁▂
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▂▁▂▄▂▃█▄▁▁
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▃▂▅▅▆▄█▃▇▆
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▂█▂▅▂▂▃▁
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▃▄█▁▄▁▇▁▁▃▅▄▄▂▄▆▄
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▂▂█▁▂▁▁▁▁▆▃█▅▄▆▃▃
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▂▂▂▁▃▁▁▁▁▃▂▄█▂▅▂▃
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▂▄▁▁▁▃▁▁▁▁▄▁▅▃▇▄▄█
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▃▂▁█▅▄▃▁▁▁▄▂▃▃▂▂▃▆
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▃▃▇▇▄▃▁▁▁▃▄▅▃▄▅▃█▇
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▃▁▁▇▁█▇▁▄▅▄▇▄▄▅▄▅▆▇▇
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▄▁▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch ▁▂▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▂▁▂▂▁▁▁▁█▁▁▁▁▁▁▁▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch ▅▂▂▂▁█▁▁▁▂▁▂▂▁▂▂▂▁▂▄
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▃▂▁▁▁▁▁▁▂▁▁▃█▂▁▄▁▁▄▃
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▂▁▁▁▁█▃▂▁▆▁▁▁
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▂▁
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▂▄▁▂▂▂▁▁▃▁▅▁█▅▂▁
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▂▁▁▁█▂▁▁▁▁▂▂▂
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▂▁▁▁
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▄
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▃▃▁█▅▂▁▁▁▆▆▁▃
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁█▁▁
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▃▁▁▁▁▂▂▁▁▁▁█▁▁▁▁▃
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▂▆▂▂▁▃▁▄▂▁▄▅▁▆█▁▆▃▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▂▁█▂▁▃▁▁▁▁▁▃▃▁▄▁▁▁▄▁
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▂▁▁▆▁▁▃▁▂▃▁▁█▁▁▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch ▁▂▁▁▃▄▂▁▁▁█▁▁▁▁▁▁▄▁▁
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch █▁▂▁▁▂▃▁▅▆▄▃▂▁▃▂▃▅▁▂
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch ▃▁▅▄▂▇▁▂█▂▄▁▁▁▁▁▁▁▁▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▆▂▆▆▁▂█▇▄▅▂▂▅▅▆▃▆▃▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▂▄▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▅█▅▆▄▂▄▂▃▄▄▁▂▁▃▆▃▃▁▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▁▂▂▃▂▁▁▂▁▁▂▂▂▁▁▂▁▁▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▃█▆▄▄▆▁▁▅▄▃▆▅▂▂▁▃▄▂▄
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▅▁▃▆▁█▅▃▄▆▄█▄▄▃▄▇▁▆▆
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▆▃▃█▁▃▆▂▃▄▁▁▅▄▅▄▄▄
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▂▃▂▆▂▁▂█▁▁▁▁▁▁▁▁▁▁▁
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▂▁▃▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▃▁▁▆█▁▇▂▁▃▃▁▃▂▁▂▃▃▃
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▅▃▅▇█▄▄▅▃▂▃▃▁▃▃▁▃▁▁▂
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▆▂█▂▁▁▅▂▂▁▁▁▁▁▁▁▁▁▁
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▂▁▁▂▁▁▁▂▂▁▂▁▂▁▁▁▁█
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁█▁▁▁
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▁▁█▁▁▁▂▁▂▁▁▁▁▂▁▁
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▂▁▁▂▁█▂▁▁▃▄▆▂▂▂▄▂▁▂
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁█
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▃▃▂▁▁▁▂▂▂▂▂▁▂▁▄█▃▄█▂
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▂▄▁▁▁▁▂▂▂▁▂▄▂▆█▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▁▂▂▂▃▂▂▁▃▅▂▂█▃▄▅
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▂▁▂▂▄▂▃▁▁▁▁▁▁▄▃▁▁█▅
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▂▁▁▃▂▂▂▁▁▂▂▃▃▃▃▁▁█▁
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▂▁▁▁▁▄▁▁▁▄▁▄█▅▃▄▄▆▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▂▃▃▃▃▅▃▃▂▂▃█▂▄▁▃▃▆▃▃
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch ▃▄▄▄▅▃▄▆▅█▄▃▅▆▅▃▁▅▅▅
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch ▆▅▅▆▅▅▅▅▅▆▅▆▅▇▆█▅▁▄▆
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch ▃▂▃▃▃▄▃▃▃▁▄▃▁█▄▃█▃▄▂
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▄▂▂▂█
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▄▁▃▃█
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▅▃▃█▁▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▂▁▁▁▁▁▁▁▃▁▁█▂▁▅▁▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▁▁▁▁▃▁▁▁▂▁▂▃▃▄█▁▂▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▃▃▃▃▃▁▃▃▃▃▄█▃▃▃▆▅▃▃▃
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▃▃▃▃▃▃▃▃▃▅▃▃▃▇▁█▃▁▃▃
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▄▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▂█▁▁▁▁▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▂█▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▂▁▁▃█▁▁▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▁█▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▂▁▁▁▅▁▁▆▂▂▁▂▂▂▁█▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ███████████████▁████
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▂▂▂▂▂▁▂▂▂▂▂▂▂▇▂▂▂▂▃█
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▂▂▂▂▁▂▂▂▃▂▂▁█▂▃▂▃▂▂
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▂▁▅▂█▁▁▁▁▁▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▃▁▁▁█▁▁▁▁▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁█▁▁▁▁▁▁▃▁▁▂▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▂▁▁▁█▂▁▁▁▁
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch ▁▂▂▁▃▂▁▂▁▂▂▂▂▂▂▁█▁▂▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▁▁▁▁▁
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▁▁▁▂
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▃▃▃▃▃▆▃▁█▃▂▆▃▃▃▃▂▇▃▃
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▂▁▂▂▂▂▂▂▂▂▂▂▂█▂▁▂▃▃
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▂▃▂▁▂▃▃▂▃▁▁▃▄▂▃▃█▃▃
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▄▄▄▄▄▄▄▄▃▄▂▃▁▃▄▃█▄▄
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▂▁▁▁▁▁▁█▁▁▁▂▁
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁█▁
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▂▃▁▁█▁▁▂▁
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▂▃▁▁▁▁▁▁▁▁▂█
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▂▁▁▁▁█▁▁▁▁▂
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▄▁▄▃▃▄▄▄▄▄▄█▃▃▃▃▃▃▃▄
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▃▅▃▂▃▃▃▂▃▂█▂▂▂▂▂▂▂
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▃▃▃▄▄▂▁▆█▃▃▂▄▃▃▃▃▃▃▃
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▂▂▂▂▂▃▁▂▂█▂▂▂▂▂▂▂▂▂▂
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ██▇▇▁███▇█▇█▇██▇█▇▇▇
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▂▃▃▃▄▃▃▁█▃▃▂▂▃▂▂▂▃▂
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▂▁▁▄█▁▁▁▁▁▁▁
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▂▁▁
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch ▂▂▂▁▂▂▂▃▂▂█▂▂▂▃▂▂▅▂▂
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch ▂▂▂▁▂▃▁█▂▂▂▆▁▂▂▁▂▂▂▃
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▃
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄█▅
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▂
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▇
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▆█
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch █████▇█▇███▄▆▂▅▇▄▁▃▄
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch █▅████████▇▂▅▆▂▂▁▂▃▁
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch ███████████▄▂▆▁▂▅▂▃▃
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch ██████████▄▇▄▂▇▄▁▂▇▁
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch ██████████▆▆▃▄▄▆▂▆▁▁
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch ████▄█████▄▇▁▁▂▂▄▁▅▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ███▁▁▁█▁█▁██▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ███▁▁▁█▁████▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ███▁▁▁█▁████▁▄▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ██▁▁███▁████▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ██▁▁█▁▁▁▁███▁▁▁▁▁▁▁▁
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ██▁▁▁▁▁▁███▆▁▁▁▁▁▁▁▁
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▅██▁█▁▁█▅▁▅▁▅▅▁▅▅▁▁▁
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▁█▁▅▂▅█▁▁█▁▁▁▁▁▁▁▅▁▅
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch █▃▃▅▁█▃▃█▃█▃▃▃▅▃▃▃█▃
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch ▁▅▁▁▃▂▅▁▁▁▇▁▁██▁▁█▅▁
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch ▁▃▃▃▆▃▆██▃▆▃▃█▃▃▃▆▃▄
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▂▁█▁▁▁▁▁▁▁▁▁▂▁▁▁
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▂▂▁▂█▂▁▂▁▁▁▂
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▂▁▁▃▁▁▁
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch ▅▄▅▄▄▅▅▄▅█▅▅▁▅▅▅▆▆▅▅
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▁▇▆▇
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▄▄▄█
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇▆▇▇█
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch ▇▇▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇█▇▇
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch ▃▂▁▅▂▁▁▃▂▃▁▁▃▁▁▃▁▃▃█
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch ████████████▁███████
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch ████████████▇███▁███
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▃▂▂▂▃▁▃▂▂▃▁▂▃▂█▃▅▁▃▃
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▃▂▁▂▂▂▂▃▂▄▃▅█▂█▃▃▃█▃
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch ▅▂▄▁▄▅▃▅▅▃▂▃▃▄▄█▂▄▂▅
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch █▁▆▆▃▄▃▆█▃▂▃▆▃█▆▆▇██
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch ▁▆▄██▄▆█▄▃▄▄▄█▅▄▄▄▆▄
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch ▁▁▁▁▁▁▅▁▁▁▁█▅█▁▁█▁▅█
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▅██▇▇██▁▂▇▇██▇▇▇▇▇▇█
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▅███▁██████████▄████
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▃▁█▃███████████▃████
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▅██▂▁████████▇██████
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▅▁▄██▇██▇██▇▇███████
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▆█▆▄█▁▄▇▅▃▅▁▅▅▆▅▂█▃▃
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▂▅▇▃▇▂▂█▂▂▁▂▂▁▁▁▁▁▁
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▄▅▅▇▄▄▄█▄▄▄▄▄▄▄▄▄▄▄
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▃▂█▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▅▅▅█▆▅▆▅▅▅▅▅▅▅▅▅▅▅▅
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▃▄▄█▃▂▂▃▃▃▂▃▃▂▃▃▃▃▃
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▂▅█▂▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁█
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁█▁▂▁
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▁▁█▁▁▂▂▁▆▁▁▁▁▁▁▁
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁█
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▂▂▂▂▂▂▂▂▂▂▂▂▅█▂▃▄▂
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂█▃██▁
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▂█▃▃▃▃▅▄
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▁▁▁▂▂▂▂▁▁▁▁▁▁▆▃▁▁█▄
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▅▄▅▅▅▅▄▅▅▅▅▅▆▆▆█▆▅▁▅
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▃▃▃▃▄▁▃▃▄▄▃▇▄█▄▆▅▆▃
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch █▄▄▃▃▃▃▃▃▃▃▂▃▂▃▃▄▂▂▁
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch █▄▄▃▃▃▃▃▃▃▃▂▂▃▂▁▁▄▂▃
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch █▅▅▄▅▅▅▅▅▄▅▄▅▄▄▄▅▅▄▁
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch █▅▃▃▄▃▄▄▄▃▃▁▄▃▂▃▁▆▁▄
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch █▆▅▅▅▄▅▅▆▆▄▅▅▄▅▄▁▃▃▁
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch █▇▆▆▅▅▆▆▄▅▆▅▅▅▅▁▄▃▂▁
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▄▂▂▂▁▁▁▂▁▁▂▁▁▂▃▃▃▂▁
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▄▂▁▂▂▂▁▂▁▂▂▁▁▂▄▃▃▂▁
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▃▂▂▁▂▃▂▂▁▂▂▂▁▂▄▄▅▂▁
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▃▂▂▂▃▃▂▃▁▃▂▂▂▂▄▅▆▂▂
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▄▃▃▂▃▃▃▃▁▃▃▃▃▃▄▆▅▃▃
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▄▄▂▁▂▁▂▁▃▂▂▁▂▄▁▇▃▂▂
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch █▄▃▃▃▃▃▂▂▃▃▃▃▃▄▃▃▁▂▃
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch █▃▃▃▂▃▁▂▃▂▂▃▃▃▄▃▂▃▂▂
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch █▅▅▄▄▃▂▂▂▄▂▄▄▄▄▄▁▁▁▃
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch █▇▆▅▆▁▃▄▄▄▄▅▅▆▆▅▄▄▄▅
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch ██▆▆▄▃▂▄▄▆▆▆▆▆▆▆▁▄▅▅
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch ▇▇▇▅▆▁▄▆█▇▇█▇▇█▃▄▅█▇
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▂▂▁▁▁▂▁▂▁▁▂▁▁▂▂▂▂▃▂
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▃▃▂▁▃▃▂▂▂▃▁▂▂▂▂▂▃▄▁
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▂▂▂▃▂▁▂▂▂▂▁▁▁▂▂▂▂▂▂
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁█▂▂▃▂
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▅▂▂▁▂▂▂▂▁▂▂▂▂▂▁█▄▃▃▄
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▇▂▅▁▂▄▃▃▁▅▄▄▁▁▃▅▁▃▅█
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch █▄▁▃▂▃▃▂▂▁▁▁▁▂▁▂▁▂▂▂
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch █▃▃▃▂▃▃▁▁▂▂▃▂▂▂▂▂▂▂▂
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch █▅▃▃▃▄▃▁▂▂▁▂▁▂▂▂▃▂▂▂
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch █▆▁▇▂▃▄▃▄▁▂▃▁▂▃▂▂▃▂▃
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch ▅▄▅█▃▅▄█▃▂▃▃▁▄▄▄▅▄▄▄
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch █▁▅▃█▅▅▃▄▂▃▂▃▄▃▂▃▃▃▃
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▄▁▂▂▂▂▁▁▄██▅▅▅▅▅▅▂▂
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▃▁▁▁▁▁▂▁▅██▅▅▅▅▅▅▁▁
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▇▃▂▁▁▁▁▂▂▄██▅▆▅▅▅▅▁▂
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▅▂▂▁▁▁▂▁▂▄██▅▅▅▅▅▄▂▁
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▅▂▁▁▁▂▂▁▂▅██▅▅▅▅▅▃▁▁
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▅▁▁▂▁▂▁▁▁▇█▇▅▅▅▅▅▁▂▂
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch █▄▄▄▃▃▂▄▃▁▃▃▄▃▄▃▄▄▃▄
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch █▂▃▄▃▁▂▃▂▁▁▂▃▂▂▂▂▃▃▃
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch █▇▅▂▁▃▂▂▄▂▄▄▃▃▅▅▅▄▅▅
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch █▆▆▇▇▇▁▇▃▂▇▇▆▆▇▆▆▅▇▇
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch █▇▂▄▁▁▄▅▄▆▆▆▄▆▇▆▆▅▆▆
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch ▆█▇▄▄▂▇▁▄▅▇█▆▅▆▇▆▅▆▆
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▃▃▃▃▂▂▂▁▂▂▂▃▂▃▂▁▂▂▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▃▂▃▂▃▂▂▃▃▂▅▂▂▂▂▂▁▂▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▂▃▃▅▂▃▃▂▂▁▃▂▁▂▁▁▂▂▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▃▄▃▂▃▂▃▃▃▂▂▃▂▂▂▂▂▃▁
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▄▄▄▄▅▃▃▄▄▄▄▃▃▃▃▃▃▁▂
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▇▅▄▃▁▃▆▆▆▆▆▆▅▅▄▃▇▇▇█
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▆▅▄▁▄▁▃▅▅▅▅▅▅▄▆▄▆█▇▅
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch ▄▄▃▂▂▁▃▄▅▄▄▄▄▄▄▃▅▆█▄
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch ▄▃▁▂▂▄▃▄▃▄▄▃▄█▁▃▄▅▆▃
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch ▄▃▂▁▂▄▃▄▄▄▄▄▄█▂▃█▃▅▂
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch ▄▄▁▂▃▄▅▄▆▄▄▅▅█▅▇█▆▇▃
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▆▃▂▂▁▁█▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▄▂▂▂▇▆█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▄▄▁▂▃██▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▄▃▂▂▁██▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▂▄▂▂▁█▃▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▂▅▃▂▁█▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▄▁▁▆▂▇▄▃▅█████████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▁▇▃▁▁▃▄▄▃▂██████████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch ▂▅▂▁▂▅▂▄▃▃▇█████████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch ▂▅▂▁▁▄▂▃▂▆██████████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch ▁▃▁▁▁▃▁▃▃▄██████████
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch ▁▂▁▁▇▅▂▄▂▃██████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▃▅███▆█▅█▄▅████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▅▅███▅██▆▄▄████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▁▄▄███▄█▆▅▄▄████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▁▄██▆▇▅█▅▅▄▄████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▁▃██▆████▅▄▄████████
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▁▄██████▄▄▆▄████████
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch █▃▃▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch █▅▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▁▂
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch █▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch █▅▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch █▆▃▄▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▅▄▃▃▃▃▃▂▃▂▃▃▄▆▁▂▇█▅▇
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▄▂▂▂▃▁▂▁▁▃▂▃▂▇▃▄▇█▅▆
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch ▂▂▂▂▂▁▁▂▂▃▂▂▂█▂▁▄▄▂▄
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch ▅▄▄▄▄▄▃▃▄▅▅▃▆▆▆▁▇▇█▆
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch ▃▃▃▃▂▃▃▃▃▃▃▄▄▄▂▁▆█▆▄
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch ▃▃▃▃▂▂▂▂▃▄▃▂▅▃▁▂█▇▆▄
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▃▃▃▃▃▄▃▁▁▁▂▃▃█▃▃▃▄▃▃
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▄▄▄▄▄▂▄▁▁▂█▁▄▅▄▄▄▄▄▄
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch ▅▅▅▅▅▆▅▂█▁▅▅▅▅▅▅▆▅▅▅
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch ▃▃▃▄▃▄▄▁▁▁▃▃▃▃▄▃▄▃▇█
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch ▃▃▃▃▃▃▁▁▁▁▃▄█▃▃▃▃▃▃▃
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch ▃▃▃▃▃▃▁▁▁▁▃▃█▃▃▃▅▃▃▃
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch █▃▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▁▁▁
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch █▃▃▁▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch █▃▃▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▂
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch █▃▃▂▂▃▃▂▁▂▂▂▁▁▁▁▁▁▁▁
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch █▂▃▁▃▄▁▂▂▂▄▂▂▂▃▂▃▃▂▃
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch █▂▃▁▃▂▂▂▂▃▂▂▂▂▂▂▄▃▃▂
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▅
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▄▂▂▃▃▃▃▂▂▂▂▂▁▂▂▁▁▁▂
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▄▃▃▃▃▂▄▃▃▃▂▂▃▂▂▂▁▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▃▃▃▃▃▃▂▃▃▂▂▃▂▂▂▂▁▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▅▅▄▄▃▃▄▅▄▃▃▃▃▂▂▂▁▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▅▇▅▅▄▄▅▄▄▃▃▃▃▄▃▃▂▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▂▂▂▂▁▁▁▁▁▁▆▁▁▁▁▂▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▃▂▂▂▂▁▁▁▁▁▃▁▁▁▆▁▁▁▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▆▃▂▃▃▁▁▂▁▁▁▁█▁▁▁▄▁▄▁
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▄▃▃▅▂▁▂▁▂▁▃▁▁▂▃▁▁▁▁
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▆▆▂▁▁▁▁▇▅█▄▄▂▄▂▂▆▅▄▁
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▁▁▁▁▁█▃▃▂▂▁▁▁▂▁▁▁▂▁
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch ▂▁▁▁▁▁█▂▁▄▂▂▁▁▂▂▃▁▂▁
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch ▅▂▁▁▁▁█▆▅▂▅▂▃▃▂▃▂▂▂▂
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch ▅▁▁▁▁▁▄▃▇█▄▂▆▂▂▃▄▂▂▁
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch ▆▁▁▁▁▁▅▄▆▆▃▃▇▅▄▄█▂▁▁
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▃
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▄▃▂▂▂▂▂▁▁▃▁▁▁▁▁▁▂▂▃
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch █▄▄▃▃▂▂▂▁▁▂▁▂▁▁▁▁▁▂▂
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch █▄▃▂▃▂▂▂▂▁▂▁▁▁▂▁▁▂▃▃
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch █▄▃▃▂▂▂▁▁▁▁▁▂▁▁▁▁▃▄▃
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch █▄▄▃▂▂▂▁▂▂▂▂▁▂▁▂▁▂▃▄
wandb: 
wandb: Run summary:
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 10.58841
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 3.73741
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch 0
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch 5.84445
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch 29.92896
wandb:                    cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch 242.89058
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 212.24634
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 8.23777
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 29.18723
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 41.18684
wandb:           cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 15.4398
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 34.691
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch 0
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch 16.73339
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch 74.69309
wandb:                    cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch 0.57657
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 5.65128
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0.24755
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0.94306
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 307.95244
wandb:           cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 12.61294
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 6.39985
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 51.41764
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch 0.44677
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch 0.54123
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch 13.30688
wandb:                    cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch 0.6195
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 97.58409
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 39.73038
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 34.99292
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 6.05359
wandb:           cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.94462
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 11.05243
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 0.12857
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch 1.39308
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch 0.23299
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch 235.76348
wandb:              cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch 146.02641
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0.22304
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 28.58534
wandb:     cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 2.83199
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 127.9445
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 0.4322
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch 12.67932
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch 1.70956
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch 110.62366
wandb:               cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch 77.61742
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 294.74438
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 656.21011
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 608.03915
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 260.87135
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 634.76434
wandb:      cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 426.92348
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 9080.44052
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 369.47432
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch 1270.9035
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch 924.7793
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch 13032.12117
wandb:                cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch 1077.15896
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 6448.09184
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 515.76938
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 1345.70909
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 2944.63844
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 15203.35835
wandb:       cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 2001.31754
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 8.96056
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 0.10108
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch 0.56662
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch 0.35038
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch 27.35883
wandb:                       cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch 0.60815
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 68.78207
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 2.96994
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch 0
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch 0
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch 163.67169
wandb:                      cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 1440.93777
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 15.05098
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch 9.95838
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch 29.96025
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch 89.12134
wandb:                         cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 1.28131
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch 0
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch 0
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch 6.2168
wandb:                       cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0.4331
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 21.97766
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 2.71641
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 24.16099
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0.92703
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0.92195
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 2.92206
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 7.35591
wandb:   cumulative_loss_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 1.81779
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 495.7918
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 14.26743
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 1397.91129
wandb:      cumulative_loss_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 56.10659
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 8.66885
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 78.43944
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 62.88237
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 13.01653
wandb:    cumulative_loss_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.88346
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 2.2459
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 2.07673
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch 0
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch 1.03304
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch -4
wandb:                  cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch 66.29622
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 71.53397
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 4.07534
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 1.96071
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.05714
wandb:         cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.14128
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 80.80209
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch 0
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch 44.7313
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch 11.50219
wandb:                  cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch -1
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2.69715
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0.13675
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0.37922
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 17.33236
wandb:         cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.15181
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch -2
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 60.84904
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch -1
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch -1
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch -2
wandb:                  cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch -2
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2.14015
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 8.37584
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 1.6158
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0.00632
wandb:         cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.00206
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch -2
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch -1
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch -1
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch -1
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch 183.03626
wandb:            cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch 558.96332
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.01316
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.17536
wandb:   cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -0.04233
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch -6.1338
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch -3
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch 6.55981
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch -0.65364
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch 1.58562
wandb:             cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch 3.8076
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 53.44757
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 1618.15588
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 459.61635
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 190.90955
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 13.84036
wandb:    cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 102.28838
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch -1080.5216
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch -2012.47129
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch -1311.935
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch -2009.19823
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch -2017.03024
wandb:              cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch -1954.11713
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -510.29783
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -510.17028
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch -510.10354
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch -510.19877
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -510.23469
wandb:     cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -510.12994
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch -2
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch -1
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch -2
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch -2
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch -1.68806
wandb:                     cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch -2
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 9.97382
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch -1.01757
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch 0
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch 0
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch 13.46621
wandb:                    cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 786.23507
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 25.54955
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch -8.03804
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch 6.59583
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch -1.75234
wandb:                       cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 0
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch -1.66896
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch 0
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch 0
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch -2
wandb:                     cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch 0
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.01521
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.11804
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -0.02774
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.28225
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.0133
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch -0.03104
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch -0.09572
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.04237
wandb: cumulative_reward_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch -0.01857
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 41.03965
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 171.78575
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 54.52764
wandb:    cumulative_reward_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.7276
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 1.22618
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 14.89369
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 11.00554
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch -0.0028
wandb:  cumulative_reward_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0.00341
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 703
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 805
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task2_per_epoch 617
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task3_per_epoch 823
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task4_per_epoch 682
wandb:                             errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task5_per_epoch 707
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 44
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 34
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 34
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 42
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 45
wandb:                    errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 38
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 819
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 806
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task2_per_epoch 812
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task3_per_epoch 813
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task4_per_epoch 813
wandb:                             errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task5_per_epoch 818
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 52
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 32
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 57
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 51
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 68
wandb:                    errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 75
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 821
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 822
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task2_per_epoch 821
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task3_per_epoch 821
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task4_per_epoch 821
wandb:                             errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task5_per_epoch 820
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 69
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 51
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 59
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 46
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 44
wandb:                    errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 58
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 823
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 822
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task2_per_epoch 824
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task3_per_epoch 826
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task4_per_epoch 822
wandb:                       errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task5_per_epoch 824
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 22
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 31
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 22
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 27
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 18
wandb:              errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 25
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 906
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 849
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task2_per_epoch 842
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task3_per_epoch 817
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task4_per_epoch 790
wandb:                        errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task5_per_epoch 800
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 0
wandb:               errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 0
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 2002
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 2028
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task2_per_epoch 2017
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task3_per_epoch 2040
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task4_per_epoch 2040
wandb:                         errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task5_per_epoch 2040
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 510
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 510
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 510
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 510
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 510
wandb:                errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 510
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task2_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task3_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task4_per_epoch 207
wandb:                                errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task5_per_epoch 206
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 1009
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 976
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task2_per_epoch 981
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task3_per_epoch 912
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task4_per_epoch 887
wandb:                               errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task5_per_epoch 909
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 233
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 220
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task2_per_epoch 205
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task3_per_epoch 389
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task4_per_epoch 201
wandb:                                  errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task5_per_epoch 200
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 205
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task2_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task3_per_epoch 206
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task4_per_epoch 208
wandb:                                errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task5_per_epoch 205
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 118
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 35
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 7
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 8
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 5
wandb:             errors_run{'meta_recurrent_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 4
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 24
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 9
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 8
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 6
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 7
wandb:            errors_run{'meta_recurrent_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 6
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 1
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 0
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 7
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 1
wandb:               errors_run{'meta_recurrent_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 5
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 60
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 61
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task2_per_epoch 33
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task3_per_epoch 49
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task4_per_epoch 51
wandb:             errors_run{'meta_recurrent_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task5_per_epoch 66
wandb: 
wandb: 🚀 View run tune_1225_2215 at: https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/b2a1c3kf
wandb: ️⚡ View job at https://wandb.ai/mattstachyra/reinforcement-meta-learning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzY1Nzc3Ng==/version_details/v57
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231225_221533-b2a1c3kf/logs
