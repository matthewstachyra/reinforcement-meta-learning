wandb: Appending key for api.wandb.ai to your netrc file: /cluster/home/mstach01/.netrc
2023-12-23 22:00:17.465108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.
wandb: Currently logged in as: mattstachyra. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/wandb/run-20231223_220034-cmuswfh9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tuning_1223_2200
wandb: ⭐️ View project at https://wandb.ai/mattstachyra/reinforcement-meta-learning
wandb: 🚀 View run at https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/cmuswfh9
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  tasks_targets = torch.tensor(np.array([
/cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/disc-episode-adding-layers.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tasks_targets = torch.tensor(np.array([
wandb: Network error (ReadTimeout), entering retry loop.
/cluster/home/mstach01/condaenv/mthesis/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 5
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=5 and n_envs=1)
  warnings.warn(
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.040 MB uploaded (0.000 MB deduped)wandb: | 0.086 MB of 0.086 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▂▂▂▁▄▁▁▁▂█▁▃▄▄▄▆▂▂▂▂
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▆▁█▆▂▂▁▁▃▂▂▄▁▁▂▄▄▄▁▂
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▃▁▁▇▃▇▁▃▄▁█▁▂▂▂▁▂▁▂▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃█▅▁▅▇▂▄▄▄▂▁▁▆▃▃▆▁▅▅
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▁▂▁▁▁▁▅▃▆▂▁▂█▆▇█▃▅▆▆
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▆▅▅▂▄▂▃▁▃▂▇▂▄█▅▁▂▅▂▂
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▃▃▆▁▁▁█▁▂▁▂▂▂▂▁▁▁▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁█▆▄▂▇▄▂▂▄▁▃▄▂▄▂▁▃▃▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▂▁▂█▂▂▃▂▁▃▁▁
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▄▂▁▁▂▁▁▂▁▂▁▃▄▂▂▂█▁▂▄
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▂█▄▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▂▂▂▁▄▅▂▂▂▁▁▁▁▁█▁▂▁▂
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▁█▁█▃▁▂▂▁▃▁▁▁▄▆▂▁▂▁▂
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▄▂▃▃▂▁▁▁▁▃▄▇▅▄▄█▃▁▃▁
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▄▆▁▄█▄▂▁▂▂▃▁▂▂▂▂▂▁▄▃
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▆▁▂█▂▁▄▃▃▄▃▄▄▃▅▄▃▃▅
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▂▁▁▁▁▃▁▁█▁▂▁▁▃▁▁▂▂▁
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▃▂█▅▁▂▃▁▁▂▆█▃▅▅▃▃▄▄▂
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂█▁▇▂▂▂▃▃▃▃▃▂▁▂▁▁▁▁▁
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▃▁█▇▄▄▁▁▂▂▅▄▁▇▅▁▇▇▁
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▃▁█▁▂
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁█▁▁
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▂▁▂▁▁▁▂▁▁▁▁▂▇▁█
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▄▄█
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▂▃▄▂█▁▂▁▁▁▁▁▁▁▁▁▁▁▄▂
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch █▁▅█▂▃▄▁▂▁▁▁▁▃▂▁▄▁▃▄
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▃▄▃▃▁▁▄▃▁▄▇▅█▄▅▃▃▂▁
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▃▂▃▁▃▃▄▃▁▆▁▃█▂▂▁▂▂▂
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▂▁▁▁▁▃▂▁▁▂▁▁▁▁▁▅▁█
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▄▇▃▂▁▃▂▁▂▂▁▃▁▄▂▁▁█▁▁
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁█▁▁▃▅▁▂▄▅▆▂▁▂▁▁▂▂▂
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▂▅▁▃▄▃▇█▇█▁▂▂▂▃▁▃▂▄
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▃▁▂▁▁▁▁▁▁▁▁▁▆▁▁█
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▂▁▁█▆▁▂▂▆▂
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▁▂▂▂█
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▃▃▁▁▁▁▂▁▄▁▁▁▂▁█▃
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch █▅▇▁▇▅▂▁▂▄▂▂▄▂▁▃▃▂▁▁
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▂▁▂▁▁▂▂▁▁▂▁▄▁█▁
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▁▂▁▂▁▁▂▁▂▂▁▂▁▃▂▁█▅▁
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch ▂▁▄▃▂▃▃▃▂▅▃▃▅▃██▂▅▂▁
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch ▃█▅▃▅▇█▇▇▆▇▆██▅▄▁▇█▇
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▄▃▄▄█▄▅▄▄▆▄▃▄▄▄▄▄▄▄
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▇▇█▆▅█▇▇▇▇▇▇▇▇▇▇▇▇
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch ▃▁▂▂▂▂▃▂▃▂▃▂▅▅▄█▁▃▃▄
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch ▇▇▆▆▆▆▅▇▅▆▇▆█▁█▇███▆
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▂▃▆▂▂▂█▂▂▂▂▁▂▂▂▂▂▂
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▅▁▃▃▆█▆█▅▆▅▅▅▅▅▅▅▅▅▅
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch ▂▂▂▂▂▂▂▂▂▂▂█▃▄▅▃▂▃▁▂
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch ▅▅█▆▅█▅▅▆▅█▁▇▆▆▄▆█▇█
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▂█▆█▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▁▅▅▅▅
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch ▃▃▃▅▃▃▃▂▃▆▃▃▄█▅▁▃▁▄▃
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch ▄▅▅▄▄▇▇▇▇▃█▆█▇▁▃▆▇█▇
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▂▁▂█▄▃▂▂▂▂▂▂▂▂▂▂▂▁▂
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▅▁▅▁█▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▃▁▁█▁▁▁▂▂▂▁▁▁▁
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch ▅▄██▆▅▆▇▆▇▆▃▄▂▁▆▇▅▅▅
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▄▃█▆▆▄▅▄▄▄▄▄▄▄▄▄▄▄▄
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▅▅▅▆▅█▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ███████▃███████▃█▁█▄
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▇██████████▇█████▁██
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █████▄▆▅███▅▆███▁▂█▁
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ███████████████▃▁▂▁▁
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch ▁▄▁▃▄▄▂▃▃▄▄▃▄▄▄▄▃▄█▃
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch ▁█▄▂▄▄▄▆▆▆▆██▄▆█▄█▄▄
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▂▁▁▃▃▃▃▆▄▃▅▆▄█▆▄▃▃▃▃
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▆▄▄▇▆█▆▇▇█▇▇▇▆▇▇▆▆▆
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch ▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂█▁▆
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch ▂▂▃▂▃▃▂▃▂▂▃▁▃▃▂▃▃█▃▃
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▃▂▂▃▅▂▂▇▃█▂▂▁▂▂▂▂▂
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▂▂▃▃▃▄▅▆█▂▂▂▂▂▂▂▂▂
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▁▂▁▁▂▂▁▂▁▂▂▂▂▂▂▂█▂▂▁
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch █████████████████▁██
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▆▆▆▆▆▆▆▆▆▆▃▆▆▃▅█▅▁▁▅
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ███████████▇█▃▃█▁▁▁▁
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch ▁██▅▅▆█▅▆▅▃▅▆▅▆▅▅▆▇▅
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch ▁▄▂█▁▅▅▇▇▄█▅▅▅▇▅▅▅██
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁█▁
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▃▂
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch █▅▄▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch █▆▄▄▃▃▃▃▂▂▂▂▂▂▃▂▁▁▁▄
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▆▅▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▆▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch █▆▄▄▃▃▃▃▃▃▃▂▂▁▁▁▁▂▁▃
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch █▅▄▄▃▄▂▃▃▂▁▂▂▂▂▁▂▂▂▂
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch █▆▅▅▅▅▅▄▅▄▃▄▄▂▄▃▃▁▁▁
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch █▇▆▆▆▅▅▅▅▅▄▄▄▃▃▅▁▁▃▁
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▆▃▂▂▂▂▂▂▂▁▁▁▁▁▁▆▅▂▁
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▄▃▂▂▂▂▂▁▃▁▁▁▁▁▁█▃▂▁▁
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch █▅▄▃▂▂▂▂▂▁▃▂▄▃▄▄▇▇▆▄
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch █▄▃▃▃▃▁▂▃▄▂▅▂▃▃▄▆▆▄▄
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch █▅▂▃▁▃▂▂▁▃▃▂▃▆▃▄▇█▃▄
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch █▃▃▄▄▃▂▃▃▃▁▁▆▃▅▅▅▅▅▂
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▆▄▃▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ██▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch ▁▃▁▁▁▃▁▇▃▂▅▃▂▁▅█▇█▅▆
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch ▅▃▃▃▅▄▃▆▄▃▃▅▂▄▁█▇▇▄▃
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▃▁▁▆▂▇▅▅▂▄▅▅▅▂▂▄█▇▂█
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▂▃▁▃▃▆▅▂▂▂▄▂▃▃▄▇█▇██
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch █▅▃▁▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch █▄▃▄▃▃▂▃▂▂▂▂▂▂▁▁▂▂▂▂
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▅▄▃▂▁▂▂▂▂▁▂▁▁▂▁▂▁▂
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▅▅▄▃▂▁▂▂▁▁▂▂▂▁▁▂▂▁▁
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch █▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▂
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch █▆▄▄▄▃▁▂▂▂▄▁▃▂▂▃▂▂▂▁
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▆▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▆▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch ▇▃▃▁▁▃▁▁▂▁▄▆▁▁▁▂▂▁▂█
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch ▃▃▂▁▂▁▂▁▁▂▄▁▃▁▁▁▁█▄▁
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch ▃▃▂▂▁▁▁▁▁▁▅▁▁▅██████
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch ▃▂▃▂▁▁▁▁▁▁▂▂▁███████
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch █▄▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch █▅▂▃▃▂▂▂▁▃▂▂▂▂▁▂▂▂▂▂
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch █▇▆▆▄▃▂▂▁▁▂▁▁▂▁▁▁▁▂▂
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch █▇▆▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 2.32187
wandb:                  cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 7.95794
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0.76735
wandb:         cumulative_loss_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 22.70943
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 6.56408
wandb:                  cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 14.26202
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.58807
wandb:         cumulative_loss_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 1.76219
wandb:                  cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 53.024
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.46421
wandb:         cumulative_loss_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 22.88796
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 2.46093
wandb:            cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 3.17565
wandb:   cumulative_loss_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 31.95323
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 1.46174
wandb:             cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 14.64805
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:    cumulative_loss_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 2838.37593
wandb:              cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 6.23897
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 109351.50116
wandb:     cumulative_loss_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 413298.11494
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 2.63687
wandb:                     cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 33.24466
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0.62769
wandb:            cumulative_loss_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 23.49425
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 74.77536
wandb:                    cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 7.27608
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.70332
wandb:           cumulative_loss_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 40.46804
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 50.00274
wandb:                       cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2827.8495
wandb:              cumulative_loss_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 87800.72023
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 3.11916
wandb:                     cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.20787
wandb:            cumulative_loss_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch -3
wandb:                cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch -1
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.03934
wandb:       cumulative_reward_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.18469
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 2.06101
wandb:                cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch -2
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.08203
wandb:       cumulative_reward_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch -0.41395
wandb:                cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 0.15155
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.07488
wandb:       cumulative_reward_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.18646
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch -1.12363
wandb:          cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.16181
wandb: cumulative_reward_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.26052
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch -0.65088
wandb:           cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch -2
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 0
wandb:  cumulative_reward_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch -529.25141
wandb:            cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch -1
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -1945.71851
wandb:   cumulative_reward_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -1983.3033
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch -1.92373
wandb:                   cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch -2.04263
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.01262
wandb:          cumulative_reward_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -0.18015
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 57.97057
wandb:                  cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch -1
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -0.0877
wandb:         cumulative_reward_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -1.26508
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch -3.91678
wandb:                     cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch -140.87849
wandb:            cumulative_reward_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch -496.73723
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch -1.54149
wandb:                   cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 0
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 1.00871
wandb:          cumulative_reward_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 0
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task0_per_epoch 773
wandb:                           errors_run{'meta_clip_range': 0.1, 'sb3_model': 'PPO'}_task1_per_epoch 862
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 29
wandb:                  errors_run{'meta_clip_range': 0.1, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 24
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task0_per_epoch 815
wandb:                           errors_run{'meta_clip_range': 0.2, 'sb3_model': 'PPO'}_task1_per_epoch 789
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 17
wandb:                  errors_run{'meta_clip_range': 0.2, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 20
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task0_per_epoch 720
wandb:                           errors_run{'meta_clip_range': 0.3, 'sb3_model': 'PPO'}_task1_per_epoch 757
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 21
wandb:                  errors_run{'meta_clip_range': 0.3, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 28
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task0_per_epoch 862
wandb:                     errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'PPO'}_task1_per_epoch 843
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 14
wandb:            errors_run{'meta_learning_rate': 0.0003, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 36
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task0_per_epoch 854
wandb:                      errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'PPO'}_task1_per_epoch 799
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 8
wandb:             errors_run{'meta_learning_rate': 0.001, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 9
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task0_per_epoch 1746
wandb:                       errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'PPO'}_task1_per_epoch 983
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 2001
wandb:              errors_run{'meta_learning_rate': 0.01, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 1997
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task0_per_epoch 207
wandb:                              errors_run{'meta_n_steps': 128, 'sb3_model': 'PPO'}_task1_per_epoch 207
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 55
wandb:                     errors_run{'meta_n_steps': 128, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 36
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task0_per_epoch 784
wandb:                             errors_run{'meta_n_steps': 2048, 'sb3_model': 'PPO'}_task1_per_epoch 779
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 12
wandb:                    errors_run{'meta_n_steps': 2048, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 25
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task0_per_epoch 237
wandb:                                errors_run{'meta_n_steps': 5, 'sb3_model': 'PPO'}_task1_per_epoch 203
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 492
wandb:                       errors_run{'meta_n_steps': 5, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 497
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task0_per_epoch 209
wandb:                              errors_run{'meta_n_steps': 512, 'sb3_model': 'PPO'}_task1_per_epoch 211
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task0_per_epoch 55
wandb:                     errors_run{'meta_n_steps': 512, 'sb3_model': 'RecurrentPPO'}_task1_per_epoch 48
wandb: 
wandb: 🚀 View run tuning_1223_2200 at: https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/cmuswfh9
wandb: ️⚡ View job at https://wandb.ai/mattstachyra/reinforcement-meta-learning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzY1Nzc3Ng==/version_details/v54
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_220034-cmuswfh9/logs
