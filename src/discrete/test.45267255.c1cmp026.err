wandb: Appending key for api.wandb.ai to your netrc file: /cluster/home/mstach01/.netrc
2023-12-20 00:58:12.706093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.
wandb: Currently logged in as: mattstachyra. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /cluster/home/mstach01/code/reinforcement-meta-learning/src/discrete/wandb/run-20231220_005823-ad3wjzv3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test
wandb: â­ï¸ View project at https://wandb.ai/mattstachyra/reinforcement-meta-learning
wandb: ğŸš€ View run at https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/ad3wjzv3
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   cumulative_loss_run1_task0_per_epoch â–â–ˆ
wandb:   cumulative_loss_run1_task1_per_epoch â–ˆâ–
wandb:   cumulative_loss_run2_task0_per_epoch â–â–ˆ
wandb:   cumulative_loss_run2_task1_per_epoch â–â–ˆ
wandb: cumulative_reward_run1_task0_per_epoch â–â–ˆ
wandb: cumulative_reward_run1_task1_per_epoch â–â–ˆ
wandb: cumulative_reward_run2_task0_per_epoch â–â–ˆ
wandb: cumulative_reward_run2_task1_per_epoch â–ˆâ–
wandb:            errors_run1_task0_per_epoch â–ˆâ–
wandb:            errors_run1_task1_per_epoch â–ˆâ–
wandb:            errors_run2_task0_per_epoch â–ˆâ–
wandb:            errors_run2_task1_per_epoch â–ˆâ–
wandb:                    loss_task0_per_step â–â–â–â–â–â–â–â–…â–â–â–ƒâ–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–ˆâ–ƒâ–‡â–â–â–â–
wandb:                    loss_task1_per_step â–†â–ƒâ–…â–†â–„â–„â–ƒâ–…â–…â–ˆâ–‡â–‡â–…â–‚â–†â–†â–„â–„â–„â–„â–â–„â–‚â–„â–ƒâ–„â–†â–ƒâ–ƒâ–„â–‚â–…â–„â–‚
wandb:                    loss_task2_per_step â–ƒâ–‚â–„â–„â–…â–ƒâ–â–ƒâ–…â–„â–„â–â–ƒâ–ƒâ–â–â–ƒâ–…â–ƒâ–â–ˆâ–‚â–…â–…â–‡â–ƒâ–‚â–ƒâ–…â–„â–‚â–„â–ƒâ–…â–ƒâ–â–â–ƒâ–ƒâ–‚
wandb:                  reward_task0_per_step â–â–â–„â–â–…â–â–…â–â–â–…â–â–…â–ˆâ–â–â–…â–â–…â–â–â–â–â–…â–â–â–…â–â–…â–â–…â–…â–…â–â–„â–…â–…â–…â–â–â–…
wandb:                  reward_task1_per_step â–â–â–â– â–â–â–â– â–â–â– â–â–  â–â–  â–â–â–â– â–â– â–â–â– 
wandb:                  reward_task2_per_step â–‡â–†â–â–â–â–†â–‡â–â–‡â–â–â–‡â–‡â–â–‡â–‡â–â–ˆâ–â–‡â–â–†â–â–…â–â–†â–‡â–â–‡â–â–â–â–‡â–‡â–â–‡â–‡â–‡â–â–
wandb: 
wandb: Run summary:
wandb:   cumulative_loss_run1_task0_per_epoch 1.49786
wandb:   cumulative_loss_run1_task1_per_epoch 0
wandb:   cumulative_loss_run2_task0_per_epoch 1.08226
wandb:   cumulative_loss_run2_task1_per_epoch 86.50693
wandb: cumulative_reward_run1_task0_per_epoch -1.95351
wandb: cumulative_reward_run1_task1_per_epoch 0
wandb: cumulative_reward_run2_task0_per_epoch -1.00568
wandb: cumulative_reward_run2_task1_per_epoch -2.0601
wandb:            errors_run1_task0_per_epoch 232
wandb:            errors_run1_task1_per_epoch 186
wandb:            errors_run2_task0_per_epoch 213
wandb:            errors_run2_task1_per_epoch 197
wandb:                    loss_task0_per_step 0.04353
wandb:                    loss_task1_per_step 1.57129
wandb:                    loss_task2_per_step 2.28558
wandb:                  reward_task0_per_step -0.0016
wandb:                  reward_task1_per_step nan
wandb:                  reward_task2_per_step -0.0275
wandb: 
wandb: ğŸš€ View run test at: https://wandb.ai/mattstachyra/reinforcement-meta-learning/runs/ad3wjzv3
wandb: ï¸âš¡ View job at https://wandb.ai/mattstachyra/reinforcement-meta-learning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzY1Nzc3Ng==/version_details/v14
wandb: Synced 6 W&B file(s), 11 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231220_005823-ad3wjzv3/logs
