Idea 1 - Neurosymbolic approach combining RL and DNN. The idea is to use RL to select which ANNs in sequence to complete supervised learning tasks in a related domain such as classification of objects (i.e., RL as a means to choose which "neurons to fire and in what sequence"). We would be trying to minimize loss by the end of the neural network chain and maximize reward for the RL algorithm (e.g., DQN). 

Initial thinking on design.
There are a number (e.g., 300) neural networks of some uniform architectures that function as neurons. RL (e.g., DQN) is used to traverse this (like a gridworld) until the output coming through some n neural networks approaches the target. The RL would learn to use those neural networks (e.g., treating the neural networks as states); the neural networks would learn (the loss from the final neuron / neural network would be backpropagated to the prior neural networks involved). The total network of, for example, 300 neural networks and the DQN would be trained on numerous supervised learning tasks to test whether there is a generalization across similar tasks (i.e., that certain neural networks / neurons trained for related tasks are used; and we would know they are used because each neural network would retain tags/symbols of what it was trained for). 

More details.
- The states are neural networks and we chain them together to produce some output compared to a target output in simple and related supervised learning tasks.
- Discrete action space where action is which neural network to pass the output to.
- The motivation is to break computation down into numerous neural networks for explainability and for generalizibility
- Explainability because each neural network will be tagged with a symbol for what it solves (e.g., "red" for detecting red objects or "circle" for detecting circle or "cat" for detecting cats).
- Generalizability because each next task may (hopefully) use trained neural networks if that produces lower loss and higher reward. 
- To overcome input dimension issues, there is an initial layer of neural networks (for input) and then the remainder of neural networks coexist together in the next layer. This is because after an input passes through one neural network then it loses its original dimension. This will be the job of the initial layer. Then after the first neural network, the output (whatever n-dimensional tensor it is) can be passed to any neural network in the next layer because all have the same dimension of input as is the dimension of the output of any neural network in the initial layer.

Areas to research / sample papers I'd plan to read.
(1) How Reinforcement Learning has been used for optimizing ANNS.
- Learning to optimize neural nets (https://arxiv.org/abs/1703.00441; https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/) 
- Neural Network Optimization for Reinforcement Learning Tasks Using Sparse Computations (https://arxiv.org/abs/2201.02571)
(2) Alternatives to backpropogation.
- Decoupled Neural Interfaces using Synthetic Gradients (https://arxiv.org/pdf/1608.05343.pdf)
- HSIC Bottleneck (Hilbert-Schmidt Independence Criterion) (https://arxiv.org/abs/1908.01580)
- equilibrium propagation (https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full)
- contrastive hebbian learning (https://www.ics.uci.edu/~xhx/publications/chl_nc.pdf)
(3) Advances in neurosymbolic reinforcement learning.
- Neuro-Symbolic Reinforcement Learning with First-Order Logic (https://arxiv.org/abs/2110.10963)
- Towards deep symbolic reinforcement learning (https://arxiv.org/abs/1609.05518)


