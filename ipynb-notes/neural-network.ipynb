{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "Nested structure of modules that subclass `nn.Module`. \n",
    "\n",
    "The model itself subclasses `nn.Module` as do all its layers (e.g., densely connected layer).\n",
    "\n",
    "Containers store layers. You then use the container like a function to pass input through all the layers. E.g., `nn.Sequential()` will store layers like `nn.Linear()` and `nn.ReLU()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits\n",
    "\n",
    "x/(1-x)\n",
    "\n",
    "Also referred to as log-odds.\n",
    "\n",
    "We say the output is the logits. We choose to *interpret* the output this way. We say that the matrix multiplication operations of a neural network produce the logits. Then something like the softmax converts these logit values to probability values by squeezing them to [0,1] from (-inf,inf). The squeezing is done by a function using e^-x, so that having the output in terms of log makes the calculation easier.\n",
    "\n",
    "Reminder: odds the ratio of it happening to it not happening. E.g., p=0.75 means that odd are 3 to 1 of some event occuring. After 4 races, we will win 3 of them and lose 1 of them.\n",
    "\n",
    "The logit function is the inverse of the sigmoid function. Logit function is log((x)/(1/x)). Sigmoid function is e^((x)/(1/x)). As expected, sigmoid(logit(p)) = p for some probability p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid \n",
    "\n",
    "1/1+e^-x\n",
    "\n",
    "Inverse function to log odds function. It is an S-shaped curve.\n",
    "\n",
    "Sigmoid inputs 1 logit and outputs 1 probability.\n",
    "\n",
    "Softmax inputs a vector of logits and outputs a vector of probabilities.\n",
    "\n",
    "You use sigmoid for binary classification and softmax for multiclass classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "e^xi/Sum_ij e^xij\n",
    "\n",
    "The component probability for each component of the output vector is the odds of it divided by the sum of all the odds. This results in normalization. It is a probability distribution over the classes. ('Over' refers to the components in the vector that each value in the distribution is tied to.)\n",
    "\n",
    "This works because the exponential function e^x times the logits produces the odds, which is [0, inf) and monotonically increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellany\n",
    "`nn.flatten()` is different than `torch.squeeze()`. Flatten removes dimensions. E.g., flattening a d=2 (28,28) dimension image to d=1 (784,). So from a 2d to 1d vector. In addition, you can specify where to start and end the flatten. Squeeze removes all dimensions of size 1.\n",
    "\n",
    "`nn.Linear()` applies a linear transformation to the input vector. y=xA^T + b. E.g., take a (3, 784) dimension image that needs to be transformed to (3, 20) dimension output. The matrix would need to be (784, 20) dimension to accomodate the transformation. It is the transform of A, A^T, because xA^T = Ax, because we can do (3, 784)(784,20) and (20,784)(784,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 28, 28, device=device)\n",
    "print(x.size()) # note this is the same as print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits are tensor([[-0.0197, -0.0396,  0.0082, -0.0159,  0.0155,  0.0200,  0.0305, -0.0899,\n",
      "         -0.0075,  0.0784]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Predicted probability is tensor([[0.0982, 0.0962, 0.1009, 0.0985, 0.1017, 0.1021, 0.1032, 0.0915, 0.0994,\n",
      "         0.1083]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(x)\n",
    "print(f\"Logits are {logits}\")\n",
    "yhat = nn.Softmax(dim=1)(logits)\n",
    "print()\n",
    "print(f\"Predicted probability is {yhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('master-thesis-rqFFn_mi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6803830149994344f405acd158f41316c80d4030c2f52c5a61d6af244d3bc4b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
